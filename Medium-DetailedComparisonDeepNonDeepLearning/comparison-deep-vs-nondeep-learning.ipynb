{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81eabc96",
   "metadata": {},
   "source": [
    "# Detailed comparison between deep and non-deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d52bdb",
   "metadata": {},
   "source": [
    "## By Brecht De Boeck - on behalf of Group 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf8d340",
   "metadata": {},
   "source": [
    "The difference between deep learning and non-deep learning is very simple: Any neural network with at least three layers is considered deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82919d8",
   "metadata": {},
   "source": [
    "For this part of our assignment, I'll be creating two separate image classification models, one deep learning and one non-deep, and comparing the two based on both the speed at which they make a prediction, and their overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7b125e",
   "metadata": {},
   "source": [
    "### Creating the non-deep learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5d558cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2 as cv\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ce019d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the basic data from the labeled dataset\n",
    "data = pd.read_csv(\"data/swarmIoT_full_labeled_dataset.csv\")\n",
    "images = data[\"image\"].str.split(\"-\", n = 1, expand = True)\n",
    "data[\"image\"] = images[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375da77d",
   "metadata": {},
   "source": [
    "Neural networks are the most obvious choice when it comes to image classification, but these generally fall under deep learning. We could try using a neural network without any hidden layers, where input and output are directly connected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8972f4",
   "metadata": {},
   "source": [
    "However, we have seen a lot of non-deep learning models throughout this course. Sure, none of them were explicitly used for image classification, but that doesn't mean we can't try using one of them for it. But then the question remains: which algorithm do we choose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c96350",
   "metadata": {},
   "source": [
    "Our options:\n",
    "- kNN: Nope. Not using this one. Not only is it way too slow, but it's pretty much undeployable, seeing as there isn't a real model that's created.\n",
    "- Support Vector Machines: While an interesting choice, these kinds of models don't do very well in the way of speed and scaleability. Especially in the case of that first one, we don't want to give our neural network an unfair advantage.\n",
    "- Decision Trees: Seems to be our best fit. Though it cannot actually see the images, but only their raw values, it is capable of making decisions based on the given input, similar to how a deep learning neural network would do it.\n",
    "- Logistic Regression: As bizarre as it might seem, we can also use Logistic Regression if we reshape the dimensions of our training and test set to consist of at most two dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95c7c4a",
   "metadata": {},
   "source": [
    "For this task, I'll be making both a Decision Tree and a Logistic Regression model. The Decision Tree (at least from my perspective) seems to lie the closest to emulating a neural network, while for Logistic Regression, I'm simply curious to find out what results the model might give us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c8de8d",
   "metadata": {},
   "source": [
    "And finally, as discussed, I'll be making a neural network without any hidden layers. All of these models will be compared to our deep learning models made in the next chapter of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f169e18e",
   "metadata": {},
   "source": [
    "#### Extracting features from the images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35fa3ac",
   "metadata": {},
   "source": [
    "The problem with decision trees is that, of course, we can't just feed raw images to our model. The decision tree expects features with differing values, which it can then use to make splits to gain as much information gain as possible. The same goes for our Logistic Regression model: we can't just expect to be able to throw in raw image data and have the model make accurate predictions. Our first order of business is thus to create a new dataset, that combines our classifiers and their images with features we extract from the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0a03268c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "9bd69a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function will be used to process every image within our dataset\n",
    "def extract_features_from_image(camera, partition, img):\n",
    "    st = time.time()\n",
    "    path = \"/\".join(['data','images',camera,partition,img])\n",
    "    partition_path = \"/\".join(['data','images',camera,partition])\n",
    "    image = plt.imread(path)\n",
    "    \n",
    "    dicto = []\n",
    "    \n",
    "    if image is not None:\n",
    "        #Extract features using SIFT\n",
    "        sift = cv.SIFT_create()\n",
    "        kp, des = sift.detectAndCompute(image,None)\n",
    "        for d in des:\n",
    "            dicto.append(d)\n",
    "        \n",
    "        k = 30\n",
    "        batch_size = np.size(os.listdir(partition_path)) * 3\n",
    "        kmeans = MiniBatchKMeans(n_clusters=k, batch_size=batch_size, verbose=1).fit(dico)\n",
    "        \n",
    "        histo = np.zeros(k)\n",
    "        nkp = np.size(kp)\n",
    "        for d in des:\n",
    "            idx = kmeans.predict([d.astype(np.double)])\n",
    "            histo[idx] += 1/nkp\n",
    "    return histo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a46482a",
   "metadata": {},
   "source": [
    "Using the above method should work when working with machine learning models, but we cannot use it on all of our images. That would simply be a fool's errand, clustering the keypoints of more than 6000 images. It's a good thing our images are split amongst partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781b1bb4",
   "metadata": {},
   "source": [
    "I'll be using the images from Camera 3, specifically the first partition, which our group labeled ourselves and only contains 856 images. I'll be taking the first 50 images from this partition and splitting it into training and test datasets. That's all the abuse I'm willing to inflict on my poor computer, even if the resulting sample won't give us nearly as much information as when working on the full dataset or even just a simple partition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21deb15",
   "metadata": {},
   "source": [
    "**For the Record**: Yes, I did try getting the features of an entire partition. That attempt resulted in my computer crashing. Again. I work with what I have in this assignment, so please just bare with me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "1d12a7ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 291826708.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 290078818.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 286772473.0\n",
      "Minibatch step 1/100: mean batch inertia: 137716.42173288655\n",
      "Minibatch step 2/100: mean batch inertia: 91830.39866856401, ewa inertia: 91830.39866856401\n",
      "Minibatch step 3/100: mean batch inertia: 89250.01682988797, ewa inertia: 89250.01682988797\n",
      "Minibatch step 4/100: mean batch inertia: 89442.3004474216, ewa inertia: 89442.3004474216\n",
      "Minibatch step 5/100: mean batch inertia: 89046.21283550088, ewa inertia: 89046.21283550088\n",
      "Minibatch step 6/100: mean batch inertia: 89473.04884279845, ewa inertia: 89473.04884279845\n",
      "Minibatch step 7/100: mean batch inertia: 88998.75502006288, ewa inertia: 88998.75502006288\n",
      "Minibatch step 8/100: mean batch inertia: 87933.95818179616, ewa inertia: 87933.95818179616\n",
      "Minibatch step 9/100: mean batch inertia: 88829.28346020142, ewa inertia: 88829.28346020142\n",
      "Minibatch step 10/100: mean batch inertia: 88889.71160603735, ewa inertia: 88889.71160603735\n",
      "Minibatch step 11/100: mean batch inertia: 88399.32184230472, ewa inertia: 88399.32184230472\n",
      "Minibatch step 12/100: mean batch inertia: 89119.15779758699, ewa inertia: 89119.15779758699\n",
      "Minibatch step 13/100: mean batch inertia: 88357.07866343221, ewa inertia: 88357.07866343221\n",
      "Minibatch step 14/100: mean batch inertia: 87980.77468287668, ewa inertia: 87980.77468287668\n",
      "Minibatch step 15/100: mean batch inertia: 88433.78008666965, ewa inertia: 88433.78008666965\n",
      "Minibatch step 16/100: mean batch inertia: 88749.12169867555, ewa inertia: 88749.12169867555\n",
      "Minibatch step 17/100: mean batch inertia: 88934.82079773712, ewa inertia: 88934.82079773712\n",
      "Minibatch step 18/100: mean batch inertia: 88503.4030264824, ewa inertia: 88503.4030264824\n",
      "Converged (lack of improvement in inertia) at step 18/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 289068101.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 286070393.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 288896724.0\n",
      "Minibatch step 1/100: mean batch inertia: 137437.56199138344\n",
      "Minibatch step 2/100: mean batch inertia: 93037.85649165805, ewa inertia: 93037.85649165805\n",
      "Minibatch step 3/100: mean batch inertia: 90028.59801615696, ewa inertia: 90028.59801615696\n",
      "Minibatch step 4/100: mean batch inertia: 89278.1878997089, ewa inertia: 89278.1878997089\n",
      "Minibatch step 5/100: mean batch inertia: 89844.75701503576, ewa inertia: 89844.75701503576\n",
      "Minibatch step 6/100: mean batch inertia: 89119.92515245198, ewa inertia: 89119.92515245198\n",
      "Minibatch step 7/100: mean batch inertia: 89154.16215770101, ewa inertia: 89154.16215770101\n",
      "Minibatch step 8/100: mean batch inertia: 89421.00648220864, ewa inertia: 89421.00648220864\n",
      "Minibatch step 9/100: mean batch inertia: 88220.81565188276, ewa inertia: 88220.81565188276\n",
      "Minibatch step 10/100: mean batch inertia: 88478.20672790683, ewa inertia: 88478.20672790683\n",
      "Minibatch step 11/100: mean batch inertia: 88804.0330086008, ewa inertia: 88804.0330086008\n",
      "Minibatch step 12/100: mean batch inertia: 89273.20714160455, ewa inertia: 89273.20714160455\n",
      "Minibatch step 13/100: mean batch inertia: 88730.13188102456, ewa inertia: 88730.13188102456\n",
      "Minibatch step 14/100: mean batch inertia: 88053.21502361803, ewa inertia: 88053.21502361803\n",
      "Minibatch step 15/100: mean batch inertia: 88906.38604486082, ewa inertia: 88906.38604486082\n",
      "Minibatch step 16/100: mean batch inertia: 88774.10826057738, ewa inertia: 88774.10826057738\n",
      "Minibatch step 17/100: mean batch inertia: 88732.60703287678, ewa inertia: 88732.60703287678\n",
      "Minibatch step 18/100: mean batch inertia: 87923.2642796804, ewa inertia: 87923.2642796804\n",
      "Minibatch step 19/100: mean batch inertia: 87925.37145239468, ewa inertia: 87925.37145239468\n",
      "Minibatch step 20/100: mean batch inertia: 88611.09439969526, ewa inertia: 88611.09439969526\n",
      "Minibatch step 21/100: mean batch inertia: 88594.96879445165, ewa inertia: 88594.96879445165\n",
      "Minibatch step 22/100: mean batch inertia: 88626.22259002896, ewa inertia: 88626.22259002896\n",
      "Minibatch step 23/100: mean batch inertia: 88047.31989869507, ewa inertia: 88047.31989869507\n",
      "Minibatch step 24/100: mean batch inertia: 88743.3371236342, ewa inertia: 88743.3371236342\n",
      "Minibatch step 25/100: mean batch inertia: 87737.57511283875, ewa inertia: 87737.57511283875\n",
      "Minibatch step 26/100: mean batch inertia: 88217.60933597844, ewa inertia: 88217.60933597844\n",
      "Minibatch step 27/100: mean batch inertia: 87819.90324538482, ewa inertia: 87819.90324538482\n",
      "Minibatch step 28/100: mean batch inertia: 86977.42370062692, ewa inertia: 86977.42370062692\n",
      "Minibatch step 29/100: mean batch inertia: 87842.55003874682, ewa inertia: 87842.55003874682\n",
      "Minibatch step 30/100: mean batch inertia: 88728.51074353684, ewa inertia: 88728.51074353684\n",
      "Minibatch step 31/100: mean batch inertia: 88586.12719636748, ewa inertia: 88586.12719636748\n",
      "Minibatch step 32/100: mean batch inertia: 87204.18919848558, ewa inertia: 87204.18919848558\n",
      "Minibatch step 33/100: mean batch inertia: 89076.66259825503, ewa inertia: 89076.66259825503\n",
      "Minibatch step 34/100: mean batch inertia: 87311.93677092223, ewa inertia: 87311.93677092223\n",
      "Minibatch step 35/100: mean batch inertia: 88519.54731346133, ewa inertia: 88519.54731346133\n",
      "Minibatch step 36/100: mean batch inertia: 88097.00594376512, ewa inertia: 88097.00594376512\n",
      "Minibatch step 37/100: mean batch inertia: 87996.5237584521, ewa inertia: 87996.5237584521\n",
      "Minibatch step 38/100: mean batch inertia: 87728.2425200092, ewa inertia: 87728.2425200092\n",
      "Converged (lack of improvement in inertia) at step 38/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 288073303.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 287760085.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 287575485.0\n",
      "Minibatch step 1/100: mean batch inertia: 138234.52369554812\n",
      "Minibatch step 2/100: mean batch inertia: 92068.73041253048, ewa inertia: 92068.73041253048\n",
      "Minibatch step 3/100: mean batch inertia: 90277.25502564685, ewa inertia: 90277.25502564685\n",
      "Minibatch step 4/100: mean batch inertia: 89443.67300733752, ewa inertia: 89443.67300733752\n",
      "Minibatch step 5/100: mean batch inertia: 89276.95389395955, ewa inertia: 89276.95389395955\n",
      "Minibatch step 6/100: mean batch inertia: 88712.47654428866, ewa inertia: 88712.47654428866\n",
      "Minibatch step 7/100: mean batch inertia: 88268.32289911073, ewa inertia: 88268.32289911073\n",
      "Minibatch step 8/100: mean batch inertia: 88842.7029254882, ewa inertia: 88842.7029254882\n",
      "Minibatch step 9/100: mean batch inertia: 88568.6714196585, ewa inertia: 88568.6714196585\n",
      "Minibatch step 10/100: mean batch inertia: 87881.9067045675, ewa inertia: 87881.9067045675\n",
      "Minibatch step 11/100: mean batch inertia: 87369.44243687396, ewa inertia: 87369.44243687396\n",
      "Minibatch step 12/100: mean batch inertia: 88140.04067077555, ewa inertia: 88140.04067077555\n",
      "Minibatch step 13/100: mean batch inertia: 88584.97663939091, ewa inertia: 88584.97663939091\n",
      "Minibatch step 14/100: mean batch inertia: 88751.22259510195, ewa inertia: 88751.22259510195\n",
      "Minibatch step 15/100: mean batch inertia: 87795.60470064262, ewa inertia: 87795.60470064262\n",
      "Minibatch step 16/100: mean batch inertia: 88246.14607214522, ewa inertia: 88246.14607214522\n",
      "Minibatch step 17/100: mean batch inertia: 88397.53557667669, ewa inertia: 88397.53557667669\n",
      "Minibatch step 18/100: mean batch inertia: 87471.51647664545, ewa inertia: 87471.51647664545\n",
      "Minibatch step 19/100: mean batch inertia: 87158.84088418023, ewa inertia: 87158.84088418023\n",
      "Minibatch step 20/100: mean batch inertia: 88092.76970408917, ewa inertia: 88092.76970408917\n",
      "Minibatch step 21/100: mean batch inertia: 87839.95454936287, ewa inertia: 87839.95454936287\n",
      "Minibatch step 22/100: mean batch inertia: 88755.4868715166, ewa inertia: 88755.4868715166\n",
      "Minibatch step 23/100: mean batch inertia: 87771.72440510981, ewa inertia: 87771.72440510981\n",
      "Minibatch step 24/100: mean batch inertia: 88090.43188892005, ewa inertia: 88090.43188892005\n",
      "Minibatch step 25/100: mean batch inertia: 87889.0722145802, ewa inertia: 87889.0722145802\n",
      "Minibatch step 26/100: mean batch inertia: 88517.4511935496, ewa inertia: 88517.4511935496\n",
      "Minibatch step 27/100: mean batch inertia: 87873.43259612279, ewa inertia: 87873.43259612279\n",
      "Minibatch step 28/100: mean batch inertia: 88388.60903589708, ewa inertia: 88388.60903589708\n",
      "Minibatch step 29/100: mean batch inertia: 86930.22542908869, ewa inertia: 86930.22542908869\n",
      "Minibatch step 30/100: mean batch inertia: 87336.65645649256, ewa inertia: 87336.65645649256\n",
      "Minibatch step 31/100: mean batch inertia: 87812.55490261408, ewa inertia: 87812.55490261408\n",
      "Minibatch step 32/100: mean batch inertia: 87890.4098125874, ewa inertia: 87890.4098125874\n",
      "Minibatch step 33/100: mean batch inertia: 88099.28032206518, ewa inertia: 88099.28032206518\n",
      "Minibatch step 34/100: mean batch inertia: 88755.56595180626, ewa inertia: 88755.56595180626\n",
      "Minibatch step 35/100: mean batch inertia: 86939.87199275402, ewa inertia: 86939.87199275402\n",
      "Minibatch step 36/100: mean batch inertia: 87615.3036682603, ewa inertia: 87615.3036682603\n",
      "Minibatch step 37/100: mean batch inertia: 88274.62589218479, ewa inertia: 88274.62589218479\n",
      "Minibatch step 38/100: mean batch inertia: 87074.2843501847, ewa inertia: 87074.2843501847\n",
      "Minibatch step 39/100: mean batch inertia: 87453.83729274914, ewa inertia: 87453.83729274914\n",
      "Converged (lack of improvement in inertia) at step 39/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 291589989.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 289753095.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 290657245.0\n",
      "Minibatch step 1/100: mean batch inertia: 137497.52321685015\n",
      "Minibatch step 2/100: mean batch inertia: 91146.25572444827, ewa inertia: 91146.25572444827\n",
      "Minibatch step 3/100: mean batch inertia: 89922.56653992823, ewa inertia: 89922.56653992823\n",
      "Minibatch step 4/100: mean batch inertia: 89720.82157424149, ewa inertia: 89720.82157424149\n",
      "Minibatch step 5/100: mean batch inertia: 88952.55425015138, ewa inertia: 88952.55425015138\n",
      "Minibatch step 6/100: mean batch inertia: 88819.58304383726, ewa inertia: 88819.58304383726\n",
      "Minibatch step 7/100: mean batch inertia: 89330.93698041106, ewa inertia: 89330.93698041106\n",
      "Minibatch step 8/100: mean batch inertia: 89149.73899283912, ewa inertia: 89149.73899283912\n",
      "Minibatch step 9/100: mean batch inertia: 88199.99507673422, ewa inertia: 88199.99507673422\n",
      "Minibatch step 10/100: mean batch inertia: 89389.72959757502, ewa inertia: 89389.72959757502\n",
      "Minibatch step 11/100: mean batch inertia: 88321.90393199673, ewa inertia: 88321.90393199673\n",
      "Minibatch step 12/100: mean batch inertia: 87904.19511521884, ewa inertia: 87904.19511521884\n",
      "Minibatch step 13/100: mean batch inertia: 88928.29873865718, ewa inertia: 88928.29873865718\n",
      "Minibatch step 14/100: mean batch inertia: 88502.49548744368, ewa inertia: 88502.49548744368\n",
      "Minibatch step 15/100: mean batch inertia: 87070.13162431748, ewa inertia: 87070.13162431748\n",
      "Minibatch step 16/100: mean batch inertia: 88329.09374956711, ewa inertia: 88329.09374956711\n",
      "Minibatch step 17/100: mean batch inertia: 88761.92108740029, ewa inertia: 88761.92108740029\n",
      "Minibatch step 18/100: mean batch inertia: 87920.23919933558, ewa inertia: 87920.23919933558\n",
      "Minibatch step 19/100: mean batch inertia: 87376.24873732409, ewa inertia: 87376.24873732409\n",
      "Minibatch step 20/100: mean batch inertia: 88272.35786171761, ewa inertia: 88272.35786171761\n",
      "Minibatch step 21/100: mean batch inertia: 88177.79347123601, ewa inertia: 88177.79347123601\n",
      "Minibatch step 22/100: mean batch inertia: 87776.79245440612, ewa inertia: 87776.79245440612\n",
      "Minibatch step 23/100: mean batch inertia: 87644.568888252, ewa inertia: 87644.568888252\n",
      "Minibatch step 24/100: mean batch inertia: 87305.19347994655, ewa inertia: 87305.19347994655\n",
      "Minibatch step 25/100: mean batch inertia: 87286.0141807906, ewa inertia: 87286.0141807906\n",
      "Converged (lack of improvement in inertia) at step 25/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 282317265.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 284151586.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 283665220.0\n",
      "Minibatch step 1/100: mean batch inertia: 135738.3671613212\n",
      "Minibatch step 2/100: mean batch inertia: 92126.75250937606, ewa inertia: 92126.75250937606\n",
      "Minibatch step 3/100: mean batch inertia: 88572.05091407632, ewa inertia: 88572.05091407632\n",
      "Minibatch step 4/100: mean batch inertia: 89117.69563691698, ewa inertia: 89117.69563691698\n",
      "Minibatch step 5/100: mean batch inertia: 89681.59760618818, ewa inertia: 89681.59760618818\n",
      "Minibatch step 6/100: mean batch inertia: 88859.17861693642, ewa inertia: 88859.17861693642\n",
      "Minibatch step 7/100: mean batch inertia: 87662.15855759766, ewa inertia: 87662.15855759766\n",
      "Minibatch step 8/100: mean batch inertia: 88097.38044701224, ewa inertia: 88097.38044701224\n",
      "Minibatch step 9/100: mean batch inertia: 88170.11760439507, ewa inertia: 88170.11760439507\n",
      "Minibatch step 10/100: mean batch inertia: 88182.99627761889, ewa inertia: 88182.99627761889\n",
      "Minibatch step 11/100: mean batch inertia: 87163.12264537321, ewa inertia: 87163.12264537321\n",
      "Minibatch step 12/100: mean batch inertia: 89231.2439167184, ewa inertia: 89231.2439167184\n",
      "Minibatch step 13/100: mean batch inertia: 87988.05464342721, ewa inertia: 87988.05464342721\n",
      "Minibatch step 14/100: mean batch inertia: 87532.1478089395, ewa inertia: 87532.1478089395\n",
      "Minibatch step 15/100: mean batch inertia: 87379.41924207102, ewa inertia: 87379.41924207102\n",
      "Minibatch step 16/100: mean batch inertia: 87332.15932990347, ewa inertia: 87332.15932990347\n",
      "Minibatch step 17/100: mean batch inertia: 86969.15598570889, ewa inertia: 86969.15598570889\n",
      "Minibatch step 18/100: mean batch inertia: 87420.27614703836, ewa inertia: 87420.27614703836\n",
      "Minibatch step 19/100: mean batch inertia: 87599.18963093888, ewa inertia: 87599.18963093888\n",
      "Minibatch step 20/100: mean batch inertia: 88307.7581190582, ewa inertia: 88307.7581190582\n",
      "Minibatch step 21/100: mean batch inertia: 87522.72503855053, ewa inertia: 87522.72503855053\n",
      "Minibatch step 22/100: mean batch inertia: 86995.60480955358, ewa inertia: 86995.60480955358\n",
      "Minibatch step 23/100: mean batch inertia: 88651.32666775666, ewa inertia: 88651.32666775666\n",
      "Minibatch step 24/100: mean batch inertia: 87505.48170935981, ewa inertia: 87505.48170935981\n",
      "Minibatch step 25/100: mean batch inertia: 87464.08636796985, ewa inertia: 87464.08636796985\n",
      "Minibatch step 26/100: mean batch inertia: 87979.71505913751, ewa inertia: 87979.71505913751\n",
      "Minibatch step 27/100: mean batch inertia: 87771.28319139798, ewa inertia: 87771.28319139798\n",
      "Converged (lack of improvement in inertia) at step 27/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 285830062.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 287572906.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 290911700.0\n",
      "Minibatch step 1/100: mean batch inertia: 135246.11010052657\n",
      "Minibatch step 2/100: mean batch inertia: 92453.65364220366, ewa inertia: 92453.65364220366\n",
      "Minibatch step 3/100: mean batch inertia: 91192.20409500744, ewa inertia: 91192.20409500744\n",
      "Minibatch step 4/100: mean batch inertia: 89457.4150296668, ewa inertia: 89457.4150296668\n",
      "Minibatch step 5/100: mean batch inertia: 89318.54344934147, ewa inertia: 89318.54344934147\n",
      "Minibatch step 6/100: mean batch inertia: 88331.97974098456, ewa inertia: 88331.97974098456\n",
      "Minibatch step 7/100: mean batch inertia: 88643.30806831195, ewa inertia: 88643.30806831195\n",
      "Minibatch step 8/100: mean batch inertia: 88561.21812641785, ewa inertia: 88561.21812641785\n",
      "Minibatch step 9/100: mean batch inertia: 87674.91435437408, ewa inertia: 87674.91435437408\n",
      "Minibatch step 10/100: mean batch inertia: 88629.71875945035, ewa inertia: 88629.71875945035\n",
      "Minibatch step 11/100: mean batch inertia: 88962.79627860588, ewa inertia: 88962.79627860588\n",
      "Minibatch step 12/100: mean batch inertia: 88268.88981916213, ewa inertia: 88268.88981916213\n",
      "Minibatch step 13/100: mean batch inertia: 88105.31019536057, ewa inertia: 88105.31019536057\n",
      "Minibatch step 14/100: mean batch inertia: 88578.32410926235, ewa inertia: 88578.32410926235\n",
      "Minibatch step 15/100: mean batch inertia: 87527.71766083641, ewa inertia: 87527.71766083641\n",
      "Minibatch step 16/100: mean batch inertia: 87784.93267898692, ewa inertia: 87784.93267898692\n",
      "Minibatch step 17/100: mean batch inertia: 87695.51366189423, ewa inertia: 87695.51366189423\n",
      "Minibatch step 18/100: mean batch inertia: 87551.21763283067, ewa inertia: 87551.21763283067\n",
      "Minibatch step 19/100: mean batch inertia: 87631.06117173744, ewa inertia: 87631.06117173744\n",
      "Minibatch step 20/100: mean batch inertia: 88028.07091610445, ewa inertia: 88028.07091610445\n",
      "Minibatch step 21/100: mean batch inertia: 87101.87103785251, ewa inertia: 87101.87103785251\n",
      "Minibatch step 22/100: mean batch inertia: 87609.04397768865, ewa inertia: 87609.04397768865\n",
      "Minibatch step 23/100: mean batch inertia: 87268.01947860813, ewa inertia: 87268.01947860813\n",
      "Minibatch step 24/100: mean batch inertia: 87585.60217180943, ewa inertia: 87585.60217180943\n",
      "Minibatch step 25/100: mean batch inertia: 88184.30980530406, ewa inertia: 88184.30980530406\n",
      "Minibatch step 26/100: mean batch inertia: 87568.1612819928, ewa inertia: 87568.1612819928\n",
      "Minibatch step 27/100: mean batch inertia: 87807.58958745848, ewa inertia: 87807.58958745848\n",
      "Minibatch step 28/100: mean batch inertia: 88254.7581125935, ewa inertia: 88254.7581125935\n",
      "Minibatch step 29/100: mean batch inertia: 88534.58976949826, ewa inertia: 88534.58976949826\n",
      "Minibatch step 30/100: mean batch inertia: 86879.32030915275, ewa inertia: 86879.32030915275\n",
      "Minibatch step 31/100: mean batch inertia: 88225.13317800907, ewa inertia: 88225.13317800907\n",
      "Minibatch step 32/100: mean batch inertia: 87317.23067596141, ewa inertia: 87317.23067596141\n",
      "Minibatch step 33/100: mean batch inertia: 88168.46975764162, ewa inertia: 88168.46975764162\n",
      "Minibatch step 34/100: mean batch inertia: 87136.60051194763, ewa inertia: 87136.60051194763\n",
      "Minibatch step 35/100: mean batch inertia: 87630.89558646837, ewa inertia: 87630.89558646837\n",
      "Minibatch step 36/100: mean batch inertia: 88587.11652439112, ewa inertia: 88587.11652439112\n",
      "Minibatch step 37/100: mean batch inertia: 88403.8821732085, ewa inertia: 88403.8821732085\n",
      "Minibatch step 38/100: mean batch inertia: 87762.30530039438, ewa inertia: 87762.30530039438\n",
      "Minibatch step 39/100: mean batch inertia: 86691.92338943892, ewa inertia: 86691.92338943892\n",
      "Minibatch step 40/100: mean batch inertia: 86725.60523137896, ewa inertia: 86725.60523137896\n",
      "Minibatch step 41/100: mean batch inertia: 87638.18940715447, ewa inertia: 87638.18940715447\n",
      "Minibatch step 42/100: mean batch inertia: 87541.38392175018, ewa inertia: 87541.38392175018\n",
      "Minibatch step 43/100: mean batch inertia: 88824.5999149841, ewa inertia: 88824.5999149841\n",
      "Minibatch step 44/100: mean batch inertia: 88058.79875959747, ewa inertia: 88058.79875959747\n",
      "Minibatch step 45/100: mean batch inertia: 88632.08746141175, ewa inertia: 88632.08746141175\n",
      "Minibatch step 46/100: mean batch inertia: 87775.94024328019, ewa inertia: 87775.94024328019\n",
      "Minibatch step 47/100: mean batch inertia: 88262.8435728678, ewa inertia: 88262.8435728678\n",
      "Minibatch step 48/100: mean batch inertia: 87881.00147258239, ewa inertia: 87881.00147258239\n",
      "Minibatch step 49/100: mean batch inertia: 87026.9173501329, ewa inertia: 87026.9173501329\n",
      "Converged (lack of improvement in inertia) at step 49/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 286797720.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 288835677.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 290040655.0\n",
      "Minibatch step 1/100: mean batch inertia: 136433.2647199617\n",
      "Minibatch step 2/100: mean batch inertia: 90421.23927364034, ewa inertia: 90421.23927364034\n",
      "Minibatch step 3/100: mean batch inertia: 90680.7840392156, ewa inertia: 90680.7840392156\n",
      "Minibatch step 4/100: mean batch inertia: 90364.69651061854, ewa inertia: 90364.69651061854\n",
      "Minibatch step 5/100: mean batch inertia: 88631.74955727976, ewa inertia: 88631.74955727976\n",
      "Minibatch step 6/100: mean batch inertia: 88353.96435248062, ewa inertia: 88353.96435248062\n",
      "Minibatch step 7/100: mean batch inertia: 89019.85765884095, ewa inertia: 89019.85765884095\n",
      "Minibatch step 8/100: mean batch inertia: 88184.59908227212, ewa inertia: 88184.59908227212\n",
      "Minibatch step 9/100: mean batch inertia: 89097.66750295844, ewa inertia: 89097.66750295844\n",
      "Minibatch step 10/100: mean batch inertia: 88086.69575761809, ewa inertia: 88086.69575761809\n",
      "Minibatch step 11/100: mean batch inertia: 87858.88551647797, ewa inertia: 87858.88551647797\n",
      "Minibatch step 12/100: mean batch inertia: 88130.99717013644, ewa inertia: 88130.99717013644\n",
      "Minibatch step 13/100: mean batch inertia: 87763.44387286833, ewa inertia: 87763.44387286833\n",
      "Minibatch step 14/100: mean batch inertia: 88357.16042954265, ewa inertia: 88357.16042954265\n",
      "Minibatch step 15/100: mean batch inertia: 88592.04605063178, ewa inertia: 88592.04605063178\n",
      "Minibatch step 16/100: mean batch inertia: 87652.74571357104, ewa inertia: 87652.74571357104\n",
      "Minibatch step 17/100: mean batch inertia: 88063.97677241, ewa inertia: 88063.97677241\n",
      "Minibatch step 18/100: mean batch inertia: 88178.25437964122, ewa inertia: 88178.25437964122\n",
      "Minibatch step 19/100: mean batch inertia: 87471.61081809616, ewa inertia: 87471.61081809616\n",
      "Minibatch step 20/100: mean batch inertia: 87883.23709324661, ewa inertia: 87883.23709324661\n",
      "Minibatch step 21/100: mean batch inertia: 87832.58146219539, ewa inertia: 87832.58146219539\n",
      "Minibatch step 22/100: mean batch inertia: 88341.74111325263, ewa inertia: 88341.74111325263\n",
      "Minibatch step 23/100: mean batch inertia: 87361.98537067573, ewa inertia: 87361.98537067573\n",
      "Minibatch step 24/100: mean batch inertia: 87366.48086503572, ewa inertia: 87366.48086503572\n",
      "Minibatch step 25/100: mean batch inertia: 88291.96909702482, ewa inertia: 88291.96909702482\n",
      "Minibatch step 26/100: mean batch inertia: 88035.24985573805, ewa inertia: 88035.24985573805\n",
      "Minibatch step 27/100: mean batch inertia: 88509.71107160275, ewa inertia: 88509.71107160275\n",
      "Minibatch step 28/100: mean batch inertia: 88068.55877896804, ewa inertia: 88068.55877896804\n",
      "Minibatch step 29/100: mean batch inertia: 87834.85541551249, ewa inertia: 87834.85541551249\n",
      "Minibatch step 30/100: mean batch inertia: 87505.54121376835, ewa inertia: 87505.54121376835\n",
      "Minibatch step 31/100: mean batch inertia: 88353.03104087994, ewa inertia: 88353.03104087994\n",
      "Minibatch step 32/100: mean batch inertia: 88383.15390547135, ewa inertia: 88383.15390547135\n",
      "Minibatch step 33/100: mean batch inertia: 87444.59993236701, ewa inertia: 87444.59993236701\n",
      "Converged (lack of improvement in inertia) at step 33/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 290094516.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 289787921.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 287686805.0\n",
      "Minibatch step 1/100: mean batch inertia: 137482.52752513165\n",
      "Minibatch step 2/100: mean batch inertia: 91326.69012835741, ewa inertia: 91326.69012835741\n",
      "Minibatch step 3/100: mean batch inertia: 89869.42891218173, ewa inertia: 89869.42891218173\n",
      "Minibatch step 4/100: mean batch inertia: 90503.86162197443, ewa inertia: 90503.86162197443\n",
      "Minibatch step 5/100: mean batch inertia: 89897.60839785408, ewa inertia: 89897.60839785408\n",
      "Minibatch step 6/100: mean batch inertia: 89321.58770562062, ewa inertia: 89321.58770562062\n",
      "Minibatch step 7/100: mean batch inertia: 88301.14765053567, ewa inertia: 88301.14765053567\n",
      "Minibatch step 8/100: mean batch inertia: 87844.30806116496, ewa inertia: 87844.30806116496\n",
      "Minibatch step 9/100: mean batch inertia: 89066.652212564, ewa inertia: 89066.652212564\n",
      "Minibatch step 10/100: mean batch inertia: 89212.10488159316, ewa inertia: 89212.10488159316\n",
      "Minibatch step 11/100: mean batch inertia: 88202.71594081861, ewa inertia: 88202.71594081861\n",
      "Minibatch step 12/100: mean batch inertia: 88187.36578679034, ewa inertia: 88187.36578679034\n",
      "Minibatch step 13/100: mean batch inertia: 88400.19831793806, ewa inertia: 88400.19831793806\n",
      "Minibatch step 14/100: mean batch inertia: 88060.20352770078, ewa inertia: 88060.20352770078\n",
      "Minibatch step 15/100: mean batch inertia: 88152.76011062982, ewa inertia: 88152.76011062982\n",
      "Minibatch step 16/100: mean batch inertia: 88456.0161612042, ewa inertia: 88456.0161612042\n",
      "Minibatch step 17/100: mean batch inertia: 87236.51604903614, ewa inertia: 87236.51604903614\n",
      "Minibatch step 18/100: mean batch inertia: 88209.95114402493, ewa inertia: 88209.95114402493\n",
      "Minibatch step 19/100: mean batch inertia: 89020.78150653887, ewa inertia: 89020.78150653887\n",
      "Minibatch step 20/100: mean batch inertia: 88729.62837780442, ewa inertia: 88729.62837780442\n",
      "Minibatch step 21/100: mean batch inertia: 88152.83266602142, ewa inertia: 88152.83266602142\n",
      "Minibatch step 22/100: mean batch inertia: 88437.33071136534, ewa inertia: 88437.33071136534\n",
      "Minibatch step 23/100: mean batch inertia: 88093.3178098392, ewa inertia: 88093.3178098392\n",
      "Minibatch step 24/100: mean batch inertia: 87379.57993866375, ewa inertia: 87379.57993866375\n",
      "Minibatch step 25/100: mean batch inertia: 87823.02365286519, ewa inertia: 87823.02365286519\n",
      "Minibatch step 26/100: mean batch inertia: 87943.11160369053, ewa inertia: 87943.11160369053\n",
      "Minibatch step 27/100: mean batch inertia: 87962.99333181119, ewa inertia: 87962.99333181119\n",
      "Converged (lack of improvement in inertia) at step 27/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 291144905.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 287004233.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 285765977.0\n",
      "Minibatch step 1/100: mean batch inertia: 137145.33317376734\n",
      "Minibatch step 2/100: mean batch inertia: 92854.05226556977, ewa inertia: 92854.05226556977\n",
      "Minibatch step 3/100: mean batch inertia: 90264.91441180793, ewa inertia: 90264.91441180793\n",
      "Minibatch step 4/100: mean batch inertia: 89801.17039382568, ewa inertia: 89801.17039382568\n",
      "Minibatch step 5/100: mean batch inertia: 89472.67083116106, ewa inertia: 89472.67083116106\n",
      "Minibatch step 6/100: mean batch inertia: 88689.62854156471, ewa inertia: 88689.62854156471\n",
      "Minibatch step 7/100: mean batch inertia: 89366.68319189007, ewa inertia: 89366.68319189007\n",
      "Minibatch step 8/100: mean batch inertia: 88684.28480200049, ewa inertia: 88684.28480200049\n",
      "Minibatch step 9/100: mean batch inertia: 88312.06218168036, ewa inertia: 88312.06218168036\n",
      "Minibatch step 10/100: mean batch inertia: 88085.75335751801, ewa inertia: 88085.75335751801\n",
      "Minibatch step 11/100: mean batch inertia: 88090.55302771342, ewa inertia: 88090.55302771342\n",
      "Minibatch step 12/100: mean batch inertia: 89036.32406449689, ewa inertia: 89036.32406449689\n",
      "Minibatch step 13/100: mean batch inertia: 88332.25225107495, ewa inertia: 88332.25225107495\n",
      "Minibatch step 14/100: mean batch inertia: 89082.02139170002, ewa inertia: 89082.02139170002\n",
      "Minibatch step 15/100: mean batch inertia: 88331.69674469787, ewa inertia: 88331.69674469787\n",
      "Minibatch step 16/100: mean batch inertia: 88519.96345265761, ewa inertia: 88519.96345265761\n",
      "Minibatch step 17/100: mean batch inertia: 88109.52514477857, ewa inertia: 88109.52514477857\n",
      "Minibatch step 18/100: mean batch inertia: 88520.45472615348, ewa inertia: 88520.45472615348\n",
      "Minibatch step 19/100: mean batch inertia: 88426.40286610804, ewa inertia: 88426.40286610804\n",
      "Minibatch step 20/100: mean batch inertia: 88194.40151259323, ewa inertia: 88194.40151259323\n",
      "Converged (lack of improvement in inertia) at step 20/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 284589921.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 286242553.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 292588252.0\n",
      "Minibatch step 1/100: mean batch inertia: 137254.073719483\n",
      "Minibatch step 2/100: mean batch inertia: 92071.92777098565, ewa inertia: 92071.92777098565\n",
      "Minibatch step 3/100: mean batch inertia: 91223.90046926138, ewa inertia: 91223.90046926138\n",
      "Minibatch step 4/100: mean batch inertia: 89656.02176311446, ewa inertia: 89656.02176311446\n",
      "Minibatch step 5/100: mean batch inertia: 88710.90038557287, ewa inertia: 88710.90038557287\n",
      "Minibatch step 6/100: mean batch inertia: 88958.45026407012, ewa inertia: 88958.45026407012\n",
      "Minibatch step 7/100: mean batch inertia: 89025.89623848416, ewa inertia: 89025.89623848416\n",
      "Minibatch step 8/100: mean batch inertia: 89262.28150180135, ewa inertia: 89262.28150180135\n",
      "Minibatch step 9/100: mean batch inertia: 89151.67427949088, ewa inertia: 89151.67427949088\n",
      "Minibatch step 10/100: mean batch inertia: 88409.49578383414, ewa inertia: 88409.49578383414\n",
      "Minibatch step 11/100: mean batch inertia: 88451.07708732419, ewa inertia: 88451.07708732419\n",
      "Minibatch step 12/100: mean batch inertia: 88675.18109053216, ewa inertia: 88675.18109053216\n",
      "Minibatch step 13/100: mean batch inertia: 87930.53117329846, ewa inertia: 87930.53117329846\n",
      "Minibatch step 14/100: mean batch inertia: 88431.91489901017, ewa inertia: 88431.91489901017\n",
      "Minibatch step 15/100: mean batch inertia: 88966.6988137704, ewa inertia: 88966.6988137704\n",
      "Minibatch step 16/100: mean batch inertia: 88905.71159047939, ewa inertia: 88905.71159047939\n",
      "Minibatch step 17/100: mean batch inertia: 88128.73113593068, ewa inertia: 88128.73113593068\n",
      "Minibatch step 18/100: mean batch inertia: 88557.40778578556, ewa inertia: 88557.40778578556\n",
      "Minibatch step 19/100: mean batch inertia: 88468.00026040555, ewa inertia: 88468.00026040555\n",
      "Minibatch step 20/100: mean batch inertia: 88468.78081456965, ewa inertia: 88468.78081456965\n",
      "Minibatch step 21/100: mean batch inertia: 88784.32488757739, ewa inertia: 88784.32488757739\n",
      "Minibatch step 22/100: mean batch inertia: 89210.89463789345, ewa inertia: 89210.89463789345\n",
      "Minibatch step 23/100: mean batch inertia: 87342.21033290411, ewa inertia: 87342.21033290411\n",
      "Minibatch step 24/100: mean batch inertia: 87737.91211412977, ewa inertia: 87737.91211412977\n",
      "Minibatch step 25/100: mean batch inertia: 88517.10552727942, ewa inertia: 88517.10552727942\n",
      "Minibatch step 26/100: mean batch inertia: 87639.78027914483, ewa inertia: 87639.78027914483\n",
      "Minibatch step 27/100: mean batch inertia: 87952.66102365809, ewa inertia: 87952.66102365809\n",
      "Minibatch step 28/100: mean batch inertia: 88666.37609179539, ewa inertia: 88666.37609179539\n",
      "Minibatch step 29/100: mean batch inertia: 88246.76999670373, ewa inertia: 88246.76999670373\n",
      "Minibatch step 30/100: mean batch inertia: 88207.87891068666, ewa inertia: 88207.87891068666\n",
      "Minibatch step 31/100: mean batch inertia: 87884.15414768888, ewa inertia: 87884.15414768888\n",
      "Minibatch step 32/100: mean batch inertia: 88655.45953882331, ewa inertia: 88655.45953882331\n",
      "Minibatch step 33/100: mean batch inertia: 88057.21630410131, ewa inertia: 88057.21630410131\n",
      "Converged (lack of improvement in inertia) at step 33/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 288385976.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 286518330.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 290627477.0\n",
      "Minibatch step 1/100: mean batch inertia: 137753.1416945907\n",
      "Minibatch step 2/100: mean batch inertia: 91800.80291509944, ewa inertia: 91800.80291509944\n",
      "Minibatch step 3/100: mean batch inertia: 90329.39230129578, ewa inertia: 90329.39230129578\n",
      "Minibatch step 4/100: mean batch inertia: 89669.9597004948, ewa inertia: 89669.9597004948\n",
      "Minibatch step 5/100: mean batch inertia: 88132.3499663287, ewa inertia: 88132.3499663287\n",
      "Minibatch step 6/100: mean batch inertia: 88881.51803582688, ewa inertia: 88881.51803582688\n",
      "Minibatch step 7/100: mean batch inertia: 89260.48171531716, ewa inertia: 89260.48171531716\n",
      "Minibatch step 8/100: mean batch inertia: 88639.23727034754, ewa inertia: 88639.23727034754\n",
      "Minibatch step 9/100: mean batch inertia: 88416.41719836152, ewa inertia: 88416.41719836152\n",
      "Minibatch step 10/100: mean batch inertia: 87886.5555128476, ewa inertia: 87886.5555128476\n",
      "Minibatch step 11/100: mean batch inertia: 88409.38200005578, ewa inertia: 88409.38200005578\n",
      "Minibatch step 12/100: mean batch inertia: 87494.26003384717, ewa inertia: 87494.26003384717\n",
      "Minibatch step 13/100: mean batch inertia: 88075.00736458915, ewa inertia: 88075.00736458915\n",
      "Minibatch step 14/100: mean batch inertia: 87962.15726208175, ewa inertia: 87962.15726208175\n",
      "Minibatch step 15/100: mean batch inertia: 88140.88140073935, ewa inertia: 88140.88140073935\n",
      "Minibatch step 16/100: mean batch inertia: 87875.39148594196, ewa inertia: 87875.39148594196\n",
      "Minibatch step 17/100: mean batch inertia: 88561.34111042235, ewa inertia: 88561.34111042235\n",
      "Minibatch step 18/100: mean batch inertia: 87311.17306547464, ewa inertia: 87311.17306547464\n",
      "Minibatch step 19/100: mean batch inertia: 88334.45496242438, ewa inertia: 88334.45496242438\n",
      "Minibatch step 20/100: mean batch inertia: 87525.05085818688, ewa inertia: 87525.05085818688\n",
      "Minibatch step 21/100: mean batch inertia: 87835.2390741935, ewa inertia: 87835.2390741935\n",
      "Minibatch step 22/100: mean batch inertia: 88219.91802659459, ewa inertia: 88219.91802659459\n",
      "Minibatch step 23/100: mean batch inertia: 87284.43982239177, ewa inertia: 87284.43982239177\n",
      "Minibatch step 24/100: mean batch inertia: 88056.14857839205, ewa inertia: 88056.14857839205\n",
      "Minibatch step 25/100: mean batch inertia: 87516.68978273559, ewa inertia: 87516.68978273559\n",
      "Minibatch step 26/100: mean batch inertia: 87707.04205692797, ewa inertia: 87707.04205692797\n",
      "Minibatch step 27/100: mean batch inertia: 88255.09898374272, ewa inertia: 88255.09898374272\n",
      "Minibatch step 28/100: mean batch inertia: 87680.28128088273, ewa inertia: 87680.28128088273\n",
      "Minibatch step 29/100: mean batch inertia: 87474.8619553357, ewa inertia: 87474.8619553357\n",
      "Minibatch step 30/100: mean batch inertia: 86893.20421714603, ewa inertia: 86893.20421714603\n",
      "Minibatch step 31/100: mean batch inertia: 88210.10127134978, ewa inertia: 88210.10127134978\n",
      "Minibatch step 32/100: mean batch inertia: 87967.2565692932, ewa inertia: 87967.2565692932\n",
      "Minibatch step 33/100: mean batch inertia: 87079.01615947283, ewa inertia: 87079.01615947283\n",
      "Minibatch step 34/100: mean batch inertia: 87911.06084412451, ewa inertia: 87911.06084412451\n",
      "Minibatch step 35/100: mean batch inertia: 87150.62755348238, ewa inertia: 87150.62755348238\n",
      "Minibatch step 36/100: mean batch inertia: 88414.20684087387, ewa inertia: 88414.20684087387\n",
      "Minibatch step 37/100: mean batch inertia: 87176.67867617794, ewa inertia: 87176.67867617794\n",
      "Minibatch step 38/100: mean batch inertia: 88084.77079095456, ewa inertia: 88084.77079095456\n",
      "Minibatch step 39/100: mean batch inertia: 87990.06894322162, ewa inertia: 87990.06894322162\n",
      "Minibatch step 40/100: mean batch inertia: 87891.77215237079, ewa inertia: 87891.77215237079\n",
      "Converged (lack of improvement in inertia) at step 40/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 289621293.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 281987887.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 290178181.0\n",
      "Minibatch step 1/100: mean batch inertia: 133783.5753949258\n",
      "Minibatch step 2/100: mean batch inertia: 91431.49306446059, ewa inertia: 91431.49306446059\n",
      "Minibatch step 3/100: mean batch inertia: 90467.08505692621, ewa inertia: 90467.08505692621\n",
      "Minibatch step 4/100: mean batch inertia: 89612.79645692492, ewa inertia: 89612.79645692492\n",
      "Minibatch step 5/100: mean batch inertia: 89324.76728488338, ewa inertia: 89324.76728488338\n",
      "Minibatch step 6/100: mean batch inertia: 88310.22245100123, ewa inertia: 88310.22245100123\n",
      "Minibatch step 7/100: mean batch inertia: 88984.04289271917, ewa inertia: 88984.04289271917\n",
      "Minibatch step 8/100: mean batch inertia: 89670.17020087203, ewa inertia: 89670.17020087203\n",
      "Minibatch step 9/100: mean batch inertia: 88777.99528699744, ewa inertia: 88777.99528699744\n",
      "Minibatch step 10/100: mean batch inertia: 88242.76625019264, ewa inertia: 88242.76625019264\n",
      "Minibatch step 11/100: mean batch inertia: 88379.90685931315, ewa inertia: 88379.90685931315\n",
      "Minibatch step 12/100: mean batch inertia: 87523.20795282748, ewa inertia: 87523.20795282748\n",
      "Minibatch step 13/100: mean batch inertia: 86888.17687887973, ewa inertia: 86888.17687887973\n",
      "Minibatch step 14/100: mean batch inertia: 88760.65814041768, ewa inertia: 88760.65814041768\n",
      "Minibatch step 15/100: mean batch inertia: 86942.56334224925, ewa inertia: 86942.56334224925\n",
      "Minibatch step 16/100: mean batch inertia: 88438.13216479884, ewa inertia: 88438.13216479884\n",
      "Minibatch step 17/100: mean batch inertia: 88136.15792486478, ewa inertia: 88136.15792486478\n",
      "Minibatch step 18/100: mean batch inertia: 88211.21269256184, ewa inertia: 88211.21269256184\n",
      "Minibatch step 19/100: mean batch inertia: 88238.7846199743, ewa inertia: 88238.7846199743\n",
      "Minibatch step 20/100: mean batch inertia: 87514.0145009384, ewa inertia: 87514.0145009384\n",
      "Minibatch step 21/100: mean batch inertia: 88221.21977550545, ewa inertia: 88221.21977550545\n",
      "Minibatch step 22/100: mean batch inertia: 88038.24078602507, ewa inertia: 88038.24078602507\n",
      "Minibatch step 23/100: mean batch inertia: 87003.71674386089, ewa inertia: 87003.71674386089\n",
      "Converged (lack of improvement in inertia) at step 23/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 289247069.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 285371968.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 287155615.0\n",
      "Minibatch step 1/100: mean batch inertia: 137408.50263283867\n",
      "Minibatch step 2/100: mean batch inertia: 92348.12862367659, ewa inertia: 92348.12862367659\n",
      "Minibatch step 3/100: mean batch inertia: 90531.11398697953, ewa inertia: 90531.11398697953\n",
      "Minibatch step 4/100: mean batch inertia: 88364.61687939147, ewa inertia: 88364.61687939147\n",
      "Minibatch step 5/100: mean batch inertia: 89122.29761838025, ewa inertia: 89122.29761838025\n",
      "Minibatch step 6/100: mean batch inertia: 88166.23358685743, ewa inertia: 88166.23358685743\n",
      "Minibatch step 7/100: mean batch inertia: 89640.68645661174, ewa inertia: 89640.68645661174\n",
      "Minibatch step 8/100: mean batch inertia: 89505.90694187199, ewa inertia: 89505.90694187199\n",
      "Minibatch step 9/100: mean batch inertia: 88288.56952842811, ewa inertia: 88288.56952842811\n",
      "Minibatch step 10/100: mean batch inertia: 88929.2084704064, ewa inertia: 88929.2084704064\n",
      "Minibatch step 11/100: mean batch inertia: 88546.12118637159, ewa inertia: 88546.12118637159\n",
      "Minibatch step 12/100: mean batch inertia: 88199.98398285364, ewa inertia: 88199.98398285364\n",
      "Minibatch step 13/100: mean batch inertia: 88035.78255161486, ewa inertia: 88035.78255161486\n",
      "Minibatch step 14/100: mean batch inertia: 88693.07719038076, ewa inertia: 88693.07719038076\n",
      "Minibatch step 15/100: mean batch inertia: 88920.85585097817, ewa inertia: 88920.85585097817\n",
      "Minibatch step 16/100: mean batch inertia: 87667.30810270167, ewa inertia: 87667.30810270167\n",
      "Minibatch step 17/100: mean batch inertia: 88496.99672228117, ewa inertia: 88496.99672228117\n",
      "Minibatch step 18/100: mean batch inertia: 86931.28878172318, ewa inertia: 86931.28878172318\n",
      "Minibatch step 19/100: mean batch inertia: 87862.69407385585, ewa inertia: 87862.69407385585\n",
      "Minibatch step 20/100: mean batch inertia: 88119.98551446493, ewa inertia: 88119.98551446493\n",
      "Minibatch step 21/100: mean batch inertia: 88413.93002375547, ewa inertia: 88413.93002375547\n",
      "Minibatch step 22/100: mean batch inertia: 88480.52955865358, ewa inertia: 88480.52955865358\n",
      "Minibatch step 23/100: mean batch inertia: 87720.03359790963, ewa inertia: 87720.03359790963\n",
      "Minibatch step 24/100: mean batch inertia: 87960.47946948388, ewa inertia: 87960.47946948388\n",
      "Minibatch step 25/100: mean batch inertia: 87285.08082163366, ewa inertia: 87285.08082163366\n",
      "Minibatch step 26/100: mean batch inertia: 88426.52732860207, ewa inertia: 88426.52732860207\n",
      "Minibatch step 27/100: mean batch inertia: 88595.53601651742, ewa inertia: 88595.53601651742\n",
      "Minibatch step 28/100: mean batch inertia: 87630.43469880894, ewa inertia: 87630.43469880894\n",
      "Converged (lack of improvement in inertia) at step 28/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 281975340.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 286841639.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 286113833.0\n",
      "Minibatch step 1/100: mean batch inertia: 136755.270464337\n",
      "Minibatch step 2/100: mean batch inertia: 90833.33186858262, ewa inertia: 90833.33186858262\n",
      "Minibatch step 3/100: mean batch inertia: 89216.17177501136, ewa inertia: 89216.17177501136\n",
      "Minibatch step 4/100: mean batch inertia: 89244.5059233639, ewa inertia: 89244.5059233639\n",
      "Minibatch step 5/100: mean batch inertia: 89803.92399729507, ewa inertia: 89803.92399729507\n",
      "Minibatch step 6/100: mean batch inertia: 88963.1619192075, ewa inertia: 88963.1619192075\n",
      "Minibatch step 7/100: mean batch inertia: 87839.21181041587, ewa inertia: 87839.21181041587\n",
      "Minibatch step 8/100: mean batch inertia: 87675.60596921941, ewa inertia: 87675.60596921941\n",
      "Minibatch step 9/100: mean batch inertia: 88676.76634729389, ewa inertia: 88676.76634729389\n",
      "Minibatch step 10/100: mean batch inertia: 88088.15511018685, ewa inertia: 88088.15511018685\n",
      "Minibatch step 11/100: mean batch inertia: 88461.71708894914, ewa inertia: 88461.71708894914\n",
      "Minibatch step 12/100: mean batch inertia: 87802.52466662612, ewa inertia: 87802.52466662612\n",
      "Minibatch step 13/100: mean batch inertia: 88088.71107202265, ewa inertia: 88088.71107202265\n",
      "Minibatch step 14/100: mean batch inertia: 88415.63557368178, ewa inertia: 88415.63557368178\n",
      "Minibatch step 15/100: mean batch inertia: 87650.89422206895, ewa inertia: 87650.89422206895\n",
      "Minibatch step 16/100: mean batch inertia: 89191.95187703684, ewa inertia: 89191.95187703684\n",
      "Minibatch step 17/100: mean batch inertia: 87834.28032153304, ewa inertia: 87834.28032153304\n",
      "Minibatch step 18/100: mean batch inertia: 87497.74510707225, ewa inertia: 87497.74510707225\n",
      "Minibatch step 19/100: mean batch inertia: 87712.81978628083, ewa inertia: 87712.81978628083\n",
      "Minibatch step 20/100: mean batch inertia: 88036.57471094292, ewa inertia: 88036.57471094292\n",
      "Minibatch step 21/100: mean batch inertia: 88064.48565346024, ewa inertia: 88064.48565346024\n",
      "Minibatch step 22/100: mean batch inertia: 88663.39327149432, ewa inertia: 88663.39327149432\n",
      "Minibatch step 23/100: mean batch inertia: 89053.19523030538, ewa inertia: 89053.19523030538\n",
      "Minibatch step 24/100: mean batch inertia: 88517.41327535083, ewa inertia: 88517.41327535083\n",
      "Minibatch step 25/100: mean batch inertia: 88014.74022960014, ewa inertia: 88014.74022960014\n",
      "Minibatch step 26/100: mean batch inertia: 87804.717078348, ewa inertia: 87804.717078348\n",
      "Minibatch step 27/100: mean batch inertia: 88388.9137936962, ewa inertia: 88388.9137936962\n",
      "Minibatch step 28/100: mean batch inertia: 88315.4267431678, ewa inertia: 88315.4267431678\n",
      "Converged (lack of improvement in inertia) at step 28/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 293339209.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 290775822.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 289575386.0\n",
      "Minibatch step 1/100: mean batch inertia: 137700.10674964098\n",
      "Minibatch step 2/100: mean batch inertia: 91410.57647416476, ewa inertia: 91410.57647416476\n",
      "Minibatch step 3/100: mean batch inertia: 90699.61938657926, ewa inertia: 90699.61938657926\n",
      "Minibatch step 4/100: mean batch inertia: 89582.96619829396, ewa inertia: 89582.96619829396\n",
      "Minibatch step 5/100: mean batch inertia: 88412.34884691285, ewa inertia: 88412.34884691285\n",
      "Minibatch step 6/100: mean batch inertia: 88587.67087536302, ewa inertia: 88587.67087536302\n",
      "Minibatch step 7/100: mean batch inertia: 88270.60867650213, ewa inertia: 88270.60867650213\n",
      "Minibatch step 8/100: mean batch inertia: 87996.73533845962, ewa inertia: 87996.73533845962\n",
      "Minibatch step 9/100: mean batch inertia: 87650.5351780118, ewa inertia: 87650.5351780118\n",
      "Minibatch step 10/100: mean batch inertia: 87908.79532939375, ewa inertia: 87908.79532939375\n",
      "Minibatch step 11/100: mean batch inertia: 88488.6912247113, ewa inertia: 88488.6912247113\n",
      "Minibatch step 12/100: mean batch inertia: 88166.12059651609, ewa inertia: 88166.12059651609\n",
      "Minibatch step 13/100: mean batch inertia: 88278.3462350528, ewa inertia: 88278.3462350528\n",
      "Minibatch step 14/100: mean batch inertia: 88292.680094021, ewa inertia: 88292.680094021\n",
      "Minibatch step 15/100: mean batch inertia: 88858.11526086021, ewa inertia: 88858.11526086021\n",
      "Minibatch step 16/100: mean batch inertia: 87439.7734844675, ewa inertia: 87439.7734844675\n",
      "Minibatch step 17/100: mean batch inertia: 87393.97610749029, ewa inertia: 87393.97610749029\n",
      "Minibatch step 18/100: mean batch inertia: 88168.70583348774, ewa inertia: 88168.70583348774\n",
      "Minibatch step 19/100: mean batch inertia: 87345.44709701465, ewa inertia: 87345.44709701465\n",
      "Minibatch step 20/100: mean batch inertia: 87871.25393214771, ewa inertia: 87871.25393214771\n",
      "Minibatch step 21/100: mean batch inertia: 88495.81727871117, ewa inertia: 88495.81727871117\n",
      "Minibatch step 22/100: mean batch inertia: 87724.84687346322, ewa inertia: 87724.84687346322\n",
      "Minibatch step 23/100: mean batch inertia: 87815.40890390062, ewa inertia: 87815.40890390062\n",
      "Minibatch step 24/100: mean batch inertia: 88038.57986970097, ewa inertia: 88038.57986970097\n",
      "Minibatch step 25/100: mean batch inertia: 88227.8592096888, ewa inertia: 88227.8592096888\n",
      "Minibatch step 26/100: mean batch inertia: 88054.0658556435, ewa inertia: 88054.0658556435\n",
      "Minibatch step 27/100: mean batch inertia: 87905.77513807437, ewa inertia: 87905.77513807437\n",
      "Minibatch step 28/100: mean batch inertia: 87686.19669382856, ewa inertia: 87686.19669382856\n",
      "Minibatch step 29/100: mean batch inertia: 88300.31864457918, ewa inertia: 88300.31864457918\n",
      "Converged (lack of improvement in inertia) at step 29/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 282406026.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 290761718.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 284958069.0\n",
      "Minibatch step 1/100: mean batch inertia: 136930.95117280996\n",
      "Minibatch step 2/100: mean batch inertia: 91473.89275044097, ewa inertia: 91473.89275044097\n",
      "Minibatch step 3/100: mean batch inertia: 90691.61011050109, ewa inertia: 90691.61011050109\n",
      "Minibatch step 4/100: mean batch inertia: 90133.81138413447, ewa inertia: 90133.81138413447\n",
      "Minibatch step 5/100: mean batch inertia: 88972.99181948758, ewa inertia: 88972.99181948758\n",
      "Minibatch step 6/100: mean batch inertia: 89848.2657080337, ewa inertia: 89848.2657080337\n",
      "Minibatch step 7/100: mean batch inertia: 89505.45297038827, ewa inertia: 89505.45297038827\n",
      "Minibatch step 8/100: mean batch inertia: 88727.91588861865, ewa inertia: 88727.91588861865\n",
      "Minibatch step 9/100: mean batch inertia: 87975.63536447428, ewa inertia: 87975.63536447428\n",
      "Minibatch step 10/100: mean batch inertia: 88079.15961015361, ewa inertia: 88079.15961015361\n",
      "Minibatch step 11/100: mean batch inertia: 88402.46056486241, ewa inertia: 88402.46056486241\n",
      "Minibatch step 12/100: mean batch inertia: 88107.63586699835, ewa inertia: 88107.63586699835\n",
      "Minibatch step 13/100: mean batch inertia: 89052.35827295478, ewa inertia: 89052.35827295478\n",
      "Minibatch step 14/100: mean batch inertia: 89285.08769748712, ewa inertia: 89285.08769748712\n",
      "Minibatch step 15/100: mean batch inertia: 87651.63725301612, ewa inertia: 87651.63725301612\n",
      "Minibatch step 16/100: mean batch inertia: 87886.53380549725, ewa inertia: 87886.53380549725\n",
      "Minibatch step 17/100: mean batch inertia: 88041.17993289516, ewa inertia: 88041.17993289516\n",
      "Minibatch step 18/100: mean batch inertia: 88215.99507559297, ewa inertia: 88215.99507559297\n",
      "Minibatch step 19/100: mean batch inertia: 88484.14279999599, ewa inertia: 88484.14279999599\n",
      "Minibatch step 20/100: mean batch inertia: 88187.3859354562, ewa inertia: 88187.3859354562\n",
      "Minibatch step 21/100: mean batch inertia: 88240.90951479798, ewa inertia: 88240.90951479798\n",
      "Minibatch step 22/100: mean batch inertia: 87715.58067322266, ewa inertia: 87715.58067322266\n",
      "Minibatch step 23/100: mean batch inertia: 87950.28263768097, ewa inertia: 87950.28263768097\n",
      "Minibatch step 24/100: mean batch inertia: 87693.58388235509, ewa inertia: 87693.58388235509\n",
      "Minibatch step 25/100: mean batch inertia: 88121.9073847623, ewa inertia: 88121.9073847623\n",
      "Converged (lack of improvement in inertia) at step 25/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 287739630.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 296673843.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 296168978.0\n",
      "Minibatch step 1/100: mean batch inertia: 136458.99138343704\n",
      "Minibatch step 2/100: mean batch inertia: 91619.20471484259, ewa inertia: 91619.20471484259\n",
      "Minibatch step 3/100: mean batch inertia: 90027.76808462452, ewa inertia: 90027.76808462452\n",
      "Minibatch step 4/100: mean batch inertia: 89202.4249494724, ewa inertia: 89202.4249494724\n",
      "Minibatch step 5/100: mean batch inertia: 88894.61154610128, ewa inertia: 88894.61154610128\n",
      "Minibatch step 6/100: mean batch inertia: 88477.8228144067, ewa inertia: 88477.8228144067\n",
      "Minibatch step 7/100: mean batch inertia: 88182.84624156344, ewa inertia: 88182.84624156344\n",
      "Minibatch step 8/100: mean batch inertia: 89175.14362486781, ewa inertia: 89175.14362486781\n",
      "Minibatch step 9/100: mean batch inertia: 89177.2418336143, ewa inertia: 89177.2418336143\n",
      "Minibatch step 10/100: mean batch inertia: 87784.52852314072, ewa inertia: 87784.52852314072\n",
      "Minibatch step 11/100: mean batch inertia: 88132.01883708072, ewa inertia: 88132.01883708072\n",
      "Minibatch step 12/100: mean batch inertia: 87874.96906823775, ewa inertia: 87874.96906823775\n",
      "Minibatch step 13/100: mean batch inertia: 87303.95745577409, ewa inertia: 87303.95745577409\n",
      "Minibatch step 14/100: mean batch inertia: 88029.7901773946, ewa inertia: 88029.7901773946\n",
      "Minibatch step 15/100: mean batch inertia: 88347.23684131449, ewa inertia: 88347.23684131449\n",
      "Minibatch step 16/100: mean batch inertia: 87078.03426566331, ewa inertia: 87078.03426566331\n",
      "Minibatch step 17/100: mean batch inertia: 88520.03239328097, ewa inertia: 88520.03239328097\n",
      "Minibatch step 18/100: mean batch inertia: 87996.347006164, ewa inertia: 87996.347006164\n",
      "Minibatch step 19/100: mean batch inertia: 87335.91027549059, ewa inertia: 87335.91027549059\n",
      "Minibatch step 20/100: mean batch inertia: 87595.42839213237, ewa inertia: 87595.42839213237\n",
      "Minibatch step 21/100: mean batch inertia: 88145.2549359439, ewa inertia: 88145.2549359439\n",
      "Minibatch step 22/100: mean batch inertia: 87862.79770074828, ewa inertia: 87862.79770074828\n",
      "Minibatch step 23/100: mean batch inertia: 87981.60792593275, ewa inertia: 87981.60792593275\n",
      "Minibatch step 24/100: mean batch inertia: 87916.26162652747, ewa inertia: 87916.26162652747\n",
      "Minibatch step 25/100: mean batch inertia: 87857.75533577442, ewa inertia: 87857.75533577442\n",
      "Minibatch step 26/100: mean batch inertia: 87569.9779021095, ewa inertia: 87569.9779021095\n",
      "Converged (lack of improvement in inertia) at step 26/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 282988516.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 287417615.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 284913004.0\n",
      "Minibatch step 1/100: mean batch inertia: 134245.3106749641\n",
      "Minibatch step 2/100: mean batch inertia: 91907.02823824348, ewa inertia: 91907.02823824348\n",
      "Minibatch step 3/100: mean batch inertia: 88709.35694059469, ewa inertia: 88709.35694059469\n",
      "Minibatch step 4/100: mean batch inertia: 89146.34479643124, ewa inertia: 89146.34479643124\n",
      "Minibatch step 5/100: mean batch inertia: 88774.75453053691, ewa inertia: 88774.75453053691\n",
      "Minibatch step 6/100: mean batch inertia: 88059.42524072337, ewa inertia: 88059.42524072337\n",
      "Minibatch step 7/100: mean batch inertia: 88380.91708561094, ewa inertia: 88380.91708561094\n",
      "Minibatch step 8/100: mean batch inertia: 88260.17257036731, ewa inertia: 88260.17257036731\n",
      "Minibatch step 9/100: mean batch inertia: 87803.2997919694, ewa inertia: 87803.2997919694\n",
      "Minibatch step 10/100: mean batch inertia: 88336.08217580689, ewa inertia: 88336.08217580689\n",
      "Minibatch step 11/100: mean batch inertia: 87479.86702285842, ewa inertia: 87479.86702285842\n",
      "Minibatch step 12/100: mean batch inertia: 87469.79148043855, ewa inertia: 87469.79148043855\n",
      "Minibatch step 13/100: mean batch inertia: 88057.59671043222, ewa inertia: 88057.59671043222\n",
      "Minibatch step 14/100: mean batch inertia: 87307.41415738239, ewa inertia: 87307.41415738239\n",
      "Minibatch step 15/100: mean batch inertia: 87584.05318041578, ewa inertia: 87584.05318041578\n",
      "Minibatch step 16/100: mean batch inertia: 87854.81558165787, ewa inertia: 87854.81558165787\n",
      "Minibatch step 17/100: mean batch inertia: 88300.98506553334, ewa inertia: 88300.98506553334\n",
      "Minibatch step 18/100: mean batch inertia: 87285.55226722776, ewa inertia: 87285.55226722776\n",
      "Minibatch step 19/100: mean batch inertia: 87920.0806857932, ewa inertia: 87920.0806857932\n",
      "Minibatch step 20/100: mean batch inertia: 87671.23767767918, ewa inertia: 87671.23767767918\n",
      "Minibatch step 21/100: mean batch inertia: 88538.78234457102, ewa inertia: 88538.78234457102\n",
      "Minibatch step 22/100: mean batch inertia: 87353.27661661357, ewa inertia: 87353.27661661357\n",
      "Minibatch step 23/100: mean batch inertia: 87546.15028472825, ewa inertia: 87546.15028472825\n",
      "Minibatch step 24/100: mean batch inertia: 88223.26423353763, ewa inertia: 88223.26423353763\n",
      "Minibatch step 25/100: mean batch inertia: 87029.11481848081, ewa inertia: 87029.11481848081\n",
      "Minibatch step 26/100: mean batch inertia: 87230.15246626832, ewa inertia: 87230.15246626832\n",
      "Minibatch step 27/100: mean batch inertia: 87575.30355324096, ewa inertia: 87575.30355324096\n",
      "Minibatch step 28/100: mean batch inertia: 88491.14556429759, ewa inertia: 88491.14556429759\n",
      "Minibatch step 29/100: mean batch inertia: 88394.10836296571, ewa inertia: 88394.10836296571\n",
      "Minibatch step 30/100: mean batch inertia: 87044.12587349511, ewa inertia: 87044.12587349511\n",
      "Minibatch step 31/100: mean batch inertia: 88640.06801428848, ewa inertia: 88640.06801428848\n",
      "Minibatch step 32/100: mean batch inertia: 87038.10656040104, ewa inertia: 87038.10656040104\n",
      "Minibatch step 33/100: mean batch inertia: 86825.19073552766, ewa inertia: 86825.19073552766\n",
      "Minibatch step 34/100: mean batch inertia: 87465.96209238782, ewa inertia: 87465.96209238782\n",
      "Minibatch step 35/100: mean batch inertia: 87611.20573233707, ewa inertia: 87611.20573233707\n",
      "Minibatch step 36/100: mean batch inertia: 87739.69823511262, ewa inertia: 87739.69823511262\n",
      "Minibatch step 37/100: mean batch inertia: 86635.85302289578, ewa inertia: 86635.85302289578\n",
      "Minibatch step 38/100: mean batch inertia: 87483.06165117715, ewa inertia: 87483.06165117715\n",
      "Minibatch step 39/100: mean batch inertia: 87732.9963644919, ewa inertia: 87732.9963644919\n",
      "Minibatch step 40/100: mean batch inertia: 87440.43482263992, ewa inertia: 87440.43482263992\n",
      "Minibatch step 41/100: mean batch inertia: 88298.46573592108, ewa inertia: 88298.46573592108\n",
      "Minibatch step 42/100: mean batch inertia: 87549.46301181408, ewa inertia: 87549.46301181408\n",
      "Minibatch step 43/100: mean batch inertia: 87779.68865548016, ewa inertia: 87779.68865548016\n",
      "Minibatch step 44/100: mean batch inertia: 87544.9976670643, ewa inertia: 87544.9976670643\n",
      "Minibatch step 45/100: mean batch inertia: 86882.47866919829, ewa inertia: 86882.47866919829\n",
      "Minibatch step 46/100: mean batch inertia: 88063.40552057148, ewa inertia: 88063.40552057148\n",
      "Minibatch step 47/100: mean batch inertia: 87366.509057705, ewa inertia: 87366.509057705\n",
      "Converged (lack of improvement in inertia) at step 47/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 287257084.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 286637427.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 289857488.0\n",
      "Minibatch step 1/100: mean batch inertia: 136610.91670655817\n",
      "Minibatch step 2/100: mean batch inertia: 91896.17623180024, ewa inertia: 91896.17623180024\n",
      "Minibatch step 3/100: mean batch inertia: 89666.60589201622, ewa inertia: 89666.60589201622\n",
      "Minibatch step 4/100: mean batch inertia: 89729.9404085306, ewa inertia: 89729.9404085306\n",
      "Minibatch step 5/100: mean batch inertia: 90166.62527477316, ewa inertia: 90166.62527477316\n",
      "Minibatch step 6/100: mean batch inertia: 89067.96702832272, ewa inertia: 89067.96702832272\n",
      "Minibatch step 7/100: mean batch inertia: 89105.62514552905, ewa inertia: 89105.62514552905\n",
      "Minibatch step 8/100: mean batch inertia: 88477.58979928405, ewa inertia: 88477.58979928405\n",
      "Minibatch step 9/100: mean batch inertia: 88481.0159080736, ewa inertia: 88481.0159080736\n",
      "Minibatch step 10/100: mean batch inertia: 88426.64681134319, ewa inertia: 88426.64681134319\n",
      "Minibatch step 11/100: mean batch inertia: 88006.49786458653, ewa inertia: 88006.49786458653\n",
      "Minibatch step 12/100: mean batch inertia: 88468.46850662938, ewa inertia: 88468.46850662938\n",
      "Minibatch step 13/100: mean batch inertia: 87989.99166843492, ewa inertia: 87989.99166843492\n",
      "Minibatch step 14/100: mean batch inertia: 87947.59297519707, ewa inertia: 87947.59297519707\n",
      "Minibatch step 15/100: mean batch inertia: 88884.34126565584, ewa inertia: 88884.34126565584\n",
      "Minibatch step 16/100: mean batch inertia: 88837.5294652876, ewa inertia: 88837.5294652876\n",
      "Minibatch step 17/100: mean batch inertia: 88252.58380492566, ewa inertia: 88252.58380492566\n",
      "Minibatch step 18/100: mean batch inertia: 87073.62654955819, ewa inertia: 87073.62654955819\n",
      "Minibatch step 19/100: mean batch inertia: 87620.42783265347, ewa inertia: 87620.42783265347\n",
      "Minibatch step 20/100: mean batch inertia: 88196.92535837922, ewa inertia: 88196.92535837922\n",
      "Minibatch step 21/100: mean batch inertia: 87115.80356926244, ewa inertia: 87115.80356926244\n",
      "Minibatch step 22/100: mean batch inertia: 87034.42550438058, ewa inertia: 87034.42550438058\n",
      "Minibatch step 23/100: mean batch inertia: 88789.96593129095, ewa inertia: 88789.96593129095\n",
      "Minibatch step 24/100: mean batch inertia: 88571.0084404525, ewa inertia: 88571.0084404525\n",
      "Minibatch step 25/100: mean batch inertia: 88232.13598540482, ewa inertia: 88232.13598540482\n",
      "Minibatch step 26/100: mean batch inertia: 87289.69178502249, ewa inertia: 87289.69178502249\n",
      "Minibatch step 27/100: mean batch inertia: 87983.51146724472, ewa inertia: 87983.51146724472\n",
      "Minibatch step 28/100: mean batch inertia: 88320.56895388824, ewa inertia: 88320.56895388824\n",
      "Minibatch step 29/100: mean batch inertia: 88127.95217478028, ewa inertia: 88127.95217478028\n",
      "Minibatch step 30/100: mean batch inertia: 88046.11366876548, ewa inertia: 88046.11366876548\n",
      "Minibatch step 31/100: mean batch inertia: 87182.19685947508, ewa inertia: 87182.19685947508\n",
      "Minibatch step 32/100: mean batch inertia: 87593.036842285, ewa inertia: 87593.036842285\n",
      "Converged (lack of improvement in inertia) at step 32/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 285100464.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 284866640.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 284635710.0\n",
      "Minibatch step 1/100: mean batch inertia: 136803.89564384872\n",
      "Minibatch step 2/100: mean batch inertia: 90813.31151160544, ewa inertia: 90813.31151160544\n",
      "Minibatch step 3/100: mean batch inertia: 89113.10937566782, ewa inertia: 89113.10937566782\n",
      "Minibatch step 4/100: mean batch inertia: 89146.82790084305, ewa inertia: 89146.82790084305\n",
      "Minibatch step 5/100: mean batch inertia: 88020.66595731268, ewa inertia: 88020.66595731268\n",
      "Minibatch step 6/100: mean batch inertia: 89323.89410209461, ewa inertia: 89323.89410209461\n",
      "Minibatch step 7/100: mean batch inertia: 88230.32452080594, ewa inertia: 88230.32452080594\n",
      "Minibatch step 8/100: mean batch inertia: 87395.0771964278, ewa inertia: 87395.0771964278\n",
      "Minibatch step 9/100: mean batch inertia: 88367.03580843387, ewa inertia: 88367.03580843387\n",
      "Minibatch step 10/100: mean batch inertia: 87998.86740082478, ewa inertia: 87998.86740082478\n",
      "Minibatch step 11/100: mean batch inertia: 88119.93896932033, ewa inertia: 88119.93896932033\n",
      "Minibatch step 12/100: mean batch inertia: 87938.02464622917, ewa inertia: 87938.02464622917\n",
      "Minibatch step 13/100: mean batch inertia: 88465.52576509234, ewa inertia: 88465.52576509234\n",
      "Minibatch step 14/100: mean batch inertia: 88078.93774676445, ewa inertia: 88078.93774676445\n",
      "Minibatch step 15/100: mean batch inertia: 87378.01362607305, ewa inertia: 87378.01362607305\n",
      "Minibatch step 16/100: mean batch inertia: 87460.5036493837, ewa inertia: 87460.5036493837\n",
      "Minibatch step 17/100: mean batch inertia: 87378.8216274261, ewa inertia: 87378.8216274261\n",
      "Minibatch step 18/100: mean batch inertia: 87884.80985781756, ewa inertia: 87884.80985781756\n",
      "Minibatch step 19/100: mean batch inertia: 87621.94734246205, ewa inertia: 87621.94734246205\n",
      "Minibatch step 20/100: mean batch inertia: 87864.82314445541, ewa inertia: 87864.82314445541\n",
      "Minibatch step 21/100: mean batch inertia: 87961.64120374295, ewa inertia: 87961.64120374295\n",
      "Minibatch step 22/100: mean batch inertia: 87826.13928387742, ewa inertia: 87826.13928387742\n",
      "Minibatch step 23/100: mean batch inertia: 86962.07082886773, ewa inertia: 86962.07082886773\n",
      "Minibatch step 24/100: mean batch inertia: 88323.09468777064, ewa inertia: 88323.09468777064\n",
      "Minibatch step 25/100: mean batch inertia: 87982.4963853167, ewa inertia: 87982.4963853167\n",
      "Minibatch step 26/100: mean batch inertia: 87553.58534331871, ewa inertia: 87553.58534331871\n",
      "Minibatch step 27/100: mean batch inertia: 87399.00207498779, ewa inertia: 87399.00207498779\n",
      "Minibatch step 28/100: mean batch inertia: 87353.89653597069, ewa inertia: 87353.89653597069\n",
      "Minibatch step 29/100: mean batch inertia: 87256.06193413302, ewa inertia: 87256.06193413302\n",
      "Minibatch step 30/100: mean batch inertia: 87758.59385922654, ewa inertia: 87758.59385922654\n",
      "Minibatch step 31/100: mean batch inertia: 87121.3496144035, ewa inertia: 87121.3496144035\n",
      "Minibatch step 32/100: mean batch inertia: 87318.35963034965, ewa inertia: 87318.35963034965\n",
      "Minibatch step 33/100: mean batch inertia: 87918.22605515912, ewa inertia: 87918.22605515912\n",
      "Converged (lack of improvement in inertia) at step 33/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 289158087.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 289181500.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 291353513.0\n",
      "Minibatch step 1/100: mean batch inertia: 136789.88989947343\n",
      "Minibatch step 2/100: mean batch inertia: 90941.72650383526, ewa inertia: 90941.72650383526\n",
      "Minibatch step 3/100: mean batch inertia: 89491.92347831304, ewa inertia: 89491.92347831304\n",
      "Minibatch step 4/100: mean batch inertia: 88987.07712676015, ewa inertia: 88987.07712676015\n",
      "Minibatch step 5/100: mean batch inertia: 88601.1051995569, ewa inertia: 88601.1051995569\n",
      "Minibatch step 6/100: mean batch inertia: 88405.27888766254, ewa inertia: 88405.27888766254\n",
      "Minibatch step 7/100: mean batch inertia: 89430.77425668965, ewa inertia: 89430.77425668965\n",
      "Minibatch step 8/100: mean batch inertia: 88392.42830942017, ewa inertia: 88392.42830942017\n",
      "Minibatch step 9/100: mean batch inertia: 88013.03552075094, ewa inertia: 88013.03552075094\n",
      "Minibatch step 10/100: mean batch inertia: 88024.64294086606, ewa inertia: 88024.64294086606\n",
      "Minibatch step 11/100: mean batch inertia: 88158.14020976741, ewa inertia: 88158.14020976741\n",
      "Minibatch step 12/100: mean batch inertia: 88356.58531423392, ewa inertia: 88356.58531423392\n",
      "Minibatch step 13/100: mean batch inertia: 87810.96015780122, ewa inertia: 87810.96015780122\n",
      "Minibatch step 14/100: mean batch inertia: 87808.4361509665, ewa inertia: 87808.4361509665\n",
      "Minibatch step 15/100: mean batch inertia: 88941.53609700827, ewa inertia: 88941.53609700827\n",
      "Minibatch step 16/100: mean batch inertia: 87094.90115405661, ewa inertia: 87094.90115405661\n",
      "Minibatch step 17/100: mean batch inertia: 86619.53985295369, ewa inertia: 86619.53985295369\n",
      "Minibatch step 18/100: mean batch inertia: 87187.62850557435, ewa inertia: 87187.62850557435\n",
      "Minibatch step 19/100: mean batch inertia: 87523.31572663508, ewa inertia: 87523.31572663508\n",
      "Minibatch step 20/100: mean batch inertia: 87422.8445434049, ewa inertia: 87422.8445434049\n",
      "Minibatch step 21/100: mean batch inertia: 86258.22850050191, ewa inertia: 86258.22850050191\n",
      "Minibatch step 22/100: mean batch inertia: 87988.90221659292, ewa inertia: 87988.90221659292\n",
      "Minibatch step 23/100: mean batch inertia: 87136.29075837029, ewa inertia: 87136.29075837029\n",
      "Minibatch step 24/100: mean batch inertia: 88011.13054911191, ewa inertia: 88011.13054911191\n",
      "Minibatch step 25/100: mean batch inertia: 87913.9265092026, ewa inertia: 87913.9265092026\n",
      "Minibatch step 26/100: mean batch inertia: 87808.92880833363, ewa inertia: 87808.92880833363\n",
      "Minibatch step 27/100: mean batch inertia: 87378.0624893779, ewa inertia: 87378.0624893779\n",
      "Minibatch step 28/100: mean batch inertia: 87333.76171289136, ewa inertia: 87333.76171289136\n",
      "Minibatch step 29/100: mean batch inertia: 86936.00289527258, ewa inertia: 86936.00289527258\n",
      "Minibatch step 30/100: mean batch inertia: 87611.34046978985, ewa inertia: 87611.34046978985\n",
      "Minibatch step 31/100: mean batch inertia: 86727.63416087003, ewa inertia: 86727.63416087003\n",
      "Converged (lack of improvement in inertia) at step 31/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 286383809.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 293471868.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 294177053.0\n",
      "Minibatch step 1/100: mean batch inertia: 136914.7826711345\n",
      "Minibatch step 2/100: mean batch inertia: 92167.0775754708, ewa inertia: 92167.0775754708\n",
      "Minibatch step 3/100: mean batch inertia: 90852.5074840087, ewa inertia: 90852.5074840087\n",
      "Minibatch step 4/100: mean batch inertia: 90278.97514795497, ewa inertia: 90278.97514795497\n",
      "Minibatch step 5/100: mean batch inertia: 88876.55290098792, ewa inertia: 88876.55290098792\n",
      "Minibatch step 6/100: mean batch inertia: 89215.23083447211, ewa inertia: 89215.23083447211\n",
      "Minibatch step 7/100: mean batch inertia: 88858.72018358244, ewa inertia: 88858.72018358244\n",
      "Minibatch step 8/100: mean batch inertia: 88152.18112559716, ewa inertia: 88152.18112559716\n",
      "Minibatch step 9/100: mean batch inertia: 89145.69008449794, ewa inertia: 89145.69008449794\n",
      "Minibatch step 10/100: mean batch inertia: 88705.64046062372, ewa inertia: 88705.64046062372\n",
      "Minibatch step 11/100: mean batch inertia: 89052.69376883605, ewa inertia: 89052.69376883605\n",
      "Minibatch step 12/100: mean batch inertia: 88429.87993869446, ewa inertia: 88429.87993869446\n",
      "Minibatch step 13/100: mean batch inertia: 88627.27168019814, ewa inertia: 88627.27168019814\n",
      "Minibatch step 14/100: mean batch inertia: 89079.97356373539, ewa inertia: 89079.97356373539\n",
      "Minibatch step 15/100: mean batch inertia: 88465.96306467566, ewa inertia: 88465.96306467566\n",
      "Minibatch step 16/100: mean batch inertia: 88073.24462497573, ewa inertia: 88073.24462497573\n",
      "Minibatch step 17/100: mean batch inertia: 87992.7649390798, ewa inertia: 87992.7649390798\n",
      "Minibatch step 18/100: mean batch inertia: 87280.45656125342, ewa inertia: 87280.45656125342\n",
      "Minibatch step 19/100: mean batch inertia: 87660.72785625314, ewa inertia: 87660.72785625314\n",
      "Minibatch step 20/100: mean batch inertia: 87450.10270184278, ewa inertia: 87450.10270184278\n",
      "Minibatch step 21/100: mean batch inertia: 88853.9000246357, ewa inertia: 88853.9000246357\n",
      "Minibatch step 22/100: mean batch inertia: 88979.48714639038, ewa inertia: 88979.48714639038\n",
      "Minibatch step 23/100: mean batch inertia: 88185.73421781942, ewa inertia: 88185.73421781942\n",
      "Minibatch step 24/100: mean batch inertia: 88404.96748609169, ewa inertia: 88404.96748609169\n",
      "Minibatch step 25/100: mean batch inertia: 87957.31054791423, ewa inertia: 87957.31054791423\n",
      "Minibatch step 26/100: mean batch inertia: 87806.75082902666, ewa inertia: 87806.75082902666\n",
      "Minibatch step 27/100: mean batch inertia: 87299.56927020408, ewa inertia: 87299.56927020408\n",
      "Minibatch step 28/100: mean batch inertia: 88649.71256264858, ewa inertia: 88649.71256264858\n",
      "Converged (lack of improvement in inertia) at step 28/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 288499697.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 289726128.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 286924913.0\n",
      "Minibatch step 1/100: mean batch inertia: 138114.91957874582\n",
      "Minibatch step 2/100: mean batch inertia: 91175.07466847963, ewa inertia: 91175.07466847963\n",
      "Minibatch step 3/100: mean batch inertia: 90951.94485960217, ewa inertia: 90951.94485960217\n",
      "Minibatch step 4/100: mean batch inertia: 89406.3435083594, ewa inertia: 89406.3435083594\n",
      "Minibatch step 5/100: mean batch inertia: 89631.93741372136, ewa inertia: 89631.93741372136\n",
      "Minibatch step 6/100: mean batch inertia: 89097.00505471375, ewa inertia: 89097.00505471375\n",
      "Minibatch step 7/100: mean batch inertia: 88991.28409301476, ewa inertia: 88991.28409301476\n",
      "Minibatch step 8/100: mean batch inertia: 88676.46038501887, ewa inertia: 88676.46038501887\n",
      "Minibatch step 9/100: mean batch inertia: 88042.79217333038, ewa inertia: 88042.79217333038\n",
      "Minibatch step 10/100: mean batch inertia: 88942.2997773733, ewa inertia: 88942.2997773733\n",
      "Minibatch step 11/100: mean batch inertia: 87943.39841139315, ewa inertia: 87943.39841139315\n",
      "Minibatch step 12/100: mean batch inertia: 87480.49881452302, ewa inertia: 87480.49881452302\n",
      "Minibatch step 13/100: mean batch inertia: 88644.90915600568, ewa inertia: 88644.90915600568\n",
      "Minibatch step 14/100: mean batch inertia: 87959.06126371618, ewa inertia: 87959.06126371618\n",
      "Minibatch step 15/100: mean batch inertia: 88676.75780886656, ewa inertia: 88676.75780886656\n",
      "Minibatch step 16/100: mean batch inertia: 86757.20500322417, ewa inertia: 86757.20500322417\n",
      "Minibatch step 17/100: mean batch inertia: 88237.75109289972, ewa inertia: 88237.75109289972\n",
      "Minibatch step 18/100: mean batch inertia: 87944.4041608459, ewa inertia: 87944.4041608459\n",
      "Minibatch step 19/100: mean batch inertia: 88105.09236826542, ewa inertia: 88105.09236826542\n",
      "Minibatch step 20/100: mean batch inertia: 88858.107902482, ewa inertia: 88858.107902482\n",
      "Minibatch step 21/100: mean batch inertia: 88051.3853593002, ewa inertia: 88051.3853593002\n",
      "Minibatch step 22/100: mean batch inertia: 88443.11386274865, ewa inertia: 88443.11386274865\n",
      "Minibatch step 23/100: mean batch inertia: 88430.65286175684, ewa inertia: 88430.65286175684\n",
      "Minibatch step 24/100: mean batch inertia: 88206.85834369487, ewa inertia: 88206.85834369487\n",
      "Minibatch step 25/100: mean batch inertia: 87942.08943855843, ewa inertia: 87942.08943855843\n",
      "Minibatch step 26/100: mean batch inertia: 87501.0256367667, ewa inertia: 87501.0256367667\n",
      "Converged (lack of improvement in inertia) at step 26/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 283409899.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 280413059.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 288320691.0\n",
      "Minibatch step 1/100: mean batch inertia: 135238.18621349928\n",
      "Minibatch step 2/100: mean batch inertia: 90817.53612718623, ewa inertia: 90817.53612718623\n",
      "Minibatch step 3/100: mean batch inertia: 89610.35926044318, ewa inertia: 89610.35926044318\n",
      "Minibatch step 4/100: mean batch inertia: 88619.61295675772, ewa inertia: 88619.61295675772\n",
      "Minibatch step 5/100: mean batch inertia: 89594.07093947913, ewa inertia: 89594.07093947913\n",
      "Minibatch step 6/100: mean batch inertia: 89357.64485951755, ewa inertia: 89357.64485951755\n",
      "Minibatch step 7/100: mean batch inertia: 88973.13461037338, ewa inertia: 88973.13461037338\n",
      "Minibatch step 8/100: mean batch inertia: 88387.2672075863, ewa inertia: 88387.2672075863\n",
      "Minibatch step 9/100: mean batch inertia: 87629.77254867861, ewa inertia: 87629.77254867861\n",
      "Minibatch step 10/100: mean batch inertia: 87932.650312922, ewa inertia: 87932.650312922\n",
      "Minibatch step 11/100: mean batch inertia: 88528.55722301701, ewa inertia: 88528.55722301701\n",
      "Minibatch step 12/100: mean batch inertia: 87304.54619969924, ewa inertia: 87304.54619969924\n",
      "Minibatch step 13/100: mean batch inertia: 88183.0480023867, ewa inertia: 88183.0480023867\n",
      "Minibatch step 14/100: mean batch inertia: 87905.35463185716, ewa inertia: 87905.35463185716\n",
      "Minibatch step 15/100: mean batch inertia: 87856.28576284392, ewa inertia: 87856.28576284392\n",
      "Minibatch step 16/100: mean batch inertia: 87423.8943989494, ewa inertia: 87423.8943989494\n",
      "Minibatch step 17/100: mean batch inertia: 88102.252876923, ewa inertia: 88102.252876923\n",
      "Minibatch step 18/100: mean batch inertia: 87702.36042799872, ewa inertia: 87702.36042799872\n",
      "Minibatch step 19/100: mean batch inertia: 87896.02468252795, ewa inertia: 87896.02468252795\n",
      "Minibatch step 20/100: mean batch inertia: 87068.87571543208, ewa inertia: 87068.87571543208\n",
      "Minibatch step 21/100: mean batch inertia: 87747.21117003681, ewa inertia: 87747.21117003681\n",
      "Minibatch step 22/100: mean batch inertia: 87470.56830630041, ewa inertia: 87470.56830630041\n",
      "Minibatch step 23/100: mean batch inertia: 86802.65504426866, ewa inertia: 86802.65504426866\n",
      "Minibatch step 24/100: mean batch inertia: 87324.38656637642, ewa inertia: 87324.38656637642\n",
      "Minibatch step 25/100: mean batch inertia: 87366.14265900665, ewa inertia: 87366.14265900665\n",
      "Minibatch step 26/100: mean batch inertia: 88441.30745268743, ewa inertia: 88441.30745268743\n",
      "Minibatch step 27/100: mean batch inertia: 87982.95797970395, ewa inertia: 87982.95797970395\n",
      "Minibatch step 28/100: mean batch inertia: 87619.70028814631, ewa inertia: 87619.70028814631\n",
      "Minibatch step 29/100: mean batch inertia: 87817.79702157843, ewa inertia: 87817.79702157843\n",
      "Minibatch step 30/100: mean batch inertia: 87298.47653891097, ewa inertia: 87298.47653891097\n",
      "Minibatch step 31/100: mean batch inertia: 88608.72417596326, ewa inertia: 88608.72417596326\n",
      "Minibatch step 32/100: mean batch inertia: 87604.02986484849, ewa inertia: 87604.02986484849\n",
      "Minibatch step 33/100: mean batch inertia: 87736.42520615464, ewa inertia: 87736.42520615464\n",
      "Converged (lack of improvement in inertia) at step 33/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 286690477.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 289248907.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 288268772.0\n",
      "Minibatch step 1/100: mean batch inertia: 138015.02297750118\n",
      "Minibatch step 2/100: mean batch inertia: 90082.3006906558, ewa inertia: 90082.3006906558\n",
      "Minibatch step 3/100: mean batch inertia: 89176.82043871205, ewa inertia: 89176.82043871205\n",
      "Minibatch step 4/100: mean batch inertia: 89086.48984969768, ewa inertia: 89086.48984969768\n",
      "Minibatch step 5/100: mean batch inertia: 89276.68248956223, ewa inertia: 89276.68248956223\n",
      "Minibatch step 6/100: mean batch inertia: 88916.84576196624, ewa inertia: 88916.84576196624\n",
      "Minibatch step 7/100: mean batch inertia: 88941.42689786266, ewa inertia: 88941.42689786266\n",
      "Minibatch step 8/100: mean batch inertia: 88133.9681780059, ewa inertia: 88133.9681780059\n",
      "Minibatch step 9/100: mean batch inertia: 88647.68382888062, ewa inertia: 88647.68382888062\n",
      "Minibatch step 10/100: mean batch inertia: 88829.24739149182, ewa inertia: 88829.24739149182\n",
      "Minibatch step 11/100: mean batch inertia: 87868.6058475955, ewa inertia: 87868.6058475955\n",
      "Minibatch step 12/100: mean batch inertia: 87806.47307808488, ewa inertia: 87806.47307808488\n",
      "Minibatch step 13/100: mean batch inertia: 87413.60906365313, ewa inertia: 87413.60906365313\n",
      "Minibatch step 14/100: mean batch inertia: 88408.82287809688, ewa inertia: 88408.82287809688\n",
      "Minibatch step 15/100: mean batch inertia: 88321.78238416788, ewa inertia: 88321.78238416788\n",
      "Minibatch step 16/100: mean batch inertia: 88082.82575286883, ewa inertia: 88082.82575286883\n",
      "Minibatch step 17/100: mean batch inertia: 88306.43538641823, ewa inertia: 88306.43538641823\n",
      "Minibatch step 18/100: mean batch inertia: 88466.63190724757, ewa inertia: 88466.63190724757\n",
      "Minibatch step 19/100: mean batch inertia: 88015.66413679205, ewa inertia: 88015.66413679205\n",
      "Minibatch step 20/100: mean batch inertia: 88185.92636271253, ewa inertia: 88185.92636271253\n",
      "Minibatch step 21/100: mean batch inertia: 87792.93301364491, ewa inertia: 87792.93301364491\n",
      "Minibatch step 22/100: mean batch inertia: 88108.82870242889, ewa inertia: 88108.82870242889\n",
      "Minibatch step 23/100: mean batch inertia: 87472.37504154809, ewa inertia: 87472.37504154809\n",
      "Converged (lack of improvement in inertia) at step 23/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 288146023.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 284646719.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 285987467.0\n",
      "Minibatch step 1/100: mean batch inertia: 138620.58161799904\n",
      "Minibatch step 2/100: mean batch inertia: 92818.14170820035, ewa inertia: 92818.14170820035\n",
      "Minibatch step 3/100: mean batch inertia: 90516.18075179517, ewa inertia: 90516.18075179517\n",
      "Minibatch step 4/100: mean batch inertia: 89250.29809523174, ewa inertia: 89250.29809523174\n",
      "Minibatch step 5/100: mean batch inertia: 89209.54520746222, ewa inertia: 89209.54520746222\n",
      "Minibatch step 6/100: mean batch inertia: 89764.21065334805, ewa inertia: 89764.21065334805\n",
      "Minibatch step 7/100: mean batch inertia: 89065.89907339051, ewa inertia: 89065.89907339051\n",
      "Minibatch step 8/100: mean batch inertia: 89584.84601223565, ewa inertia: 89584.84601223565\n",
      "Minibatch step 9/100: mean batch inertia: 88238.26884399573, ewa inertia: 88238.26884399573\n",
      "Minibatch step 10/100: mean batch inertia: 89520.78687256535, ewa inertia: 89520.78687256535\n",
      "Minibatch step 11/100: mean batch inertia: 88403.28165930496, ewa inertia: 88403.28165930496\n",
      "Minibatch step 12/100: mean batch inertia: 88616.37813789098, ewa inertia: 88616.37813789098\n",
      "Minibatch step 13/100: mean batch inertia: 88323.75335678014, ewa inertia: 88323.75335678014\n",
      "Minibatch step 14/100: mean batch inertia: 88093.21706705684, ewa inertia: 88093.21706705684\n",
      "Minibatch step 15/100: mean batch inertia: 88420.76271635518, ewa inertia: 88420.76271635518\n",
      "Minibatch step 16/100: mean batch inertia: 88396.50013983253, ewa inertia: 88396.50013983253\n",
      "Minibatch step 17/100: mean batch inertia: 89547.6570249218, ewa inertia: 89547.6570249218\n",
      "Minibatch step 18/100: mean batch inertia: 88481.82888585771, ewa inertia: 88481.82888585771\n",
      "Minibatch step 19/100: mean batch inertia: 89260.66751289039, ewa inertia: 89260.66751289039\n",
      "Minibatch step 20/100: mean batch inertia: 88660.59645987267, ewa inertia: 88660.59645987267\n",
      "Minibatch step 21/100: mean batch inertia: 88065.74147240086, ewa inertia: 88065.74147240086\n",
      "Minibatch step 22/100: mean batch inertia: 88434.07438175856, ewa inertia: 88434.07438175856\n",
      "Minibatch step 23/100: mean batch inertia: 87963.50987395165, ewa inertia: 87963.50987395165\n",
      "Minibatch step 24/100: mean batch inertia: 88705.70105170568, ewa inertia: 88705.70105170568\n",
      "Minibatch step 25/100: mean batch inertia: 87658.85180283232, ewa inertia: 87658.85180283232\n",
      "Minibatch step 26/100: mean batch inertia: 87876.71502670538, ewa inertia: 87876.71502670538\n",
      "Minibatch step 27/100: mean batch inertia: 88520.8335327365, ewa inertia: 88520.8335327365\n",
      "Minibatch step 28/100: mean batch inertia: 87568.39619306622, ewa inertia: 87568.39619306622\n",
      "Minibatch step 29/100: mean batch inertia: 88165.5080368947, ewa inertia: 88165.5080368947\n",
      "Minibatch step 30/100: mean batch inertia: 88532.35534359641, ewa inertia: 88532.35534359641\n",
      "Minibatch step 31/100: mean batch inertia: 88945.40381668328, ewa inertia: 88945.40381668328\n",
      "Minibatch step 32/100: mean batch inertia: 88511.6583149561, ewa inertia: 88511.6583149561\n",
      "Minibatch step 33/100: mean batch inertia: 88568.14412704464, ewa inertia: 88568.14412704464\n",
      "Minibatch step 34/100: mean batch inertia: 88156.3413184569, ewa inertia: 88156.3413184569\n",
      "Minibatch step 35/100: mean batch inertia: 88443.58125473732, ewa inertia: 88443.58125473732\n",
      "Minibatch step 36/100: mean batch inertia: 88238.76305389461, ewa inertia: 88238.76305389461\n",
      "Minibatch step 37/100: mean batch inertia: 87865.10557639977, ewa inertia: 87865.10557639977\n",
      "Minibatch step 38/100: mean batch inertia: 88705.64304155688, ewa inertia: 88705.64304155688\n",
      "Converged (lack of improvement in inertia) at step 38/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 287408309.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 291016485.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 291201216.0\n",
      "Minibatch step 1/100: mean batch inertia: 137756.59167065582\n",
      "Minibatch step 2/100: mean batch inertia: 91761.7539787352, ewa inertia: 91761.7539787352\n",
      "Minibatch step 3/100: mean batch inertia: 90308.97438383155, ewa inertia: 90308.97438383155\n",
      "Minibatch step 4/100: mean batch inertia: 89848.16562965586, ewa inertia: 89848.16562965586\n",
      "Minibatch step 5/100: mean batch inertia: 88794.6328188368, ewa inertia: 88794.6328188368\n",
      "Minibatch step 6/100: mean batch inertia: 87842.09807202985, ewa inertia: 87842.09807202985\n",
      "Minibatch step 7/100: mean batch inertia: 88621.97885289036, ewa inertia: 88621.97885289036\n",
      "Minibatch step 8/100: mean batch inertia: 88340.53875340507, ewa inertia: 88340.53875340507\n",
      "Minibatch step 9/100: mean batch inertia: 88159.31007367175, ewa inertia: 88159.31007367175\n",
      "Minibatch step 10/100: mean batch inertia: 87514.91522048708, ewa inertia: 87514.91522048708\n",
      "Minibatch step 11/100: mean batch inertia: 88879.30439191264, ewa inertia: 88879.30439191264\n",
      "Minibatch step 12/100: mean batch inertia: 88874.6566981138, ewa inertia: 88874.6566981138\n",
      "Minibatch step 13/100: mean batch inertia: 88443.94999564858, ewa inertia: 88443.94999564858\n",
      "Minibatch step 14/100: mean batch inertia: 87978.3674192267, ewa inertia: 87978.3674192267\n",
      "Minibatch step 15/100: mean batch inertia: 88725.93786144794, ewa inertia: 88725.93786144794\n",
      "Minibatch step 16/100: mean batch inertia: 87608.79869735338, ewa inertia: 87608.79869735338\n",
      "Minibatch step 17/100: mean batch inertia: 88370.76212448563, ewa inertia: 88370.76212448563\n",
      "Minibatch step 18/100: mean batch inertia: 88231.27723439115, ewa inertia: 88231.27723439115\n",
      "Minibatch step 19/100: mean batch inertia: 87771.57408159439, ewa inertia: 87771.57408159439\n",
      "Minibatch step 20/100: mean batch inertia: 88362.92700969592, ewa inertia: 88362.92700969592\n",
      "Converged (lack of improvement in inertia) at step 20/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 287118853.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 285677694.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 284346677.0\n",
      "Minibatch step 1/100: mean batch inertia: 137094.51268549546\n",
      "Minibatch step 2/100: mean batch inertia: 92484.09384095333, ewa inertia: 92484.09384095333\n",
      "Minibatch step 3/100: mean batch inertia: 89993.27882879351, ewa inertia: 89993.27882879351\n",
      "Minibatch step 4/100: mean batch inertia: 90350.56362490254, ewa inertia: 90350.56362490254\n",
      "Minibatch step 5/100: mean batch inertia: 89094.27322716272, ewa inertia: 89094.27322716272\n",
      "Minibatch step 6/100: mean batch inertia: 89147.34223180357, ewa inertia: 89147.34223180357\n",
      "Minibatch step 7/100: mean batch inertia: 88248.05456682312, ewa inertia: 88248.05456682312\n",
      "Minibatch step 8/100: mean batch inertia: 88844.85929964433, ewa inertia: 88844.85929964433\n",
      "Minibatch step 9/100: mean batch inertia: 88891.3041897627, ewa inertia: 88891.3041897627\n",
      "Minibatch step 10/100: mean batch inertia: 87819.33910443008, ewa inertia: 87819.33910443008\n",
      "Minibatch step 11/100: mean batch inertia: 87424.22247624528, ewa inertia: 87424.22247624528\n",
      "Minibatch step 12/100: mean batch inertia: 87514.2742338242, ewa inertia: 87514.2742338242\n",
      "Minibatch step 13/100: mean batch inertia: 88044.09213163957, ewa inertia: 88044.09213163957\n",
      "Minibatch step 14/100: mean batch inertia: 88042.5947572709, ewa inertia: 88042.5947572709\n",
      "Minibatch step 15/100: mean batch inertia: 88251.16277601065, ewa inertia: 88251.16277601065\n",
      "Minibatch step 16/100: mean batch inertia: 87956.84278180396, ewa inertia: 87956.84278180396\n",
      "Minibatch step 17/100: mean batch inertia: 87755.42042360122, ewa inertia: 87755.42042360122\n",
      "Minibatch step 18/100: mean batch inertia: 88124.54003448291, ewa inertia: 88124.54003448291\n",
      "Minibatch step 19/100: mean batch inertia: 88542.1893013685, ewa inertia: 88542.1893013685\n",
      "Minibatch step 20/100: mean batch inertia: 87559.26945196993, ewa inertia: 87559.26945196993\n",
      "Minibatch step 21/100: mean batch inertia: 87953.92072945167, ewa inertia: 87953.92072945167\n",
      "Converged (lack of improvement in inertia) at step 21/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 289322914.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 285691625.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 284179442.0\n",
      "Minibatch step 1/100: mean batch inertia: 137855.5974150311\n",
      "Minibatch step 2/100: mean batch inertia: 91279.20380529399, ewa inertia: 91279.20380529399\n",
      "Minibatch step 3/100: mean batch inertia: 89189.75965575674, ewa inertia: 89189.75965575674\n",
      "Minibatch step 4/100: mean batch inertia: 88229.19554867336, ewa inertia: 88229.19554867336\n",
      "Minibatch step 5/100: mean batch inertia: 88762.99933985513, ewa inertia: 88762.99933985513\n",
      "Minibatch step 6/100: mean batch inertia: 88152.87063121896, ewa inertia: 88152.87063121896\n",
      "Minibatch step 7/100: mean batch inertia: 88467.31428986146, ewa inertia: 88467.31428986146\n",
      "Minibatch step 8/100: mean batch inertia: 87733.98566359859, ewa inertia: 87733.98566359859\n",
      "Minibatch step 9/100: mean batch inertia: 88195.00441835457, ewa inertia: 88195.00441835457\n",
      "Minibatch step 10/100: mean batch inertia: 87862.23452260524, ewa inertia: 87862.23452260524\n",
      "Minibatch step 11/100: mean batch inertia: 88178.00011656781, ewa inertia: 88178.00011656781\n",
      "Minibatch step 12/100: mean batch inertia: 87172.45263733214, ewa inertia: 87172.45263733214\n",
      "Minibatch step 13/100: mean batch inertia: 88174.11280165923, ewa inertia: 88174.11280165923\n",
      "Minibatch step 14/100: mean batch inertia: 88095.74483041387, ewa inertia: 88095.74483041387\n",
      "Minibatch step 15/100: mean batch inertia: 88320.57276008415, ewa inertia: 88320.57276008415\n",
      "Minibatch step 16/100: mean batch inertia: 87953.03617742764, ewa inertia: 87953.03617742764\n",
      "Minibatch step 17/100: mean batch inertia: 87368.80927630329, ewa inertia: 87368.80927630329\n",
      "Minibatch step 18/100: mean batch inertia: 87278.56515834475, ewa inertia: 87278.56515834475\n",
      "Minibatch step 19/100: mean batch inertia: 87928.21478716588, ewa inertia: 87928.21478716588\n",
      "Minibatch step 20/100: mean batch inertia: 87653.85510228173, ewa inertia: 87653.85510228173\n",
      "Minibatch step 21/100: mean batch inertia: 88194.06318844913, ewa inertia: 88194.06318844913\n",
      "Minibatch step 22/100: mean batch inertia: 88243.64507210434, ewa inertia: 88243.64507210434\n",
      "Converged (lack of improvement in inertia) at step 22/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 291688479.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 290627266.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 291822999.0\n",
      "Minibatch step 1/100: mean batch inertia: 138150.94638583055\n",
      "Minibatch step 2/100: mean batch inertia: 91629.16083496276, ewa inertia: 91629.16083496276\n",
      "Minibatch step 3/100: mean batch inertia: 89505.25590660455, ewa inertia: 89505.25590660455\n",
      "Minibatch step 4/100: mean batch inertia: 90456.38558506702, ewa inertia: 90456.38558506702\n",
      "Minibatch step 5/100: mean batch inertia: 89061.17967676291, ewa inertia: 89061.17967676291\n",
      "Minibatch step 6/100: mean batch inertia: 89233.78976081348, ewa inertia: 89233.78976081348\n",
      "Minibatch step 7/100: mean batch inertia: 87793.93483970269, ewa inertia: 87793.93483970269\n",
      "Minibatch step 8/100: mean batch inertia: 88442.19761749664, ewa inertia: 88442.19761749664\n",
      "Minibatch step 9/100: mean batch inertia: 88947.54052612389, ewa inertia: 88947.54052612389\n",
      "Minibatch step 10/100: mean batch inertia: 88315.03214388932, ewa inertia: 88315.03214388932\n",
      "Minibatch step 11/100: mean batch inertia: 87693.73918929612, ewa inertia: 87693.73918929612\n",
      "Minibatch step 12/100: mean batch inertia: 88334.63121754662, ewa inertia: 88334.63121754662\n",
      "Minibatch step 13/100: mean batch inertia: 87638.96384873205, ewa inertia: 87638.96384873205\n",
      "Minibatch step 14/100: mean batch inertia: 88556.86132906016, ewa inertia: 88556.86132906016\n",
      "Minibatch step 15/100: mean batch inertia: 88618.58864996012, ewa inertia: 88618.58864996012\n",
      "Minibatch step 16/100: mean batch inertia: 88409.75846064858, ewa inertia: 88409.75846064858\n",
      "Minibatch step 17/100: mean batch inertia: 88816.88972135355, ewa inertia: 88816.88972135355\n",
      "Minibatch step 18/100: mean batch inertia: 88435.39417965677, ewa inertia: 88435.39417965677\n",
      "Minibatch step 19/100: mean batch inertia: 88570.40347178814, ewa inertia: 88570.40347178814\n",
      "Minibatch step 20/100: mean batch inertia: 88107.83224844906, ewa inertia: 88107.83224844906\n",
      "Minibatch step 21/100: mean batch inertia: 88530.74543184596, ewa inertia: 88530.74543184596\n",
      "Minibatch step 22/100: mean batch inertia: 88198.2612009824, ewa inertia: 88198.2612009824\n",
      "Minibatch step 23/100: mean batch inertia: 87653.4545101442, ewa inertia: 87653.4545101442\n",
      "Converged (lack of improvement in inertia) at step 23/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 286698303.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 287480839.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 283467676.0\n",
      "Minibatch step 1/100: mean batch inertia: 136485.1081857348\n",
      "Minibatch step 2/100: mean batch inertia: 91537.4472274848, ewa inertia: 91537.4472274848\n",
      "Minibatch step 3/100: mean batch inertia: 89383.23442722365, ewa inertia: 89383.23442722365\n",
      "Minibatch step 4/100: mean batch inertia: 90811.76052067034, ewa inertia: 90811.76052067034\n",
      "Minibatch step 5/100: mean batch inertia: 88787.16772017026, ewa inertia: 88787.16772017026\n",
      "Minibatch step 6/100: mean batch inertia: 88533.65242372165, ewa inertia: 88533.65242372165\n",
      "Minibatch step 7/100: mean batch inertia: 88291.08209482745, ewa inertia: 88291.08209482745\n",
      "Minibatch step 8/100: mean batch inertia: 88205.53150758558, ewa inertia: 88205.53150758558\n",
      "Minibatch step 9/100: mean batch inertia: 88644.71669571946, ewa inertia: 88644.71669571946\n",
      "Minibatch step 10/100: mean batch inertia: 88378.89755762252, ewa inertia: 88378.89755762252\n",
      "Minibatch step 11/100: mean batch inertia: 88878.01212410763, ewa inertia: 88878.01212410763\n",
      "Minibatch step 12/100: mean batch inertia: 87231.07900490334, ewa inertia: 87231.07900490334\n",
      "Minibatch step 13/100: mean batch inertia: 88058.57130593543, ewa inertia: 88058.57130593543\n",
      "Minibatch step 14/100: mean batch inertia: 88392.26801394227, ewa inertia: 88392.26801394227\n",
      "Minibatch step 15/100: mean batch inertia: 88735.4459910327, ewa inertia: 88735.4459910327\n",
      "Minibatch step 16/100: mean batch inertia: 87969.68430753594, ewa inertia: 87969.68430753594\n",
      "Minibatch step 17/100: mean batch inertia: 88479.67107071375, ewa inertia: 88479.67107071375\n",
      "Minibatch step 18/100: mean batch inertia: 87950.39324303193, ewa inertia: 87950.39324303193\n",
      "Minibatch step 19/100: mean batch inertia: 88036.55133108955, ewa inertia: 88036.55133108955\n",
      "Minibatch step 20/100: mean batch inertia: 87345.51590652761, ewa inertia: 87345.51590652761\n",
      "Minibatch step 21/100: mean batch inertia: 88036.18684467858, ewa inertia: 88036.18684467858\n",
      "Minibatch step 22/100: mean batch inertia: 88402.09693239788, ewa inertia: 88402.09693239788\n",
      "Converged (lack of improvement in inertia) at step 22/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 278336484.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 286726823.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 283119455.0\n",
      "Minibatch step 1/100: mean batch inertia: 135005.52225945427\n",
      "Minibatch step 2/100: mean batch inertia: 92010.77375604887, ewa inertia: 92010.77375604887\n",
      "Minibatch step 3/100: mean batch inertia: 89839.7024733946, ewa inertia: 89839.7024733946\n",
      "Minibatch step 4/100: mean batch inertia: 90070.90456052689, ewa inertia: 90070.90456052689\n",
      "Minibatch step 5/100: mean batch inertia: 88858.76016213695, ewa inertia: 88858.76016213695\n",
      "Minibatch step 6/100: mean batch inertia: 88275.71434257206, ewa inertia: 88275.71434257206\n",
      "Minibatch step 7/100: mean batch inertia: 88123.61614127869, ewa inertia: 88123.61614127869\n",
      "Minibatch step 8/100: mean batch inertia: 88297.26737945208, ewa inertia: 88297.26737945208\n",
      "Minibatch step 9/100: mean batch inertia: 87856.4741895845, ewa inertia: 87856.4741895845\n",
      "Minibatch step 10/100: mean batch inertia: 88256.65562808287, ewa inertia: 88256.65562808287\n",
      "Minibatch step 11/100: mean batch inertia: 88573.80085884657, ewa inertia: 88573.80085884657\n",
      "Minibatch step 12/100: mean batch inertia: 87770.02819400904, ewa inertia: 87770.02819400904\n",
      "Minibatch step 13/100: mean batch inertia: 87086.3221171994, ewa inertia: 87086.3221171994\n",
      "Minibatch step 14/100: mean batch inertia: 88466.91781450613, ewa inertia: 88466.91781450613\n",
      "Minibatch step 15/100: mean batch inertia: 87370.89635390507, ewa inertia: 87370.89635390507\n",
      "Minibatch step 16/100: mean batch inertia: 88886.2738282122, ewa inertia: 88886.2738282122\n",
      "Minibatch step 17/100: mean batch inertia: 87897.20645439986, ewa inertia: 87897.20645439986\n",
      "Minibatch step 18/100: mean batch inertia: 88000.67539811718, ewa inertia: 88000.67539811718\n",
      "Minibatch step 19/100: mean batch inertia: 87961.2240539269, ewa inertia: 87961.2240539269\n",
      "Minibatch step 20/100: mean batch inertia: 87710.82191371087, ewa inertia: 87710.82191371087\n",
      "Minibatch step 21/100: mean batch inertia: 87501.85440129005, ewa inertia: 87501.85440129005\n",
      "Minibatch step 22/100: mean batch inertia: 87325.266257268, ewa inertia: 87325.266257268\n",
      "Minibatch step 23/100: mean batch inertia: 87971.03282509671, ewa inertia: 87971.03282509671\n",
      "Converged (lack of improvement in inertia) at step 23/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 292422148.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 287842627.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 288526819.0\n",
      "Minibatch step 1/100: mean batch inertia: 137635.02728578268\n",
      "Minibatch step 2/100: mean batch inertia: 91830.46258366627, ewa inertia: 91830.46258366627\n",
      "Minibatch step 3/100: mean batch inertia: 90301.46945235463, ewa inertia: 90301.46945235463\n",
      "Minibatch step 4/100: mean batch inertia: 89703.21340342192, ewa inertia: 89703.21340342192\n",
      "Minibatch step 5/100: mean batch inertia: 88366.88922512141, ewa inertia: 88366.88922512141\n",
      "Minibatch step 6/100: mean batch inertia: 89110.93009686597, ewa inertia: 89110.93009686597\n",
      "Minibatch step 7/100: mean batch inertia: 89270.84523664304, ewa inertia: 89270.84523664304\n",
      "Minibatch step 8/100: mean batch inertia: 89169.48989542128, ewa inertia: 89169.48989542128\n",
      "Minibatch step 9/100: mean batch inertia: 89355.55740673257, ewa inertia: 89355.55740673257\n",
      "Minibatch step 10/100: mean batch inertia: 88837.31514791207, ewa inertia: 88837.31514791207\n",
      "Minibatch step 11/100: mean batch inertia: 88000.33858904889, ewa inertia: 88000.33858904889\n",
      "Minibatch step 12/100: mean batch inertia: 88059.27851850068, ewa inertia: 88059.27851850068\n",
      "Minibatch step 13/100: mean batch inertia: 87997.39628627672, ewa inertia: 87997.39628627672\n",
      "Minibatch step 14/100: mean batch inertia: 88352.58358168208, ewa inertia: 88352.58358168208\n",
      "Minibatch step 15/100: mean batch inertia: 88340.52070273305, ewa inertia: 88340.52070273305\n",
      "Minibatch step 16/100: mean batch inertia: 87733.66445049066, ewa inertia: 87733.66445049066\n",
      "Minibatch step 17/100: mean batch inertia: 88543.79421869994, ewa inertia: 88543.79421869994\n",
      "Minibatch step 18/100: mean batch inertia: 87529.82473492261, ewa inertia: 87529.82473492261\n",
      "Minibatch step 19/100: mean batch inertia: 89358.2060899702, ewa inertia: 89358.2060899702\n",
      "Minibatch step 20/100: mean batch inertia: 88464.30049456288, ewa inertia: 88464.30049456288\n",
      "Minibatch step 21/100: mean batch inertia: 87760.78775144128, ewa inertia: 87760.78775144128\n",
      "Minibatch step 22/100: mean batch inertia: 88043.21907646966, ewa inertia: 88043.21907646966\n",
      "Minibatch step 23/100: mean batch inertia: 87340.70147845078, ewa inertia: 87340.70147845078\n",
      "Minibatch step 24/100: mean batch inertia: 87062.28779059369, ewa inertia: 87062.28779059369\n",
      "Minibatch step 25/100: mean batch inertia: 88119.28741858507, ewa inertia: 88119.28741858507\n",
      "Minibatch step 26/100: mean batch inertia: 88290.58419754835, ewa inertia: 88290.58419754835\n",
      "Minibatch step 27/100: mean batch inertia: 88130.53685372497, ewa inertia: 88130.53685372497\n",
      "Minibatch step 28/100: mean batch inertia: 88128.28399712524, ewa inertia: 88128.28399712524\n",
      "Minibatch step 29/100: mean batch inertia: 87567.73315599149, ewa inertia: 87567.73315599149\n",
      "Minibatch step 30/100: mean batch inertia: 88057.71861103544, ewa inertia: 88057.71861103544\n",
      "Minibatch step 31/100: mean batch inertia: 87963.96874737748, ewa inertia: 87963.96874737748\n",
      "Minibatch step 32/100: mean batch inertia: 88790.71974086428, ewa inertia: 88790.71974086428\n",
      "Minibatch step 33/100: mean batch inertia: 88338.44111789665, ewa inertia: 88338.44111789665\n",
      "Minibatch step 34/100: mean batch inertia: 88558.56757805406, ewa inertia: 88558.56757805406\n",
      "Converged (lack of improvement in inertia) at step 34/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 287525344.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 290303659.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 293876863.0\n",
      "Minibatch step 1/100: mean batch inertia: 139951.96984202968\n",
      "Minibatch step 2/100: mean batch inertia: 91177.36686387492, ewa inertia: 91177.36686387492\n",
      "Minibatch step 3/100: mean batch inertia: 90152.3196335281, ewa inertia: 90152.3196335281\n",
      "Minibatch step 4/100: mean batch inertia: 89662.94737322965, ewa inertia: 89662.94737322965\n",
      "Minibatch step 5/100: mean batch inertia: 88821.67067166977, ewa inertia: 88821.67067166977\n",
      "Minibatch step 6/100: mean batch inertia: 89198.42671279307, ewa inertia: 89198.42671279307\n",
      "Minibatch step 7/100: mean batch inertia: 89146.53144749237, ewa inertia: 89146.53144749237\n",
      "Minibatch step 8/100: mean batch inertia: 88541.05104809697, ewa inertia: 88541.05104809697\n",
      "Minibatch step 9/100: mean batch inertia: 88995.67398952374, ewa inertia: 88995.67398952374\n",
      "Minibatch step 10/100: mean batch inertia: 88605.28251415545, ewa inertia: 88605.28251415545\n",
      "Minibatch step 11/100: mean batch inertia: 87594.27520854877, ewa inertia: 87594.27520854877\n",
      "Minibatch step 12/100: mean batch inertia: 88126.41812085538, ewa inertia: 88126.41812085538\n",
      "Minibatch step 13/100: mean batch inertia: 88818.72781433846, ewa inertia: 88818.72781433846\n",
      "Minibatch step 14/100: mean batch inertia: 87344.66406202302, ewa inertia: 87344.66406202302\n",
      "Minibatch step 15/100: mean batch inertia: 87901.31420094948, ewa inertia: 87901.31420094948\n",
      "Minibatch step 16/100: mean batch inertia: 86985.70670091125, ewa inertia: 86985.70670091125\n",
      "Minibatch step 17/100: mean batch inertia: 88253.05386524681, ewa inertia: 88253.05386524681\n",
      "Minibatch step 18/100: mean batch inertia: 87493.6557251474, ewa inertia: 87493.6557251474\n",
      "Minibatch step 19/100: mean batch inertia: 87753.45266718579, ewa inertia: 87753.45266718579\n",
      "Minibatch step 20/100: mean batch inertia: 87607.16027827968, ewa inertia: 87607.16027827968\n",
      "Minibatch step 21/100: mean batch inertia: 88142.71875728625, ewa inertia: 88142.71875728625\n",
      "Minibatch step 22/100: mean batch inertia: 87369.77347445751, ewa inertia: 87369.77347445751\n",
      "Minibatch step 23/100: mean batch inertia: 87588.74069165888, ewa inertia: 87588.74069165888\n",
      "Minibatch step 24/100: mean batch inertia: 87183.09897674556, ewa inertia: 87183.09897674556\n",
      "Minibatch step 25/100: mean batch inertia: 87474.09937951477, ewa inertia: 87474.09937951477\n",
      "Minibatch step 26/100: mean batch inertia: 88187.92738014976, ewa inertia: 88187.92738014976\n",
      "Converged (lack of improvement in inertia) at step 26/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 283292983.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 281745692.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 280683542.0\n",
      "Minibatch step 1/100: mean batch inertia: 134627.35088559118\n",
      "Minibatch step 2/100: mean batch inertia: 90150.48931856267, ewa inertia: 90150.48931856267\n",
      "Minibatch step 3/100: mean batch inertia: 89713.00661655894, ewa inertia: 89713.00661655894\n",
      "Minibatch step 4/100: mean batch inertia: 89818.03645767926, ewa inertia: 89818.03645767926\n",
      "Minibatch step 5/100: mean batch inertia: 88857.21057406692, ewa inertia: 88857.21057406692\n",
      "Minibatch step 6/100: mean batch inertia: 89003.05662926407, ewa inertia: 89003.05662926407\n",
      "Minibatch step 7/100: mean batch inertia: 88947.49603219367, ewa inertia: 88947.49603219367\n",
      "Minibatch step 8/100: mean batch inertia: 89192.15669374546, ewa inertia: 89192.15669374546\n",
      "Minibatch step 9/100: mean batch inertia: 89008.85506382522, ewa inertia: 89008.85506382522\n",
      "Minibatch step 10/100: mean batch inertia: 88171.94552186517, ewa inertia: 88171.94552186517\n",
      "Minibatch step 11/100: mean batch inertia: 87637.9945919843, ewa inertia: 87637.9945919843\n",
      "Minibatch step 12/100: mean batch inertia: 87997.34365416647, ewa inertia: 87997.34365416647\n",
      "Minibatch step 13/100: mean batch inertia: 88356.0913304721, ewa inertia: 88356.0913304721\n",
      "Minibatch step 14/100: mean batch inertia: 87356.04218509917, ewa inertia: 87356.04218509917\n",
      "Minibatch step 15/100: mean batch inertia: 88298.13720429734, ewa inertia: 88298.13720429734\n",
      "Minibatch step 16/100: mean batch inertia: 87907.73643050052, ewa inertia: 87907.73643050052\n",
      "Minibatch step 17/100: mean batch inertia: 89017.83731883617, ewa inertia: 89017.83731883617\n",
      "Minibatch step 18/100: mean batch inertia: 87690.84668657804, ewa inertia: 87690.84668657804\n",
      "Minibatch step 19/100: mean batch inertia: 87351.55619517007, ewa inertia: 87351.55619517007\n",
      "Minibatch step 20/100: mean batch inertia: 87758.06633306302, ewa inertia: 87758.06633306302\n",
      "Minibatch step 21/100: mean batch inertia: 87929.18993559867, ewa inertia: 87929.18993559867\n",
      "Minibatch step 22/100: mean batch inertia: 86912.70660574827, ewa inertia: 86912.70660574827\n",
      "Minibatch step 23/100: mean batch inertia: 87744.10898205815, ewa inertia: 87744.10898205815\n",
      "Minibatch step 24/100: mean batch inertia: 87890.40718954265, ewa inertia: 87890.40718954265\n",
      "Minibatch step 25/100: mean batch inertia: 86980.54407151484, ewa inertia: 86980.54407151484\n",
      "Minibatch step 26/100: mean batch inertia: 87473.72209888391, ewa inertia: 87473.72209888391\n",
      "Minibatch step 27/100: mean batch inertia: 87771.04000129575, ewa inertia: 87771.04000129575\n",
      "Minibatch step 28/100: mean batch inertia: 87831.89956146285, ewa inertia: 87831.89956146285\n",
      "Minibatch step 29/100: mean batch inertia: 87735.54160793935, ewa inertia: 87735.54160793935\n",
      "Minibatch step 30/100: mean batch inertia: 87482.10617310781, ewa inertia: 87482.10617310781\n",
      "Minibatch step 31/100: mean batch inertia: 87838.77409292725, ewa inertia: 87838.77409292725\n",
      "Minibatch step 32/100: mean batch inertia: 87722.84621877738, ewa inertia: 87722.84621877738\n",
      "Converged (lack of improvement in inertia) at step 32/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 291275214.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 292911649.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 285626725.0\n",
      "Minibatch step 1/100: mean batch inertia: 136216.12637625658\n",
      "Minibatch step 2/100: mean batch inertia: 90704.28713737996, ewa inertia: 90704.28713737996\n",
      "Minibatch step 3/100: mean batch inertia: 89694.4659450143, ewa inertia: 89694.4659450143\n",
      "Minibatch step 4/100: mean batch inertia: 89198.7006470788, ewa inertia: 89198.7006470788\n",
      "Minibatch step 5/100: mean batch inertia: 89151.43091421724, ewa inertia: 89151.43091421724\n",
      "Minibatch step 6/100: mean batch inertia: 88890.56189006848, ewa inertia: 88890.56189006848\n",
      "Minibatch step 7/100: mean batch inertia: 88761.16200805843, ewa inertia: 88761.16200805843\n",
      "Minibatch step 8/100: mean batch inertia: 88037.6277930396, ewa inertia: 88037.6277930396\n",
      "Minibatch step 9/100: mean batch inertia: 87715.3538137042, ewa inertia: 87715.3538137042\n",
      "Minibatch step 10/100: mean batch inertia: 87567.43945859032, ewa inertia: 87567.43945859032\n",
      "Minibatch step 11/100: mean batch inertia: 88639.58005742384, ewa inertia: 88639.58005742384\n",
      "Minibatch step 12/100: mean batch inertia: 87840.1491377395, ewa inertia: 87840.1491377395\n",
      "Minibatch step 13/100: mean batch inertia: 88820.23141945362, ewa inertia: 88820.23141945362\n",
      "Minibatch step 14/100: mean batch inertia: 87958.4913121494, ewa inertia: 87958.4913121494\n",
      "Minibatch step 15/100: mean batch inertia: 87483.31260722014, ewa inertia: 87483.31260722014\n",
      "Minibatch step 16/100: mean batch inertia: 87683.92439120861, ewa inertia: 87683.92439120861\n",
      "Minibatch step 17/100: mean batch inertia: 86954.43826243783, ewa inertia: 86954.43826243783\n",
      "Minibatch step 18/100: mean batch inertia: 87779.8353271028, ewa inertia: 87779.8353271028\n",
      "Minibatch step 19/100: mean batch inertia: 86672.85351434037, ewa inertia: 86672.85351434037\n",
      "Minibatch step 20/100: mean batch inertia: 87496.46402540989, ewa inertia: 87496.46402540989\n",
      "Minibatch step 21/100: mean batch inertia: 87990.29914080616, ewa inertia: 87990.29914080616\n",
      "Minibatch step 22/100: mean batch inertia: 87601.4229727468, ewa inertia: 87601.4229727468\n",
      "Minibatch step 23/100: mean batch inertia: 87471.2507845818, ewa inertia: 87471.2507845818\n",
      "Minibatch step 24/100: mean batch inertia: 86874.58906830753, ewa inertia: 86874.58906830753\n",
      "Minibatch step 25/100: mean batch inertia: 87555.40912872266, ewa inertia: 87555.40912872266\n",
      "Minibatch step 26/100: mean batch inertia: 87655.61905675802, ewa inertia: 87655.61905675802\n",
      "Minibatch step 27/100: mean batch inertia: 87691.1814812, ewa inertia: 87691.1814812\n",
      "Minibatch step 28/100: mean batch inertia: 87777.85370634556, ewa inertia: 87777.85370634556\n",
      "Minibatch step 29/100: mean batch inertia: 87829.57167697177, ewa inertia: 87829.57167697177\n",
      "Converged (lack of improvement in inertia) at step 29/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 291137080.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 288345983.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 289987709.0\n",
      "Minibatch step 1/100: mean batch inertia: 139069.26089037818\n",
      "Minibatch step 2/100: mean batch inertia: 92409.98060400499, ewa inertia: 92409.98060400499\n",
      "Minibatch step 3/100: mean batch inertia: 90660.73439915954, ewa inertia: 90660.73439915954\n",
      "Minibatch step 4/100: mean batch inertia: 90692.0809819683, ewa inertia: 90692.0809819683\n",
      "Minibatch step 5/100: mean batch inertia: 89413.59920489811, ewa inertia: 89413.59920489811\n",
      "Minibatch step 6/100: mean batch inertia: 89631.91695428951, ewa inertia: 89631.91695428951\n",
      "Minibatch step 7/100: mean batch inertia: 89331.38052029753, ewa inertia: 89331.38052029753\n",
      "Minibatch step 8/100: mean batch inertia: 89328.93807228972, ewa inertia: 89328.93807228972\n",
      "Minibatch step 9/100: mean batch inertia: 88612.09314254251, ewa inertia: 88612.09314254251\n",
      "Minibatch step 10/100: mean batch inertia: 88842.38438633598, ewa inertia: 88842.38438633598\n",
      "Minibatch step 11/100: mean batch inertia: 88815.16575115181, ewa inertia: 88815.16575115181\n",
      "Minibatch step 12/100: mean batch inertia: 87995.4428230087, ewa inertia: 87995.4428230087\n",
      "Minibatch step 13/100: mean batch inertia: 87629.88585170743, ewa inertia: 87629.88585170743\n",
      "Minibatch step 14/100: mean batch inertia: 89255.11163267615, ewa inertia: 89255.11163267615\n",
      "Minibatch step 15/100: mean batch inertia: 87493.3445425544, ewa inertia: 87493.3445425544\n",
      "Minibatch step 16/100: mean batch inertia: 87717.62858828693, ewa inertia: 87717.62858828693\n",
      "Minibatch step 17/100: mean batch inertia: 88492.75865329227, ewa inertia: 88492.75865329227\n",
      "Minibatch step 18/100: mean batch inertia: 88597.33881261865, ewa inertia: 88597.33881261865\n",
      "Minibatch step 19/100: mean batch inertia: 88099.03921611942, ewa inertia: 88099.03921611942\n",
      "Minibatch step 20/100: mean batch inertia: 89032.08112841443, ewa inertia: 89032.08112841443\n",
      "Minibatch step 21/100: mean batch inertia: 88279.53625808438, ewa inertia: 88279.53625808438\n",
      "Minibatch step 22/100: mean batch inertia: 88973.97824271904, ewa inertia: 88973.97824271904\n",
      "Minibatch step 23/100: mean batch inertia: 87785.52159723865, ewa inertia: 87785.52159723865\n",
      "Minibatch step 24/100: mean batch inertia: 88505.51881207108, ewa inertia: 88505.51881207108\n",
      "Minibatch step 25/100: mean batch inertia: 88321.80184984655, ewa inertia: 88321.80184984655\n",
      "Converged (lack of improvement in inertia) at step 25/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 286509410.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 288475571.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 283569351.0\n",
      "Minibatch step 1/100: mean batch inertia: 134925.03015797032\n",
      "Minibatch step 2/100: mean batch inertia: 91764.39459862442, ewa inertia: 91764.39459862442\n",
      "Minibatch step 3/100: mean batch inertia: 90272.07487947485, ewa inertia: 90272.07487947485\n",
      "Minibatch step 4/100: mean batch inertia: 89953.9849998422, ewa inertia: 89953.9849998422\n",
      "Minibatch step 5/100: mean batch inertia: 88765.09504540403, ewa inertia: 88765.09504540403\n",
      "Minibatch step 6/100: mean batch inertia: 88725.3717342213, ewa inertia: 88725.3717342213\n",
      "Minibatch step 7/100: mean batch inertia: 88947.31192176358, ewa inertia: 88947.31192176358\n",
      "Minibatch step 8/100: mean batch inertia: 88981.41406691824, ewa inertia: 88981.41406691824\n",
      "Minibatch step 9/100: mean batch inertia: 89053.14965927752, ewa inertia: 89053.14965927752\n",
      "Minibatch step 10/100: mean batch inertia: 88293.5598732171, ewa inertia: 88293.5598732171\n",
      "Minibatch step 11/100: mean batch inertia: 87431.01667731076, ewa inertia: 87431.01667731076\n",
      "Minibatch step 12/100: mean batch inertia: 88622.28321318871, ewa inertia: 88622.28321318871\n",
      "Minibatch step 13/100: mean batch inertia: 86958.68082036535, ewa inertia: 86958.68082036535\n",
      "Minibatch step 14/100: mean batch inertia: 87718.9700255129, ewa inertia: 87718.9700255129\n",
      "Minibatch step 15/100: mean batch inertia: 88313.65581279104, ewa inertia: 88313.65581279104\n",
      "Minibatch step 16/100: mean batch inertia: 88991.46715894454, ewa inertia: 88991.46715894454\n",
      "Minibatch step 17/100: mean batch inertia: 87279.8852927536, ewa inertia: 87279.8852927536\n",
      "Minibatch step 18/100: mean batch inertia: 88987.21034535734, ewa inertia: 88987.21034535734\n",
      "Minibatch step 19/100: mean batch inertia: 87488.73384296174, ewa inertia: 87488.73384296174\n",
      "Minibatch step 20/100: mean batch inertia: 88330.08584331178, ewa inertia: 88330.08584331178\n",
      "Minibatch step 21/100: mean batch inertia: 88544.05042450389, ewa inertia: 88544.05042450389\n",
      "Minibatch step 22/100: mean batch inertia: 88261.0030322596, ewa inertia: 88261.0030322596\n",
      "Minibatch step 23/100: mean batch inertia: 88073.57170978442, ewa inertia: 88073.57170978442\n",
      "Converged (lack of improvement in inertia) at step 23/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 290616692.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 288773847.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 287596693.0\n",
      "Minibatch step 1/100: mean batch inertia: 136140.17472474868\n",
      "Minibatch step 2/100: mean batch inertia: 91714.36000406521, ewa inertia: 91714.36000406521\n",
      "Minibatch step 3/100: mean batch inertia: 89997.2407909244, ewa inertia: 89997.2407909244\n",
      "Minibatch step 4/100: mean batch inertia: 89235.17598386541, ewa inertia: 89235.17598386541\n",
      "Minibatch step 5/100: mean batch inertia: 89147.34690594998, ewa inertia: 89147.34690594998\n",
      "Minibatch step 6/100: mean batch inertia: 88282.03158105268, ewa inertia: 88282.03158105268\n",
      "Minibatch step 7/100: mean batch inertia: 88976.28888119991, ewa inertia: 88976.28888119991\n",
      "Minibatch step 8/100: mean batch inertia: 88427.82042322567, ewa inertia: 88427.82042322567\n",
      "Minibatch step 9/100: mean batch inertia: 89358.81578394382, ewa inertia: 89358.81578394382\n",
      "Minibatch step 10/100: mean batch inertia: 88119.00741290532, ewa inertia: 88119.00741290532\n",
      "Minibatch step 11/100: mean batch inertia: 87781.04464971, ewa inertia: 87781.04464971\n",
      "Minibatch step 12/100: mean batch inertia: 89010.92809286473, ewa inertia: 89010.92809286473\n",
      "Minibatch step 13/100: mean batch inertia: 88654.77264661905, ewa inertia: 88654.77264661905\n",
      "Minibatch step 14/100: mean batch inertia: 88321.3580265966, ewa inertia: 88321.3580265966\n",
      "Minibatch step 15/100: mean batch inertia: 87775.76765329934, ewa inertia: 87775.76765329934\n",
      "Minibatch step 16/100: mean batch inertia: 87417.80194305094, ewa inertia: 87417.80194305094\n",
      "Minibatch step 17/100: mean batch inertia: 88396.73510550626, ewa inertia: 88396.73510550626\n",
      "Minibatch step 18/100: mean batch inertia: 87290.74010677726, ewa inertia: 87290.74010677726\n",
      "Minibatch step 19/100: mean batch inertia: 88055.3773844704, ewa inertia: 88055.3773844704\n",
      "Minibatch step 20/100: mean batch inertia: 89204.31426355879, ewa inertia: 89204.31426355879\n",
      "Minibatch step 21/100: mean batch inertia: 87787.08947508273, ewa inertia: 87787.08947508273\n",
      "Minibatch step 22/100: mean batch inertia: 88163.64453396475, ewa inertia: 88163.64453396475\n",
      "Minibatch step 23/100: mean batch inertia: 87157.6031455694, ewa inertia: 87157.6031455694\n",
      "Minibatch step 24/100: mean batch inertia: 88693.25671940061, ewa inertia: 88693.25671940061\n",
      "Minibatch step 25/100: mean batch inertia: 86650.48081073149, ewa inertia: 86650.48081073149\n",
      "Minibatch step 26/100: mean batch inertia: 88057.94064869218, ewa inertia: 88057.94064869218\n",
      "Minibatch step 27/100: mean batch inertia: 87110.17728435056, ewa inertia: 87110.17728435056\n",
      "Minibatch step 28/100: mean batch inertia: 87715.35820661973, ewa inertia: 87715.35820661973\n",
      "Minibatch step 29/100: mean batch inertia: 88138.89185856293, ewa inertia: 88138.89185856293\n",
      "Minibatch step 30/100: mean batch inertia: 88711.2565681657, ewa inertia: 88711.2565681657\n",
      "Minibatch step 31/100: mean batch inertia: 87179.14058296675, ewa inertia: 87179.14058296675\n",
      "Minibatch step 32/100: mean batch inertia: 87790.46032036169, ewa inertia: 87790.46032036169\n",
      "Minibatch step 33/100: mean batch inertia: 88458.89166825158, ewa inertia: 88458.89166825158\n",
      "Minibatch step 34/100: mean batch inertia: 88073.0219931393, ewa inertia: 88073.0219931393\n",
      "Minibatch step 35/100: mean batch inertia: 87448.82485734591, ewa inertia: 87448.82485734591\n",
      "Converged (lack of improvement in inertia) at step 35/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 282792866.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 287486278.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 284434342.0\n",
      "Minibatch step 1/100: mean batch inertia: 137759.70703685973\n",
      "Minibatch step 2/100: mean batch inertia: 92086.44563964858, ewa inertia: 92086.44563964858\n",
      "Minibatch step 3/100: mean batch inertia: 90787.68523745016, ewa inertia: 90787.68523745016\n",
      "Minibatch step 4/100: mean batch inertia: 90665.78133357449, ewa inertia: 90665.78133357449\n",
      "Minibatch step 5/100: mean batch inertia: 90461.86626636017, ewa inertia: 90461.86626636017\n",
      "Minibatch step 6/100: mean batch inertia: 89476.91834111688, ewa inertia: 89476.91834111688\n",
      "Minibatch step 7/100: mean batch inertia: 89345.86376380544, ewa inertia: 89345.86376380544\n",
      "Minibatch step 8/100: mean batch inertia: 90375.22876802956, ewa inertia: 90375.22876802956\n",
      "Minibatch step 9/100: mean batch inertia: 88969.12604590341, ewa inertia: 88969.12604590341\n",
      "Minibatch step 10/100: mean batch inertia: 89481.19694970212, ewa inertia: 89481.19694970212\n",
      "Minibatch step 11/100: mean batch inertia: 90021.22469794667, ewa inertia: 90021.22469794667\n",
      "Minibatch step 12/100: mean batch inertia: 89478.27833129368, ewa inertia: 89478.27833129368\n",
      "Minibatch step 13/100: mean batch inertia: 88840.24954788157, ewa inertia: 88840.24954788157\n",
      "Minibatch step 14/100: mean batch inertia: 89534.24348337432, ewa inertia: 89534.24348337432\n",
      "Minibatch step 15/100: mean batch inertia: 88663.19971688467, ewa inertia: 88663.19971688467\n",
      "Minibatch step 16/100: mean batch inertia: 88770.44040182092, ewa inertia: 88770.44040182092\n",
      "Minibatch step 17/100: mean batch inertia: 89115.61430597327, ewa inertia: 89115.61430597327\n",
      "Minibatch step 18/100: mean batch inertia: 87817.37152104524, ewa inertia: 87817.37152104524\n",
      "Minibatch step 19/100: mean batch inertia: 88447.9998257692, ewa inertia: 88447.9998257692\n",
      "Minibatch step 20/100: mean batch inertia: 88155.86771543285, ewa inertia: 88155.86771543285\n",
      "Minibatch step 21/100: mean batch inertia: 90050.62387143086, ewa inertia: 90050.62387143086\n",
      "Minibatch step 22/100: mean batch inertia: 88875.03648717, ewa inertia: 88875.03648717\n",
      "Minibatch step 23/100: mean batch inertia: 88822.40474749217, ewa inertia: 88822.40474749217\n",
      "Minibatch step 24/100: mean batch inertia: 88367.61448412818, ewa inertia: 88367.61448412818\n",
      "Minibatch step 25/100: mean batch inertia: 88978.57262750382, ewa inertia: 88978.57262750382\n",
      "Minibatch step 26/100: mean batch inertia: 89359.09420890317, ewa inertia: 89359.09420890317\n",
      "Minibatch step 27/100: mean batch inertia: 89321.22997472236, ewa inertia: 89321.22997472236\n",
      "Minibatch step 28/100: mean batch inertia: 88996.29058671798, ewa inertia: 88996.29058671798\n",
      "Converged (lack of improvement in inertia) at step 28/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 288219207.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 285348648.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 281733061.0\n",
      "Minibatch step 1/100: mean batch inertia: 135075.9597893729\n",
      "Minibatch step 2/100: mean batch inertia: 90593.6764677315, ewa inertia: 90593.6764677315\n",
      "Minibatch step 3/100: mean batch inertia: 89505.44898512168, ewa inertia: 89505.44898512168\n",
      "Minibatch step 4/100: mean batch inertia: 89326.09448800544, ewa inertia: 89326.09448800544\n",
      "Minibatch step 5/100: mean batch inertia: 89250.24069180964, ewa inertia: 89250.24069180964\n",
      "Minibatch step 6/100: mean batch inertia: 89128.81537410493, ewa inertia: 89128.81537410493\n",
      "Minibatch step 7/100: mean batch inertia: 89572.28783428739, ewa inertia: 89572.28783428739\n",
      "Minibatch step 8/100: mean batch inertia: 88119.87844016938, ewa inertia: 88119.87844016938\n",
      "Minibatch step 9/100: mean batch inertia: 88481.01554654661, ewa inertia: 88481.01554654661\n",
      "Minibatch step 10/100: mean batch inertia: 87932.70423024784, ewa inertia: 87932.70423024784\n",
      "Minibatch step 11/100: mean batch inertia: 88318.60491843718, ewa inertia: 88318.60491843718\n",
      "Minibatch step 12/100: mean batch inertia: 89190.89639381482, ewa inertia: 89190.89639381482\n",
      "Minibatch step 13/100: mean batch inertia: 88598.86317591008, ewa inertia: 88598.86317591008\n",
      "Minibatch step 14/100: mean batch inertia: 88156.67657496585, ewa inertia: 88156.67657496585\n",
      "Minibatch step 15/100: mean batch inertia: 87591.40042039636, ewa inertia: 87591.40042039636\n",
      "Minibatch step 16/100: mean batch inertia: 87906.27927345425, ewa inertia: 87906.27927345425\n",
      "Minibatch step 17/100: mean batch inertia: 88376.99953863378, ewa inertia: 88376.99953863378\n",
      "Minibatch step 18/100: mean batch inertia: 87830.29171431839, ewa inertia: 87830.29171431839\n",
      "Minibatch step 19/100: mean batch inertia: 87765.02342341705, ewa inertia: 87765.02342341705\n",
      "Minibatch step 20/100: mean batch inertia: 88656.57052106521, ewa inertia: 88656.57052106521\n",
      "Minibatch step 21/100: mean batch inertia: 88569.80064512383, ewa inertia: 88569.80064512383\n",
      "Minibatch step 22/100: mean batch inertia: 87464.6731524235, ewa inertia: 87464.6731524235\n",
      "Minibatch step 23/100: mean batch inertia: 87363.57604887073, ewa inertia: 87363.57604887073\n",
      "Minibatch step 24/100: mean batch inertia: 87306.73794519788, ewa inertia: 87306.73794519788\n",
      "Minibatch step 25/100: mean batch inertia: 88130.13861950823, ewa inertia: 88130.13861950823\n",
      "Minibatch step 26/100: mean batch inertia: 87916.89718723149, ewa inertia: 87916.89718723149\n",
      "Minibatch step 27/100: mean batch inertia: 88254.9916668897, ewa inertia: 88254.9916668897\n",
      "Minibatch step 28/100: mean batch inertia: 87653.74557922517, ewa inertia: 87653.74557922517\n",
      "Minibatch step 29/100: mean batch inertia: 86947.77578121833, ewa inertia: 86947.77578121833\n",
      "Minibatch step 30/100: mean batch inertia: 87687.89753667064, ewa inertia: 87687.89753667064\n",
      "Minibatch step 31/100: mean batch inertia: 87531.98461025227, ewa inertia: 87531.98461025227\n",
      "Minibatch step 32/100: mean batch inertia: 87471.1007211208, ewa inertia: 87471.1007211208\n",
      "Minibatch step 33/100: mean batch inertia: 87875.40201995429, ewa inertia: 87875.40201995429\n",
      "Minibatch step 34/100: mean batch inertia: 88166.5563022606, ewa inertia: 88166.5563022606\n",
      "Minibatch step 35/100: mean batch inertia: 89056.99082014816, ewa inertia: 89056.99082014816\n",
      "Minibatch step 36/100: mean batch inertia: 87630.31429561372, ewa inertia: 87630.31429561372\n",
      "Minibatch step 37/100: mean batch inertia: 87215.98713962316, ewa inertia: 87215.98713962316\n",
      "Minibatch step 38/100: mean batch inertia: 87644.86069165678, ewa inertia: 87644.86069165678\n",
      "Minibatch step 39/100: mean batch inertia: 87611.61427453868, ewa inertia: 87611.61427453868\n",
      "Converged (lack of improvement in inertia) at step 39/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 287366911.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 289932061.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 287317337.0\n",
      "Minibatch step 1/100: mean batch inertia: 139944.04212541887\n",
      "Minibatch step 2/100: mean batch inertia: 91744.36843390563, ewa inertia: 91744.36843390563\n",
      "Minibatch step 3/100: mean batch inertia: 91433.50770481514, ewa inertia: 91433.50770481514\n",
      "Minibatch step 4/100: mean batch inertia: 89803.35960276482, ewa inertia: 89803.35960276482\n",
      "Minibatch step 5/100: mean batch inertia: 90154.99817939915, ewa inertia: 90154.99817939915\n",
      "Minibatch step 6/100: mean batch inertia: 88861.09752680836, ewa inertia: 88861.09752680836\n",
      "Minibatch step 7/100: mean batch inertia: 88950.64544392459, ewa inertia: 88950.64544392459\n",
      "Minibatch step 8/100: mean batch inertia: 89188.17020172012, ewa inertia: 89188.17020172012\n",
      "Minibatch step 9/100: mean batch inertia: 89439.61809779269, ewa inertia: 89439.61809779269\n",
      "Minibatch step 10/100: mean batch inertia: 88713.02153067278, ewa inertia: 88713.02153067278\n",
      "Minibatch step 11/100: mean batch inertia: 89450.82145157737, ewa inertia: 89450.82145157737\n",
      "Minibatch step 12/100: mean batch inertia: 88365.6761327379, ewa inertia: 88365.6761327379\n",
      "Minibatch step 13/100: mean batch inertia: 88601.37907704426, ewa inertia: 88601.37907704426\n",
      "Minibatch step 14/100: mean batch inertia: 88965.91082834122, ewa inertia: 88965.91082834122\n",
      "Minibatch step 15/100: mean batch inertia: 88570.27415815268, ewa inertia: 88570.27415815268\n",
      "Minibatch step 16/100: mean batch inertia: 88674.63152947939, ewa inertia: 88674.63152947939\n",
      "Minibatch step 17/100: mean batch inertia: 89923.79494468881, ewa inertia: 89923.79494468881\n",
      "Minibatch step 18/100: mean batch inertia: 87808.77928441923, ewa inertia: 87808.77928441923\n",
      "Minibatch step 19/100: mean batch inertia: 88105.22873516503, ewa inertia: 88105.22873516503\n",
      "Minibatch step 20/100: mean batch inertia: 88279.47689749928, ewa inertia: 88279.47689749928\n",
      "Minibatch step 21/100: mean batch inertia: 88738.11743784054, ewa inertia: 88738.11743784054\n",
      "Minibatch step 22/100: mean batch inertia: 88183.95445247467, ewa inertia: 88183.95445247467\n",
      "Minibatch step 23/100: mean batch inertia: 88098.30980835784, ewa inertia: 88098.30980835784\n",
      "Minibatch step 24/100: mean batch inertia: 88879.78365523618, ewa inertia: 88879.78365523618\n",
      "Minibatch step 25/100: mean batch inertia: 88954.9468630129, ewa inertia: 88954.9468630129\n",
      "Minibatch step 26/100: mean batch inertia: 87768.16484477463, ewa inertia: 87768.16484477463\n",
      "Minibatch step 27/100: mean batch inertia: 88328.75936623247, ewa inertia: 88328.75936623247\n",
      "Minibatch step 28/100: mean batch inertia: 87838.98046989355, ewa inertia: 87838.98046989355\n",
      "Minibatch step 29/100: mean batch inertia: 87907.2609599408, ewa inertia: 87907.2609599408\n",
      "Minibatch step 30/100: mean batch inertia: 88812.46165917887, ewa inertia: 88812.46165917887\n",
      "Minibatch step 31/100: mean batch inertia: 88177.86255485372, ewa inertia: 88177.86255485372\n",
      "Minibatch step 32/100: mean batch inertia: 88087.7553964911, ewa inertia: 88087.7553964911\n",
      "Minibatch step 33/100: mean batch inertia: 87898.42057679739, ewa inertia: 87898.42057679739\n",
      "Minibatch step 34/100: mean batch inertia: 88038.51605586658, ewa inertia: 88038.51605586658\n",
      "Minibatch step 35/100: mean batch inertia: 87639.57200648783, ewa inertia: 87639.57200648783\n",
      "Minibatch step 36/100: mean batch inertia: 87694.2501844204, ewa inertia: 87694.2501844204\n",
      "Minibatch step 37/100: mean batch inertia: 88049.02927400627, ewa inertia: 88049.02927400627\n",
      "Minibatch step 38/100: mean batch inertia: 88220.28905076165, ewa inertia: 88220.28905076165\n",
      "Minibatch step 39/100: mean batch inertia: 88046.58654979819, ewa inertia: 88046.58654979819\n",
      "Minibatch step 40/100: mean batch inertia: 88832.84882975063, ewa inertia: 88832.84882975063\n",
      "Minibatch step 41/100: mean batch inertia: 88216.71558810658, ewa inertia: 88216.71558810658\n",
      "Minibatch step 42/100: mean batch inertia: 87923.03230134795, ewa inertia: 87923.03230134795\n",
      "Minibatch step 43/100: mean batch inertia: 88122.5425050817, ewa inertia: 88122.5425050817\n",
      "Minibatch step 44/100: mean batch inertia: 88634.60554589247, ewa inertia: 88634.60554589247\n",
      "Minibatch step 45/100: mean batch inertia: 88297.12475435332, ewa inertia: 88297.12475435332\n",
      "Converged (lack of improvement in inertia) at step 45/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 286654998.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 283456421.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 284281897.0\n",
      "Minibatch step 1/100: mean batch inertia: 138044.95021541408\n",
      "Minibatch step 2/100: mean batch inertia: 91990.08565533557, ewa inertia: 91990.08565533557\n",
      "Minibatch step 3/100: mean batch inertia: 90403.22860998745, ewa inertia: 90403.22860998745\n",
      "Minibatch step 4/100: mean batch inertia: 89063.39722097907, ewa inertia: 89063.39722097907\n",
      "Minibatch step 5/100: mean batch inertia: 89716.45363432803, ewa inertia: 89716.45363432803\n",
      "Minibatch step 6/100: mean batch inertia: 89607.08301292676, ewa inertia: 89607.08301292676\n",
      "Minibatch step 7/100: mean batch inertia: 89106.75628316148, ewa inertia: 89106.75628316148\n",
      "Minibatch step 8/100: mean batch inertia: 89408.50065959319, ewa inertia: 89408.50065959319\n",
      "Minibatch step 9/100: mean batch inertia: 88462.41228102797, ewa inertia: 88462.41228102797\n",
      "Minibatch step 10/100: mean batch inertia: 88560.76987508834, ewa inertia: 88560.76987508834\n",
      "Minibatch step 11/100: mean batch inertia: 88946.29578066686, ewa inertia: 88946.29578066686\n",
      "Minibatch step 12/100: mean batch inertia: 87837.91319833307, ewa inertia: 87837.91319833307\n",
      "Minibatch step 13/100: mean batch inertia: 87843.54310035173, ewa inertia: 87843.54310035173\n",
      "Minibatch step 14/100: mean batch inertia: 88439.44978884833, ewa inertia: 88439.44978884833\n",
      "Minibatch step 15/100: mean batch inertia: 89444.81118749172, ewa inertia: 89444.81118749172\n",
      "Minibatch step 16/100: mean batch inertia: 88158.84152176132, ewa inertia: 88158.84152176132\n",
      "Minibatch step 17/100: mean batch inertia: 87722.1581177616, ewa inertia: 87722.1581177616\n",
      "Minibatch step 18/100: mean batch inertia: 88020.4440394927, ewa inertia: 88020.4440394927\n",
      "Minibatch step 19/100: mean batch inertia: 87770.70537312844, ewa inertia: 87770.70537312844\n",
      "Minibatch step 20/100: mean batch inertia: 87237.8985219814, ewa inertia: 87237.8985219814\n",
      "Minibatch step 21/100: mean batch inertia: 87942.81345794728, ewa inertia: 87942.81345794728\n",
      "Minibatch step 22/100: mean batch inertia: 87556.08227082649, ewa inertia: 87556.08227082649\n",
      "Minibatch step 23/100: mean batch inertia: 87495.50723160715, ewa inertia: 87495.50723160715\n",
      "Minibatch step 24/100: mean batch inertia: 88468.54131995502, ewa inertia: 88468.54131995502\n",
      "Minibatch step 25/100: mean batch inertia: 87845.78390225579, ewa inertia: 87845.78390225579\n",
      "Minibatch step 26/100: mean batch inertia: 88386.3990834139, ewa inertia: 88386.3990834139\n",
      "Minibatch step 27/100: mean batch inertia: 87456.31917969522, ewa inertia: 87456.31917969522\n",
      "Minibatch step 28/100: mean batch inertia: 88778.59519372461, ewa inertia: 88778.59519372461\n",
      "Minibatch step 29/100: mean batch inertia: 87537.11267694429, ewa inertia: 87537.11267694429\n",
      "Minibatch step 30/100: mean batch inertia: 87732.07804589177, ewa inertia: 87732.07804589177\n",
      "Converged (lack of improvement in inertia) at step 30/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 289724466.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 287046198.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 284996233.0\n",
      "Minibatch step 1/100: mean batch inertia: 136400.16515078986\n",
      "Minibatch step 2/100: mean batch inertia: 91983.88024198968, ewa inertia: 91983.88024198968\n",
      "Minibatch step 3/100: mean batch inertia: 91757.06807029838, ewa inertia: 91757.06807029838\n",
      "Minibatch step 4/100: mean batch inertia: 90382.12741409143, ewa inertia: 90382.12741409143\n",
      "Minibatch step 5/100: mean batch inertia: 90162.20349660741, ewa inertia: 90162.20349660741\n",
      "Minibatch step 6/100: mean batch inertia: 90309.24807774293, ewa inertia: 90309.24807774293\n",
      "Minibatch step 7/100: mean batch inertia: 89369.9913266299, ewa inertia: 89369.9913266299\n",
      "Minibatch step 8/100: mean batch inertia: 89011.75826778282, ewa inertia: 89011.75826778282\n",
      "Minibatch step 9/100: mean batch inertia: 89242.00407138029, ewa inertia: 89242.00407138029\n",
      "Minibatch step 10/100: mean batch inertia: 89269.7499234776, ewa inertia: 89269.7499234776\n",
      "Minibatch step 11/100: mean batch inertia: 89988.13959847613, ewa inertia: 89988.13959847613\n",
      "Minibatch step 12/100: mean batch inertia: 88607.34360412715, ewa inertia: 88607.34360412715\n",
      "Minibatch step 13/100: mean batch inertia: 89253.28156224226, ewa inertia: 89253.28156224226\n",
      "Minibatch step 14/100: mean batch inertia: 87876.14740605363, ewa inertia: 87876.14740605363\n",
      "Minibatch step 15/100: mean batch inertia: 89905.8001060595, ewa inertia: 89905.8001060595\n",
      "Minibatch step 16/100: mean batch inertia: 88379.69706105284, ewa inertia: 88379.69706105284\n",
      "Minibatch step 17/100: mean batch inertia: 89296.4055955199, ewa inertia: 89296.4055955199\n",
      "Minibatch step 18/100: mean batch inertia: 88881.52843170066, ewa inertia: 88881.52843170066\n",
      "Minibatch step 19/100: mean batch inertia: 87280.05678331995, ewa inertia: 87280.05678331995\n",
      "Minibatch step 20/100: mean batch inertia: 88222.2866672698, ewa inertia: 88222.2866672698\n",
      "Minibatch step 21/100: mean batch inertia: 88610.21988956268, ewa inertia: 88610.21988956268\n",
      "Minibatch step 22/100: mean batch inertia: 88298.169155481, ewa inertia: 88298.169155481\n",
      "Minibatch step 23/100: mean batch inertia: 88536.48103736884, ewa inertia: 88536.48103736884\n",
      "Minibatch step 24/100: mean batch inertia: 88139.24743909559, ewa inertia: 88139.24743909559\n",
      "Minibatch step 25/100: mean batch inertia: 88666.6455500563, ewa inertia: 88666.6455500563\n",
      "Minibatch step 26/100: mean batch inertia: 88204.05262224801, ewa inertia: 88204.05262224801\n",
      "Minibatch step 27/100: mean batch inertia: 88245.92565913043, ewa inertia: 88245.92565913043\n",
      "Minibatch step 28/100: mean batch inertia: 87887.84069975137, ewa inertia: 87887.84069975137\n",
      "Minibatch step 29/100: mean batch inertia: 88114.82735382185, ewa inertia: 88114.82735382185\n",
      "Converged (lack of improvement in inertia) at step 29/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 285001565.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 287112735.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 282692725.0\n",
      "Minibatch step 1/100: mean batch inertia: 135949.39636189563\n",
      "Minibatch step 2/100: mean batch inertia: 91369.4288215292, ewa inertia: 91369.4288215292\n",
      "Minibatch step 3/100: mean batch inertia: 89944.10025518796, ewa inertia: 89944.10025518796\n",
      "Minibatch step 4/100: mean batch inertia: 89188.9994627241, ewa inertia: 89188.9994627241\n",
      "Minibatch step 5/100: mean batch inertia: 89444.44985384705, ewa inertia: 89444.44985384705\n",
      "Minibatch step 6/100: mean batch inertia: 88629.47709913262, ewa inertia: 88629.47709913262\n",
      "Minibatch step 7/100: mean batch inertia: 88470.82996406217, ewa inertia: 88470.82996406217\n",
      "Minibatch step 8/100: mean batch inertia: 87839.84712895681, ewa inertia: 87839.84712895681\n",
      "Minibatch step 9/100: mean batch inertia: 88272.88262305764, ewa inertia: 88272.88262305764\n",
      "Minibatch step 10/100: mean batch inertia: 89406.14673802619, ewa inertia: 89406.14673802619\n",
      "Minibatch step 11/100: mean batch inertia: 88111.55867141194, ewa inertia: 88111.55867141194\n",
      "Minibatch step 12/100: mean batch inertia: 87741.17460727674, ewa inertia: 87741.17460727674\n",
      "Minibatch step 13/100: mean batch inertia: 87061.02573024307, ewa inertia: 87061.02573024307\n",
      "Minibatch step 14/100: mean batch inertia: 88259.121017353, ewa inertia: 88259.121017353\n",
      "Minibatch step 15/100: mean batch inertia: 88219.23594681872, ewa inertia: 88219.23594681872\n",
      "Minibatch step 16/100: mean batch inertia: 88250.5686482274, ewa inertia: 88250.5686482274\n",
      "Minibatch step 17/100: mean batch inertia: 88057.38488831121, ewa inertia: 88057.38488831121\n",
      "Minibatch step 18/100: mean batch inertia: 88826.4347891763, ewa inertia: 88826.4347891763\n",
      "Minibatch step 19/100: mean batch inertia: 87212.60664750877, ewa inertia: 87212.60664750877\n",
      "Minibatch step 20/100: mean batch inertia: 88742.55397703647, ewa inertia: 88742.55397703647\n",
      "Minibatch step 21/100: mean batch inertia: 87706.74873347867, ewa inertia: 87706.74873347867\n",
      "Minibatch step 22/100: mean batch inertia: 89007.85566383581, ewa inertia: 89007.85566383581\n",
      "Minibatch step 23/100: mean batch inertia: 87725.41677718869, ewa inertia: 87725.41677718869\n",
      "Converged (lack of improvement in inertia) at step 23/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 284218770.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 290264917.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 287128838.0\n",
      "Minibatch step 1/100: mean batch inertia: 136973.52848252753\n",
      "Minibatch step 2/100: mean batch inertia: 91504.38423756321, ewa inertia: 91504.38423756321\n",
      "Minibatch step 3/100: mean batch inertia: 90616.79544932967, ewa inertia: 90616.79544932967\n",
      "Minibatch step 4/100: mean batch inertia: 90530.59760227968, ewa inertia: 90530.59760227968\n",
      "Minibatch step 5/100: mean batch inertia: 89049.83309652048, ewa inertia: 89049.83309652048\n",
      "Minibatch step 6/100: mean batch inertia: 89389.08295373282, ewa inertia: 89389.08295373282\n",
      "Minibatch step 7/100: mean batch inertia: 90263.38013536009, ewa inertia: 90263.38013536009\n",
      "Minibatch step 8/100: mean batch inertia: 89079.49723722303, ewa inertia: 89079.49723722303\n",
      "Minibatch step 9/100: mean batch inertia: 88258.93729594268, ewa inertia: 88258.93729594268\n",
      "Minibatch step 10/100: mean batch inertia: 88535.73763469659, ewa inertia: 88535.73763469659\n",
      "Minibatch step 11/100: mean batch inertia: 88537.25062208198, ewa inertia: 88537.25062208198\n",
      "Minibatch step 12/100: mean batch inertia: 88554.94641700364, ewa inertia: 88554.94641700364\n",
      "Minibatch step 13/100: mean batch inertia: 87956.7217879168, ewa inertia: 87956.7217879168\n",
      "Minibatch step 14/100: mean batch inertia: 87985.23071360697, ewa inertia: 87985.23071360697\n",
      "Minibatch step 15/100: mean batch inertia: 88583.42075029084, ewa inertia: 88583.42075029084\n",
      "Minibatch step 16/100: mean batch inertia: 88496.00479401292, ewa inertia: 88496.00479401292\n",
      "Minibatch step 17/100: mean batch inertia: 88572.05114996733, ewa inertia: 88572.05114996733\n",
      "Minibatch step 18/100: mean batch inertia: 87336.5240818283, ewa inertia: 87336.5240818283\n",
      "Minibatch step 19/100: mean batch inertia: 88047.01276256621, ewa inertia: 88047.01276256621\n",
      "Minibatch step 20/100: mean batch inertia: 88733.85921105593, ewa inertia: 88733.85921105593\n",
      "Minibatch step 21/100: mean batch inertia: 88781.3542731102, ewa inertia: 88781.3542731102\n",
      "Minibatch step 22/100: mean batch inertia: 87955.00799050952, ewa inertia: 87955.00799050952\n",
      "Minibatch step 23/100: mean batch inertia: 88076.0976496367, ewa inertia: 88076.0976496367\n",
      "Minibatch step 24/100: mean batch inertia: 87830.39152742724, ewa inertia: 87830.39152742724\n",
      "Minibatch step 25/100: mean batch inertia: 86817.08399881715, ewa inertia: 86817.08399881715\n",
      "Minibatch step 26/100: mean batch inertia: 88503.16797749758, ewa inertia: 88503.16797749758\n",
      "Minibatch step 27/100: mean batch inertia: 87979.76620805454, ewa inertia: 87979.76620805454\n",
      "Minibatch step 28/100: mean batch inertia: 87689.80917786493, ewa inertia: 87689.80917786493\n",
      "Minibatch step 29/100: mean batch inertia: 88121.64845885702, ewa inertia: 88121.64845885702\n",
      "Minibatch step 30/100: mean batch inertia: 88358.7572653551, ewa inertia: 88358.7572653551\n",
      "Minibatch step 31/100: mean batch inertia: 88218.12765620978, ewa inertia: 88218.12765620978\n",
      "Minibatch step 32/100: mean batch inertia: 87880.79924470713, ewa inertia: 87880.79924470713\n",
      "Minibatch step 33/100: mean batch inertia: 87695.91560970752, ewa inertia: 87695.91560970752\n",
      "Minibatch step 34/100: mean batch inertia: 87811.86232958703, ewa inertia: 87811.86232958703\n",
      "Minibatch step 35/100: mean batch inertia: 88623.88727768164, ewa inertia: 88623.88727768164\n",
      "Converged (lack of improvement in inertia) at step 35/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 291378006.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 292012832.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 285588347.0\n",
      "Minibatch step 1/100: mean batch inertia: 136216.87888942077\n",
      "Minibatch step 2/100: mean batch inertia: 90940.9235330544, ewa inertia: 90940.9235330544\n",
      "Minibatch step 3/100: mean batch inertia: 89874.40961309311, ewa inertia: 89874.40961309311\n",
      "Minibatch step 4/100: mean batch inertia: 89332.09796469669, ewa inertia: 89332.09796469669\n",
      "Minibatch step 5/100: mean batch inertia: 88986.4443708917, ewa inertia: 88986.4443708917\n",
      "Minibatch step 6/100: mean batch inertia: 88664.00215713984, ewa inertia: 88664.00215713984\n",
      "Minibatch step 7/100: mean batch inertia: 88530.58611893952, ewa inertia: 88530.58611893952\n",
      "Minibatch step 8/100: mean batch inertia: 88977.73296063405, ewa inertia: 88977.73296063405\n",
      "Minibatch step 9/100: mean batch inertia: 88665.48949702314, ewa inertia: 88665.48949702314\n",
      "Minibatch step 10/100: mean batch inertia: 87795.57427303483, ewa inertia: 87795.57427303483\n",
      "Minibatch step 11/100: mean batch inertia: 88877.88703233367, ewa inertia: 88877.88703233367\n",
      "Minibatch step 12/100: mean batch inertia: 88563.98710922955, ewa inertia: 88563.98710922955\n",
      "Minibatch step 13/100: mean batch inertia: 88227.01902609708, ewa inertia: 88227.01902609708\n",
      "Minibatch step 14/100: mean batch inertia: 88067.69221722322, ewa inertia: 88067.69221722322\n",
      "Minibatch step 15/100: mean batch inertia: 87612.78813335818, ewa inertia: 87612.78813335818\n",
      "Minibatch step 16/100: mean batch inertia: 87680.47510229713, ewa inertia: 87680.47510229713\n",
      "Minibatch step 17/100: mean batch inertia: 88177.30495110877, ewa inertia: 88177.30495110877\n",
      "Minibatch step 18/100: mean batch inertia: 86933.62932914411, ewa inertia: 86933.62932914411\n",
      "Minibatch step 19/100: mean batch inertia: 88704.08370581643, ewa inertia: 88704.08370581643\n",
      "Minibatch step 20/100: mean batch inertia: 88147.29239079313, ewa inertia: 88147.29239079313\n",
      "Minibatch step 21/100: mean batch inertia: 87160.85895954927, ewa inertia: 87160.85895954927\n",
      "Minibatch step 22/100: mean batch inertia: 86843.86686497268, ewa inertia: 86843.86686497268\n",
      "Minibatch step 23/100: mean batch inertia: 87207.82387951536, ewa inertia: 87207.82387951536\n",
      "Minibatch step 24/100: mean batch inertia: 87523.51265015994, ewa inertia: 87523.51265015994\n",
      "Minibatch step 25/100: mean batch inertia: 88232.81541858746, ewa inertia: 88232.81541858746\n",
      "Minibatch step 26/100: mean batch inertia: 87847.43929775317, ewa inertia: 87847.43929775317\n",
      "Minibatch step 27/100: mean batch inertia: 87140.43460663616, ewa inertia: 87140.43460663616\n",
      "Minibatch step 28/100: mean batch inertia: 87745.58026371806, ewa inertia: 87745.58026371806\n",
      "Minibatch step 29/100: mean batch inertia: 88010.19602352232, ewa inertia: 88010.19602352232\n",
      "Minibatch step 30/100: mean batch inertia: 87644.17384007195, ewa inertia: 87644.17384007195\n",
      "Minibatch step 31/100: mean batch inertia: 88103.0144719293, ewa inertia: 88103.0144719293\n",
      "Minibatch step 32/100: mean batch inertia: 86845.97630189365, ewa inertia: 86845.97630189365\n",
      "Converged (lack of improvement in inertia) at step 32/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 284515313.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 288693375.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 284312382.0\n",
      "Minibatch step 1/100: mean batch inertia: 138951.20679751076\n",
      "Minibatch step 2/100: mean batch inertia: 91914.11042847371, ewa inertia: 91914.11042847371\n",
      "Minibatch step 3/100: mean batch inertia: 89782.06529601687, ewa inertia: 89782.06529601687\n",
      "Minibatch step 4/100: mean batch inertia: 89454.8778983082, ewa inertia: 89454.8778983082\n",
      "Minibatch step 5/100: mean batch inertia: 89227.42873290119, ewa inertia: 89227.42873290119\n",
      "Minibatch step 6/100: mean batch inertia: 90076.80098852377, ewa inertia: 90076.80098852377\n",
      "Minibatch step 7/100: mean batch inertia: 88821.30681513858, ewa inertia: 88821.30681513858\n",
      "Minibatch step 8/100: mean batch inertia: 88367.68750590215, ewa inertia: 88367.68750590215\n",
      "Minibatch step 9/100: mean batch inertia: 88516.90955420045, ewa inertia: 88516.90955420045\n",
      "Minibatch step 10/100: mean batch inertia: 88953.93156285021, ewa inertia: 88953.93156285021\n",
      "Minibatch step 11/100: mean batch inertia: 88586.99725175866, ewa inertia: 88586.99725175866\n",
      "Minibatch step 12/100: mean batch inertia: 88843.7337236348, ewa inertia: 88843.7337236348\n",
      "Minibatch step 13/100: mean batch inertia: 88536.21753026253, ewa inertia: 88536.21753026253\n",
      "Minibatch step 14/100: mean batch inertia: 87924.43243961166, ewa inertia: 87924.43243961166\n",
      "Minibatch step 15/100: mean batch inertia: 88027.691593925, ewa inertia: 88027.691593925\n",
      "Minibatch step 16/100: mean batch inertia: 88676.86260476144, ewa inertia: 88676.86260476144\n",
      "Minibatch step 17/100: mean batch inertia: 87968.66912198039, ewa inertia: 87968.66912198039\n",
      "Minibatch step 18/100: mean batch inertia: 88205.70695584925, ewa inertia: 88205.70695584925\n",
      "Minibatch step 19/100: mean batch inertia: 87849.2760945806, ewa inertia: 87849.2760945806\n",
      "Minibatch step 20/100: mean batch inertia: 88323.47918542157, ewa inertia: 88323.47918542157\n",
      "Minibatch step 21/100: mean batch inertia: 88069.18235502, ewa inertia: 88069.18235502\n",
      "Minibatch step 22/100: mean batch inertia: 87623.69031739859, ewa inertia: 87623.69031739859\n",
      "Minibatch step 23/100: mean batch inertia: 88702.28170201887, ewa inertia: 88702.28170201887\n",
      "Minibatch step 24/100: mean batch inertia: 88486.02179680196, ewa inertia: 88486.02179680196\n",
      "Minibatch step 25/100: mean batch inertia: 88472.06379342116, ewa inertia: 88472.06379342116\n",
      "Minibatch step 26/100: mean batch inertia: 87670.6904325074, ewa inertia: 87670.6904325074\n",
      "Minibatch step 27/100: mean batch inertia: 87926.46427609732, ewa inertia: 87926.46427609732\n",
      "Minibatch step 28/100: mean batch inertia: 88021.41521253507, ewa inertia: 88021.41521253507\n",
      "Minibatch step 29/100: mean batch inertia: 87954.73918497223, ewa inertia: 87954.73918497223\n",
      "Minibatch step 30/100: mean batch inertia: 87461.3958373953, ewa inertia: 87461.3958373953\n",
      "Minibatch step 31/100: mean batch inertia: 88146.09227165015, ewa inertia: 88146.09227165015\n",
      "Minibatch step 32/100: mean batch inertia: 87992.21603944687, ewa inertia: 87992.21603944687\n",
      "Minibatch step 33/100: mean batch inertia: 88073.57726482199, ewa inertia: 88073.57726482199\n",
      "Minibatch step 34/100: mean batch inertia: 88375.67551483627, ewa inertia: 88375.67551483627\n",
      "Minibatch step 35/100: mean batch inertia: 87442.27372587446, ewa inertia: 87442.27372587446\n",
      "Minibatch step 36/100: mean batch inertia: 87587.69474361386, ewa inertia: 87587.69474361386\n",
      "Minibatch step 37/100: mean batch inertia: 88008.73417788745, ewa inertia: 88008.73417788745\n",
      "Minibatch step 38/100: mean batch inertia: 87692.83197706936, ewa inertia: 87692.83197706936\n",
      "Minibatch step 39/100: mean batch inertia: 88159.35212356703, ewa inertia: 88159.35212356703\n",
      "Minibatch step 40/100: mean batch inertia: 89083.77703816822, ewa inertia: 89083.77703816822\n",
      "Minibatch step 41/100: mean batch inertia: 88106.76188313586, ewa inertia: 88106.76188313586\n",
      "Minibatch step 42/100: mean batch inertia: 87334.99054708162, ewa inertia: 87334.99054708162\n",
      "Minibatch step 43/100: mean batch inertia: 87292.83026454136, ewa inertia: 87292.83026454136\n",
      "Minibatch step 44/100: mean batch inertia: 87740.52369086158, ewa inertia: 87740.52369086158\n",
      "Minibatch step 45/100: mean batch inertia: 88209.95210955676, ewa inertia: 88209.95210955676\n",
      "Minibatch step 46/100: mean batch inertia: 87526.10887441112, ewa inertia: 87526.10887441112\n",
      "Minibatch step 47/100: mean batch inertia: 87861.59945612439, ewa inertia: 87861.59945612439\n",
      "Minibatch step 48/100: mean batch inertia: 87704.81700379637, ewa inertia: 87704.81700379637\n",
      "Minibatch step 49/100: mean batch inertia: 87569.16100732738, ewa inertia: 87569.16100732738\n",
      "Minibatch step 50/100: mean batch inertia: 87633.28253891789, ewa inertia: 87633.28253891789\n",
      "Minibatch step 51/100: mean batch inertia: 87949.49839995182, ewa inertia: 87949.49839995182\n",
      "Minibatch step 52/100: mean batch inertia: 87747.04850217135, ewa inertia: 87747.04850217135\n",
      "Minibatch step 53/100: mean batch inertia: 87191.74953202216, ewa inertia: 87191.74953202216\n",
      "Minibatch step 54/100: mean batch inertia: 88425.76271083727, ewa inertia: 88425.76271083727\n",
      "Minibatch step 55/100: mean batch inertia: 87832.92910638226, ewa inertia: 87832.92910638226\n",
      "Minibatch step 56/100: mean batch inertia: 87854.29101949053, ewa inertia: 87854.29101949053\n",
      "Minibatch step 57/100: mean batch inertia: 88169.83935558509, ewa inertia: 88169.83935558509\n",
      "Minibatch step 58/100: mean batch inertia: 87447.2310625551, ewa inertia: 87447.2310625551\n",
      "Minibatch step 59/100: mean batch inertia: 86932.0350809673, ewa inertia: 86932.0350809673\n",
      "Minibatch step 60/100: mean batch inertia: 87762.85556944465, ewa inertia: 87762.85556944465\n",
      "Minibatch step 61/100: mean batch inertia: 87570.98854996798, ewa inertia: 87570.98854996798\n",
      "Minibatch step 62/100: mean batch inertia: 87797.4571891228, ewa inertia: 87797.4571891228\n",
      "Minibatch step 63/100: mean batch inertia: 88025.24260727115, ewa inertia: 88025.24260727115\n",
      "Minibatch step 64/100: mean batch inertia: 87931.14277197099, ewa inertia: 87931.14277197099\n",
      "Minibatch step 65/100: mean batch inertia: 88006.01853950311, ewa inertia: 88006.01853950311\n",
      "Minibatch step 66/100: mean batch inertia: 87495.89667022938, ewa inertia: 87495.89667022938\n",
      "Minibatch step 67/100: mean batch inertia: 87554.62025643977, ewa inertia: 87554.62025643977\n",
      "Minibatch step 68/100: mean batch inertia: 87760.48477232913, ewa inertia: 87760.48477232913\n",
      "Minibatch step 69/100: mean batch inertia: 87474.37920307534, ewa inertia: 87474.37920307534\n",
      "Converged (lack of improvement in inertia) at step 69/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 290026913.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 289403253.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 288397767.0\n",
      "Minibatch step 1/100: mean batch inertia: 139567.68884633796\n",
      "Minibatch step 2/100: mean batch inertia: 91458.42338827484, ewa inertia: 91458.42338827484\n",
      "Minibatch step 3/100: mean batch inertia: 89745.25204021663, ewa inertia: 89745.25204021663\n",
      "Minibatch step 4/100: mean batch inertia: 89513.92028581498, ewa inertia: 89513.92028581498\n",
      "Minibatch step 5/100: mean batch inertia: 89371.16473354516, ewa inertia: 89371.16473354516\n",
      "Minibatch step 6/100: mean batch inertia: 89655.70240116859, ewa inertia: 89655.70240116859\n",
      "Minibatch step 7/100: mean batch inertia: 88287.18299807311, ewa inertia: 88287.18299807311\n",
      "Minibatch step 8/100: mean batch inertia: 88421.81604494035, ewa inertia: 88421.81604494035\n",
      "Minibatch step 9/100: mean batch inertia: 88263.17581181342, ewa inertia: 88263.17581181342\n",
      "Minibatch step 10/100: mean batch inertia: 88198.25039672384, ewa inertia: 88198.25039672384\n",
      "Minibatch step 11/100: mean batch inertia: 88453.389646701, ewa inertia: 88453.389646701\n",
      "Minibatch step 12/100: mean batch inertia: 88095.95707227467, ewa inertia: 88095.95707227467\n",
      "Minibatch step 13/100: mean batch inertia: 87725.45383597304, ewa inertia: 87725.45383597304\n",
      "Minibatch step 14/100: mean batch inertia: 87645.17699100294, ewa inertia: 87645.17699100294\n",
      "Minibatch step 15/100: mean batch inertia: 87866.55389397684, ewa inertia: 87866.55389397684\n",
      "Minibatch step 16/100: mean batch inertia: 87562.42766396306, ewa inertia: 87562.42766396306\n",
      "Minibatch step 17/100: mean batch inertia: 87934.1811819937, ewa inertia: 87934.1811819937\n",
      "Minibatch step 18/100: mean batch inertia: 89223.63851274124, ewa inertia: 89223.63851274124\n",
      "Minibatch step 19/100: mean batch inertia: 87638.57869636775, ewa inertia: 87638.57869636775\n",
      "Minibatch step 20/100: mean batch inertia: 87334.99863396672, ewa inertia: 87334.99863396672\n",
      "Minibatch step 21/100: mean batch inertia: 88259.88908898213, ewa inertia: 88259.88908898213\n",
      "Minibatch step 22/100: mean batch inertia: 87696.32798169556, ewa inertia: 87696.32798169556\n",
      "Minibatch step 23/100: mean batch inertia: 87873.67787959955, ewa inertia: 87873.67787959955\n",
      "Minibatch step 24/100: mean batch inertia: 86922.39085679992, ewa inertia: 86922.39085679992\n",
      "Minibatch step 25/100: mean batch inertia: 87232.16115573229, ewa inertia: 87232.16115573229\n",
      "Minibatch step 26/100: mean batch inertia: 88015.05742829453, ewa inertia: 88015.05742829453\n",
      "Minibatch step 27/100: mean batch inertia: 87495.56309945181, ewa inertia: 87495.56309945181\n",
      "Minibatch step 28/100: mean batch inertia: 88059.21634212736, ewa inertia: 88059.21634212736\n",
      "Minibatch step 29/100: mean batch inertia: 86978.55891127312, ewa inertia: 86978.55891127312\n",
      "Minibatch step 30/100: mean batch inertia: 87310.97632338079, ewa inertia: 87310.97632338079\n",
      "Minibatch step 31/100: mean batch inertia: 88690.84345615379, ewa inertia: 88690.84345615379\n",
      "Minibatch step 32/100: mean batch inertia: 88053.35746283733, ewa inertia: 88053.35746283733\n",
      "Minibatch step 33/100: mean batch inertia: 87599.6726780287, ewa inertia: 87599.6726780287\n",
      "Minibatch step 34/100: mean batch inertia: 88909.83822350681, ewa inertia: 88909.83822350681\n",
      "Converged (lack of improvement in inertia) at step 34/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1043: UserWarning: MiniBatchKMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can prevent it by setting batch_size >= 3072 or by setting the environment variable OMP_NUM_THREADS=9\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method k-means++\n",
      "Inertia for init 1/3: 291209842.0\n",
      "Init 2/3 with method k-means++\n",
      "Inertia for init 2/3: 290578165.0\n",
      "Init 3/3 with method k-means++\n",
      "Inertia for init 3/3: 286756676.0\n",
      "Minibatch step 1/100: mean batch inertia: 136860.92101483964\n",
      "Minibatch step 2/100: mean batch inertia: 90905.13090136985, ewa inertia: 90905.13090136985\n",
      "Minibatch step 3/100: mean batch inertia: 90207.23600475634, ewa inertia: 90207.23600475634\n",
      "Minibatch step 4/100: mean batch inertia: 88983.0979907, ewa inertia: 88983.0979907\n",
      "Minibatch step 5/100: mean batch inertia: 88463.5206334856, ewa inertia: 88463.5206334856\n",
      "Minibatch step 6/100: mean batch inertia: 88240.42573966074, ewa inertia: 88240.42573966074\n",
      "Minibatch step 7/100: mean batch inertia: 88736.16191746548, ewa inertia: 88736.16191746548\n",
      "Minibatch step 8/100: mean batch inertia: 88078.33504280835, ewa inertia: 88078.33504280835\n",
      "Minibatch step 9/100: mean batch inertia: 88120.18336865648, ewa inertia: 88120.18336865648\n",
      "Minibatch step 10/100: mean batch inertia: 88269.720867798, ewa inertia: 88269.720867798\n",
      "Minibatch step 11/100: mean batch inertia: 87798.61248516345, ewa inertia: 87798.61248516345\n",
      "Minibatch step 12/100: mean batch inertia: 88368.95331736423, ewa inertia: 88368.95331736423\n",
      "Minibatch step 13/100: mean batch inertia: 88349.7635242349, ewa inertia: 88349.7635242349\n",
      "Minibatch step 14/100: mean batch inertia: 88470.78899199929, ewa inertia: 88470.78899199929\n",
      "Minibatch step 15/100: mean batch inertia: 87737.06214123635, ewa inertia: 87737.06214123635\n",
      "Minibatch step 16/100: mean batch inertia: 87626.48912329513, ewa inertia: 87626.48912329513\n",
      "Minibatch step 17/100: mean batch inertia: 87648.90318781922, ewa inertia: 87648.90318781922\n",
      "Minibatch step 18/100: mean batch inertia: 87953.4454756537, ewa inertia: 87953.4454756537\n",
      "Minibatch step 19/100: mean batch inertia: 88222.85765833537, ewa inertia: 88222.85765833537\n",
      "Minibatch step 20/100: mean batch inertia: 87918.45629573792, ewa inertia: 87918.45629573792\n",
      "Minibatch step 21/100: mean batch inertia: 87010.28676186924, ewa inertia: 87010.28676186924\n",
      "Minibatch step 22/100: mean batch inertia: 86606.02300955789, ewa inertia: 86606.02300955789\n",
      "Minibatch step 23/100: mean batch inertia: 87569.76791801745, ewa inertia: 87569.76791801745\n",
      "Minibatch step 24/100: mean batch inertia: 88531.47153970595, ewa inertia: 88531.47153970595\n",
      "Minibatch step 25/100: mean batch inertia: 88085.50672542708, ewa inertia: 88085.50672542708\n",
      "Minibatch step 26/100: mean batch inertia: 87103.5606351556, ewa inertia: 87103.5606351556\n",
      "Minibatch step 27/100: mean batch inertia: 88512.31425168397, ewa inertia: 88512.31425168397\n",
      "Minibatch step 28/100: mean batch inertia: 87436.01983019663, ewa inertia: 87436.01983019663\n",
      "Minibatch step 29/100: mean batch inertia: 88149.43628783504, ewa inertia: 88149.43628783504\n",
      "Minibatch step 30/100: mean batch inertia: 87271.897566396, ewa inertia: 87271.897566396\n",
      "Minibatch step 31/100: mean batch inertia: 87220.50628384642, ewa inertia: 87220.50628384642\n",
      "Minibatch step 32/100: mean batch inertia: 87629.68300962774, ewa inertia: 87629.68300962774\n",
      "Converged (lack of improvement in inertia) at step 32/100\n"
     ]
    }
   ],
   "source": [
    "sample = os.listdir(\"data/images/3 De nayer (Loop piste)/partition 1\")\n",
    "sample = data.loc[data[\"image\"].isin(sample)].head(50)\n",
    "\n",
    "#extract_features_from_image(\"3 De nayer (Loop piste)\", \"partition 1\",sample[\"image\"].head(1).iloc[0])\n",
    "sample_feats = [extract_features_from_image(\"3 De nayer (Loop piste)\", \"partition 1\", img) for img in sample[\"image\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "5e622d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_feats_df = pd.DataFrame(sample_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f907936d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.06378738, 0.04186047, 0.03654485, 0.03189369, 0.03853821,\n",
       "        0.01960133, 0.0372093 , 0.04418605, 0.0282392 , 0.03920266,\n",
       "        0.01860465, 0.03853821, 0.02425249, 0.03289037, 0.02757475,\n",
       "        0.03222591, 0.02591362, 0.04186047, 0.03189369, 0.02491694,\n",
       "        0.02093023, 0.03122924, 0.03654485, 0.01594684, 0.04019934,\n",
       "        0.02259136, 0.05282392, 0.03421927, 0.02425249, 0.04152824]),\n",
       " array([0.04551724, 0.03517241, 0.02275862, 0.04448276, 0.04758621,\n",
       "        0.02551724, 0.02793103, 0.06689655, 0.01689655, 0.03206897,\n",
       "        0.0337931 , 0.02551724, 0.03206897, 0.02275862, 0.04551724,\n",
       "        0.04310345, 0.02689655, 0.02517241, 0.01896552, 0.02206897,\n",
       "        0.03758621, 0.02655172, 0.04241379, 0.03413793, 0.03137931,\n",
       "        0.03965517, 0.02862069, 0.02655172, 0.04172414, 0.03068966]),\n",
       " array([0.01561416, 0.02741152, 0.04510756, 0.02151284, 0.05898681,\n",
       "        0.01873699, 0.04024983, 0.03990285, 0.04510756, 0.02637058,\n",
       "        0.03469813, 0.04614851, 0.04649549, 0.0222068 , 0.02359473,\n",
       "        0.03851492, 0.03608605, 0.03816794, 0.03018737, 0.03539209,\n",
       "        0.02394171, 0.02185982, 0.03573907, 0.02671756, 0.02914643,\n",
       "        0.06072172, 0.02567661, 0.02498265, 0.01526718, 0.04545455]),\n",
       " array([0.03060184, 0.02176131, 0.01768106, 0.03298198, 0.03094186,\n",
       "        0.02822169, 0.03468208, 0.02686161, 0.06698402, 0.04420265,\n",
       "        0.0350221 , 0.03536212, 0.02074124, 0.04046243, 0.02788167,\n",
       "        0.03978239, 0.03400204, 0.02584155, 0.0316219 , 0.02958177,\n",
       "        0.02652159, 0.03842231, 0.04692282, 0.03128188, 0.03196192,\n",
       "        0.03468208, 0.01768106, 0.02414145, 0.06800408, 0.02516151]),\n",
       " array([0.04519393, 0.02428331, 0.02394604, 0.04822934, 0.0306914 ,\n",
       "        0.0317032 , 0.03575042, 0.03035413, 0.02698145, 0.02799325,\n",
       "        0.02091062, 0.03406408, 0.03204047, 0.02563238, 0.0391231 ,\n",
       "        0.02833052, 0.03473862, 0.05092749, 0.03338954, 0.0222597 ,\n",
       "        0.03338954, 0.03507589, 0.0222597 , 0.02900506, 0.03305228,\n",
       "        0.06745363, 0.02731872, 0.04890388, 0.02900506, 0.02799325]),\n",
       " array([0.03075328, 0.05252246, 0.02246026, 0.03489979, 0.04422944,\n",
       "        0.02246026, 0.02418798, 0.02626123, 0.03178991, 0.03109883,\n",
       "        0.0438839 , 0.01278507, 0.03731859, 0.03351762, 0.02315135,\n",
       "        0.03317208, 0.03662751, 0.05321355, 0.032481  , 0.02660677,\n",
       "        0.04422944, 0.03973739, 0.03697305, 0.03040774, 0.0276434 ,\n",
       "        0.04353836, 0.02729786, 0.02902557, 0.02142364, 0.0463027 ]),\n",
       " array([0.02928158, 0.04051753, 0.03132448, 0.0289411 , 0.04051753,\n",
       "        0.03234593, 0.05992509, 0.02485529, 0.02553626, 0.04426285,\n",
       "        0.02928158, 0.05856316, 0.02281239, 0.03200545, 0.04221995,\n",
       "        0.01906708, 0.02383384, 0.02826013, 0.02928158, 0.03064351,\n",
       "        0.04868914, 0.02315288, 0.02383384, 0.0330269 , 0.03541028,\n",
       "        0.02621723, 0.04153899, 0.02451481, 0.04358189, 0.02655771]),\n",
       " array([0.03542331, 0.03117251, 0.02196245, 0.02975558, 0.02408785,\n",
       "        0.02444208, 0.04038257, 0.06659582, 0.03188098, 0.03506908,\n",
       "        0.02444208, 0.02656748, 0.02231668, 0.04002834, 0.03294368,\n",
       "        0.0428622 , 0.02054552, 0.03861141, 0.03294368, 0.04357067,\n",
       "        0.01983705, 0.0460503 , 0.08466171, 0.02585902, 0.03081828,\n",
       "        0.02585902, 0.02479632, 0.02373362, 0.02763018, 0.02515055]),\n",
       " array([0.02330508, 0.07980226, 0.02648305, 0.02153955, 0.02754237,\n",
       "        0.03248588, 0.03389831, 0.0240113 , 0.04696328, 0.03072034,\n",
       "        0.02048023, 0.01271186, 0.04060734, 0.02789548, 0.03177966,\n",
       "        0.04237288, 0.04025424, 0.03884181, 0.0434322 , 0.03248588,\n",
       "        0.02436441, 0.04519774, 0.04484463, 0.03319209, 0.02577684,\n",
       "        0.03072034, 0.02966102, 0.04413842, 0.02224576, 0.02224576]),\n",
       " array([0.02532097, 0.07952924, 0.02817404, 0.05313837, 0.0310271 ,\n",
       "        0.02425107, 0.0374465 , 0.04279601, 0.0331669 , 0.0246077 ,\n",
       "        0.02746077, 0.0160485 , 0.02532097, 0.03459344, 0.03245364,\n",
       "        0.03887304, 0.02603424, 0.03673324, 0.03994294, 0.03388017,\n",
       "        0.0235378 , 0.01676177, 0.02425107, 0.04208274, 0.03958631,\n",
       "        0.02853067, 0.03673324, 0.02282454, 0.04136947, 0.03352354]),\n",
       " array([0.02868994, 0.03871414, 0.0290356 , 0.05046664, 0.03594884,\n",
       "        0.02696163, 0.03249222, 0.02730729, 0.0172831 , 0.03733149,\n",
       "        0.03422053, 0.03422053, 0.03145524, 0.03456619, 0.02627031,\n",
       "        0.02246803, 0.04424473, 0.03214656, 0.06221915, 0.01970273,\n",
       "        0.03283788, 0.01659177, 0.03249222, 0.02385067, 0.02938127,\n",
       "        0.04493605, 0.04078811, 0.04251642, 0.04666436, 0.02419634]),\n",
       " array([0.03196192, 0.04114247, 0.03944237, 0.02584155, 0.03060184,\n",
       "        0.02414145, 0.03774226, 0.03774226, 0.03230194, 0.02924175,\n",
       "        0.03706222, 0.03230194, 0.03604216, 0.04046243, 0.0350221 ,\n",
       "        0.02176131, 0.03434206, 0.02822169, 0.04114247, 0.03060184,\n",
       "        0.03196192, 0.03468208, 0.01768106, 0.04046243, 0.02482149,\n",
       "        0.02890173, 0.03264196, 0.0183611 , 0.06154369, 0.04182251]),\n",
       " array([0.01651755, 0.0615967 , 0.02890571, 0.0395733 , 0.03922918,\n",
       "        0.04301445, 0.03097041, 0.04198211, 0.01927047, 0.03613214,\n",
       "        0.03819683, 0.07536132, 0.0330351 , 0.02236752, 0.02890571,\n",
       "        0.02443221, 0.02305575, 0.02271163, 0.03509979, 0.02133517,\n",
       "        0.03131452, 0.01961459, 0.01514109, 0.01686167, 0.03441156,\n",
       "        0.04955265, 0.03785272, 0.03785272, 0.03785272, 0.03785272]),\n",
       " array([0.02547771, 0.04257459, 0.0355347 , 0.02011398, 0.03084143,\n",
       "        0.04659739, 0.02648341, 0.01340932, 0.04324506, 0.05564868,\n",
       "        0.03821656, 0.03989273, 0.02447201, 0.02111968, 0.02883004,\n",
       "        0.04290982, 0.02346631, 0.03084143, 0.03721086, 0.034529  ,\n",
       "        0.02380154, 0.04492122, 0.02246061, 0.02648341, 0.04559169,\n",
       "        0.03888703, 0.04089842, 0.02346631, 0.05430774, 0.01776735]),\n",
       " array([0.0259481 , 0.0322688 , 0.0259481 , 0.03626081, 0.03193613,\n",
       "        0.03293413, 0.04424484, 0.0332668 , 0.02428476, 0.02129075,\n",
       "        0.04224884, 0.0322688 , 0.02461743, 0.02628077, 0.02062542,\n",
       "        0.0259481 , 0.04357951, 0.05688623, 0.03958749, 0.0405855 ,\n",
       "        0.03459747, 0.0499002 , 0.02528277, 0.02162342, 0.03459747,\n",
       "        0.03060546, 0.03393214, 0.0508982 , 0.02794411, 0.02960745]),\n",
       " array([0.00834783, 0.05182609, 0.03791304, 0.04278261, 0.02991304,\n",
       "        0.03165217, 0.01843478, 0.03826087, 0.02643478, 0.03478261,\n",
       "        0.03756522, 0.06156522, 0.02921739, 0.01773913, 0.04173913,\n",
       "        0.01843478, 0.04313043, 0.04765217, 0.02956522, 0.0306087 ,\n",
       "        0.04208696, 0.04173913, 0.02956522, 0.02713043, 0.02817391,\n",
       "        0.02504348, 0.03269565, 0.0253913 , 0.03965217, 0.03095652]),\n",
       " array([0.0157532 , 0.03183459, 0.04069577, 0.01903512, 0.03117821,\n",
       "        0.02067607, 0.01772235, 0.02920906, 0.07253036, 0.03839842,\n",
       "        0.03610108, 0.03183459, 0.01805054, 0.02198884, 0.06924844,\n",
       "        0.03905481, 0.03610108, 0.02001969, 0.03216278, 0.03183459,\n",
       "        0.04233672, 0.03577289, 0.05021332, 0.02034788, 0.03610108,\n",
       "        0.01837873, 0.02625533, 0.05283886, 0.0433213 , 0.02100427]),\n",
       " array([0.04002714, 0.04715061, 0.02476255, 0.03561737, 0.02442334,\n",
       "        0.0546133 , 0.02985075, 0.02442334, 0.02815468, 0.01797829,\n",
       "        0.02238806, 0.04477612, 0.03222524, 0.03426052, 0.02679783,\n",
       "        0.01628223, 0.02815468, 0.02645862, 0.04647218, 0.02747626,\n",
       "        0.02272727, 0.03256445, 0.04884668, 0.03086839, 0.02238806,\n",
       "        0.03561737, 0.04613297, 0.05088195, 0.04545455, 0.03222524]),\n",
       " array([0.04522443, 0.03071212, 0.02936213, 0.05467432, 0.02564968,\n",
       "        0.01721228, 0.01957476, 0.03138711, 0.03408707, 0.03813702,\n",
       "        0.02328721, 0.02328721, 0.01687479, 0.02969963, 0.05501181,\n",
       "        0.02936213, 0.03611205, 0.02733716, 0.05467432, 0.02159973,\n",
       "        0.06851164, 0.04724941, 0.02193723, 0.03003712, 0.03712454,\n",
       "        0.03138711, 0.02902464, 0.03307459, 0.03644954, 0.02193723]),\n",
       " array([0.02068088, 0.05249761, 0.02704423, 0.04136176, 0.0614063 ,\n",
       "        0.0165447 , 0.02513522, 0.03340757, 0.02386255, 0.03627108,\n",
       "        0.04008909, 0.02545339, 0.03149857, 0.03818008, 0.0302259 ,\n",
       "        0.0311804 , 0.02799873, 0.02799873, 0.03181674, 0.05217945,\n",
       "        0.02290805, 0.0467706 , 0.03595291, 0.01940821, 0.0311804 ,\n",
       "        0.03690741, 0.03977092, 0.0439071 , 0.02354438, 0.02481705]),\n",
       " array([0.02636783, 0.03328939, 0.05240606, 0.03032301, 0.02999341,\n",
       "        0.03263019, 0.04449572, 0.03427818, 0.03361898, 0.02603823,\n",
       "        0.02504944, 0.02966381, 0.05174687, 0.03460778, 0.04054054,\n",
       "        0.02340145, 0.05800923, 0.02504944, 0.04119974, 0.01450231,\n",
       "        0.031971  , 0.02636783, 0.02406065, 0.03394858, 0.03724456,\n",
       "        0.03460778, 0.02340145, 0.02867502, 0.04087014, 0.0316414 ]),\n",
       " array([0.02269648, 0.03726287, 0.03895664, 0.02405149, 0.03252033,\n",
       "        0.03252033, 0.04336043, 0.0298103 , 0.02811653, 0.03014905,\n",
       "        0.02879404, 0.04945799, 0.02371274, 0.03794038, 0.02879404,\n",
       "        0.03929539, 0.03421409, 0.03794038, 0.03252033, 0.03556911,\n",
       "        0.02337398, 0.03658537, 0.02235772, 0.05555556, 0.02574526,\n",
       "        0.04844173, 0.0304878 , 0.0298103 , 0.01897019, 0.04098916]),\n",
       " array([0.02287798, 0.03547745, 0.04244032, 0.02685676, 0.02586207,\n",
       "        0.04442971, 0.03017241, 0.05404509, 0.02785146, 0.0454244 ,\n",
       "        0.03614058, 0.03680371, 0.05039788, 0.02718833, 0.0321618 ,\n",
       "        0.04011936, 0.01624668, 0.02917772, 0.03680371, 0.03514589,\n",
       "        0.02453581, 0.03812997, 0.03149867, 0.01657825, 0.02884615,\n",
       "        0.04409814, 0.03846154, 0.0321618 , 0.02519894, 0.02486737]),\n",
       " array([0.02729292, 0.03459219, 0.07331006, 0.03681371, 0.03713107,\n",
       "        0.01967629, 0.02443669, 0.028245  , 0.03776579, 0.0314186 ,\n",
       "        0.04728658, 0.03713107, 0.01840685, 0.02253253, 0.03871787,\n",
       "        0.03966995, 0.03649635, 0.03459219, 0.03586163, 0.03046652,\n",
       "        0.02348461, 0.03173596, 0.01586798, 0.04569978, 0.03966995,\n",
       "        0.04062202, 0.02411933, 0.03966995, 0.02158045, 0.02570613]),\n",
       " array([0.02583423, 0.03731611, 0.01435235, 0.04269824, 0.04413348,\n",
       "        0.04233943, 0.03301041, 0.0326516 , 0.03229279, 0.03229279,\n",
       "        0.05166846, 0.04269824, 0.0491568 , 0.03552207, 0.02081091,\n",
       "        0.02762827, 0.02511661, 0.02906351, 0.03229279, 0.01686401,\n",
       "        0.02116972, 0.02152853, 0.04305705, 0.04951561, 0.02511661,\n",
       "        0.03588088, 0.04054539, 0.03803373, 0.02009329, 0.03731611]),\n",
       " array([0.02156522, 0.03478261, 0.04486957, 0.048     , 0.01808696,\n",
       "        0.032     , 0.0306087 , 0.03930435, 0.0253913 , 0.04173913,\n",
       "        0.0333913 , 0.02330435, 0.02713043, 0.03791304, 0.02678261,\n",
       "        0.032     , 0.04243478, 0.02330435, 0.03443478, 0.02191304,\n",
       "        0.02330435, 0.03443478, 0.03478261, 0.06643478, 0.02608696,\n",
       "        0.0333913 , 0.04208696, 0.01843478, 0.05565217, 0.02643478]),\n",
       " array([0.04513541, 0.02975593, 0.00902708, 0.04948178, 0.0404547 ,\n",
       "        0.02841859, 0.03510532, 0.01738549, 0.04112337, 0.02440655,\n",
       "        0.03711133, 0.02340354, 0.03543965, 0.04179204, 0.02306921,\n",
       "        0.04580408, 0.04847877, 0.02641257, 0.04446673, 0.02240053,\n",
       "        0.02540956, 0.02440655, 0.05048479, 0.02306921, 0.02708124,\n",
       "        0.03945169, 0.02975593, 0.02474089, 0.04613842, 0.04078903]),\n",
       " array([0.01325178, 0.02990146, 0.02582399, 0.04553177, 0.03363914,\n",
       "        0.03975535, 0.04858987, 0.03771662, 0.02208631, 0.02718315,\n",
       "        0.0319402 , 0.02344546, 0.03805641, 0.03771662, 0.02650357,\n",
       "        0.03771662, 0.02310567, 0.02786273, 0.04960924, 0.03329935,\n",
       "        0.04994903, 0.03669725, 0.02990146, 0.06252124, 0.01800883,\n",
       "        0.03058104, 0.02616378, 0.02786273, 0.03126062, 0.03431872]),\n",
       " array([0.0383693 , 0.03905447, 0.02363823, 0.03254539, 0.03117506,\n",
       "        0.06097979, 0.03048989, 0.04316547, 0.03288798, 0.04898938,\n",
       "        0.03768414, 0.04727646, 0.02158273, 0.0383693 , 0.05584104,\n",
       "        0.04076739, 0.02774923, 0.02774923, 0.02124015, 0.02363823,\n",
       "        0.0284344 , 0.02706406, 0.03802672, 0.01884207, 0.02295307,\n",
       "        0.04145255, 0.03597122, 0.01678657, 0.02192532, 0.02535115]),\n",
       " array([0.02185046, 0.05018778, 0.03175145, 0.04609082, 0.0303858 ,\n",
       "        0.02082622, 0.01809491, 0.02697166, 0.03721407, 0.03892113,\n",
       "        0.02560601, 0.05735746, 0.02048481, 0.03379993, 0.02833732,\n",
       "        0.03823831, 0.02048481, 0.03277569, 0.0532605 , 0.035507  ,\n",
       "        0.02902014, 0.04506658, 0.03516559, 0.03584841, 0.03175145,\n",
       "        0.03072721, 0.02560601, 0.04165244, 0.02867873, 0.02833732]),\n",
       " array([0.02807487, 0.03977273, 0.05213904, 0.04445187, 0.05381016,\n",
       "        0.02840909, 0.03676471, 0.03041444, 0.0407754 , 0.03542781,\n",
       "        0.02941176, 0.03475936, 0.04010695, 0.03175134, 0.05013369,\n",
       "        0.02506684, 0.03676471, 0.03676471, 0.0243984 , 0.02005348,\n",
       "        0.03275401, 0.03108289, 0.03241979, 0.04177807, 0.03241979,\n",
       "        0.01504011, 0.02640374, 0.02272727, 0.01704545, 0.02907754]),\n",
       " array([0.0421123 , 0.02239305, 0.02774064, 0.03409091, 0.02774064,\n",
       "        0.02072193, 0.03308824, 0.04779412, 0.05180481, 0.07118984,\n",
       "        0.03576203, 0.04545455, 0.03442513, 0.03442513, 0.02673797,\n",
       "        0.02840909, 0.01671123, 0.02874332, 0.0230615 , 0.03342246,\n",
       "        0.02640374, 0.02941176, 0.03108289, 0.02606952, 0.03576203,\n",
       "        0.0434492 , 0.05080214, 0.02540107, 0.0177139 , 0.02807487]),\n",
       " array([0.02790065, 0.04083021, 0.04185097, 0.02790065, 0.01939435,\n",
       "        0.03674719, 0.03334468, 0.02551888, 0.03300442, 0.02687989,\n",
       "        0.02756039, 0.03300442, 0.05444029, 0.03062266, 0.04355223,\n",
       "        0.0670296 , 0.02517863, 0.02994216, 0.02994216, 0.02517863,\n",
       "        0.02347737, 0.03980946, 0.02483838, 0.04491324, 0.02790065,\n",
       "        0.02517863, 0.01769309, 0.02415788, 0.04627424, 0.04593399]),\n",
       " array([0.02979516, 0.01893234, 0.02172564, 0.03755431, 0.03103662,\n",
       "        0.06734947, 0.02265673, 0.02793296, 0.03693358, 0.02358783,\n",
       "        0.03351955, 0.03134699, 0.044072  , 0.05214153, 0.02358783,\n",
       "        0.04655493, 0.0257604 , 0.03010552, 0.03382992, 0.02731223,\n",
       "        0.03010552, 0.04127871, 0.02793296, 0.02420857, 0.05090006,\n",
       "        0.03351955, 0.02793296, 0.03414029, 0.022036  , 0.04220981]),\n",
       " array([0.03049968, 0.02725503, 0.03471772, 0.02303699, 0.03244646,\n",
       "        0.04445165, 0.02141467, 0.0421804 , 0.03374432, 0.05418559,\n",
       "        0.04834523, 0.04055808, 0.03374432, 0.0266061 , 0.0266061 ,\n",
       "        0.03698897, 0.03471772, 0.03926022, 0.03439325, 0.02757949,\n",
       "        0.03049968, 0.05223881, 0.02757949, 0.03698897, 0.03439325,\n",
       "        0.01687216, 0.02595717, 0.02790396, 0.02887735, 0.02595717]),\n",
       " array([0.02310345, 0.02344828, 0.04965517, 0.04068966, 0.03275862,\n",
       "        0.02586207, 0.03655172, 0.03172414, 0.03275862, 0.02655172,\n",
       "        0.03034483, 0.03448276, 0.01862069, 0.03586207, 0.05103448,\n",
       "        0.03655172, 0.02310345, 0.04206897, 0.04206897, 0.01551724,\n",
       "        0.03482759, 0.04068966, 0.03827586, 0.02310345, 0.02965517,\n",
       "        0.02827586, 0.03896552, 0.02310345, 0.05482759, 0.03551724]),\n",
       " array([0.02920677, 0.02920677, 0.03252572, 0.0341852 , 0.02987056,\n",
       "        0.02157318, 0.02456024, 0.0272154 , 0.03584467, 0.03119814,\n",
       "        0.03849983, 0.0338533 , 0.03053435, 0.05874544, 0.05044806,\n",
       "        0.0341852 , 0.05476269, 0.01958181, 0.0477929 , 0.02655161,\n",
       "        0.03816794, 0.03219383, 0.01891802, 0.03252572, 0.03252572,\n",
       "        0.03219383, 0.02522403, 0.04480584, 0.02987056, 0.02323266]),\n",
       " array([0.05110497, 0.01899171, 0.0265884 , 0.03867403, 0.06008287,\n",
       "        0.02520718, 0.0269337 , 0.03038674, 0.01553867, 0.02555249,\n",
       "        0.03418508, 0.03004144, 0.03487569, 0.03660221, 0.04558011,\n",
       "        0.02555249, 0.03314917, 0.03798343, 0.02727901, 0.04834254,\n",
       "        0.03107735, 0.02313536, 0.02796961, 0.02762431, 0.06319061,\n",
       "        0.04040055, 0.03004144, 0.02486188, 0.03107735, 0.02796961]),\n",
       " array([0.02772074, 0.02669405, 0.02361396, 0.04140999, 0.04551677,\n",
       "        0.05749487, 0.02327173, 0.04414784, 0.02874743, 0.03011636,\n",
       "        0.0513347 , 0.02703628, 0.02121834, 0.02874743, 0.04483231,\n",
       "        0.02053388, 0.03114305, 0.03319644, 0.05578371, 0.02874743,\n",
       "        0.04688569, 0.02429843, 0.03216975, 0.02121834, 0.03524983,\n",
       "        0.02977413, 0.02977413, 0.03456537, 0.02772074, 0.02703628]),\n",
       " array([0.05939926, 0.0239622 , 0.02497469, 0.0158623 , 0.03577455,\n",
       "        0.04387445, 0.02294971, 0.05366183, 0.0236247 , 0.04083699,\n",
       "        0.04556193, 0.06749916, 0.02902464, 0.03408707, 0.01721228,\n",
       "        0.04117449, 0.02227472, 0.02497469, 0.02699966, 0.02497469,\n",
       "        0.02801215, 0.02261222, 0.02902464, 0.03341208, 0.02902464,\n",
       "        0.04589943, 0.02531218, 0.03037462, 0.05231185, 0.02531218]),\n",
       " array([0.01826636, 0.02955829, 0.05380272, 0.03022252, 0.05014945,\n",
       "        0.03885752, 0.03786118, 0.03022252, 0.03321156, 0.04118233,\n",
       "        0.03254733, 0.01295251, 0.03088675, 0.02092328, 0.02424444,\n",
       "        0.05048157, 0.04284291, 0.01195616, 0.0405181 , 0.02358021,\n",
       "        0.03088675, 0.03454002, 0.0318831 , 0.01195616, 0.03287944,\n",
       "        0.0405181 , 0.04815676, 0.0362006 , 0.04549983, 0.03321156]),\n",
       " array([0.04948178, 0.03510532, 0.01671682, 0.03075894, 0.03778001,\n",
       "        0.03844868, 0.04078903, 0.03142762, 0.03376797, 0.01671682,\n",
       "        0.04714142, 0.04446673, 0.04747576, 0.03343363, 0.03075894,\n",
       "        0.03109328, 0.03610832, 0.0441324 , 0.02340354, 0.02540956,\n",
       "        0.03644266, 0.03610832, 0.01571381, 0.02607823, 0.02139753,\n",
       "        0.05483116, 0.04145771, 0.02407222, 0.02173186, 0.02774992]),\n",
       " array([0.02551888, 0.04014971, 0.0387887 , 0.04967676, 0.04014971,\n",
       "        0.02960191, 0.02041511, 0.02143586, 0.03028241, 0.02109561,\n",
       "        0.02926165, 0.05273903, 0.0387887 , 0.02858115, 0.05852331,\n",
       "        0.0289214 , 0.01701259, 0.03062266, 0.04661449, 0.05069752,\n",
       "        0.02687989, 0.02075536, 0.02211637, 0.03198367, 0.02347737,\n",
       "        0.02007486, 0.03504593, 0.0564818 , 0.02926165, 0.03504593]),\n",
       " array([0.04342354, 0.03272498, 0.0129012 , 0.03241032, 0.02013845,\n",
       "        0.02517306, 0.03272498, 0.03555695, 0.04782882, 0.0402769 ,\n",
       "        0.05695406, 0.03178099, 0.05097546, 0.03618628, 0.02171177,\n",
       "        0.02297042, 0.02863436, 0.03492763, 0.0188798 , 0.02548773,\n",
       "        0.0283197 , 0.02769037, 0.02611705, 0.030837  , 0.03461296,\n",
       "        0.03933291, 0.04122089, 0.04279421, 0.03807426, 0.03933291]),\n",
       " array([0.0425678 , 0.02780639, 0.02986612, 0.01888088, 0.01991074,\n",
       "        0.04188122, 0.048747  , 0.01579128, 0.02540336, 0.01956746,\n",
       "        0.02368692, 0.04016478, 0.02849296, 0.02506008, 0.06316512,\n",
       "        0.03295572, 0.03364229, 0.0456574 , 0.0394782 , 0.0425678 ,\n",
       "        0.02265705, 0.04359767, 0.01888088, 0.03364229, 0.04909028,\n",
       "        0.04119464, 0.04634398, 0.02025403, 0.02471679, 0.03432887]),\n",
       " array([0.01795213, 0.03756649, 0.03390957, 0.0362367 , 0.0206117 ,\n",
       "        0.03224734, 0.02327128, 0.03224734, 0.02493351, 0.04388298,\n",
       "        0.03324468, 0.03856383, 0.03856383, 0.03490691, 0.03191489,\n",
       "        0.04920213, 0.03291223, 0.05152926, 0.02593085, 0.03789894,\n",
       "        0.03058511, 0.0262633 , 0.01961436, 0.03324468, 0.04820479,\n",
       "        0.02194149, 0.04022606, 0.0255984 , 0.04621011, 0.03058511]),\n",
       " array([0.0306662 , 0.04970039, 0.03313359, 0.03630596, 0.0352485 ,\n",
       "        0.03278111, 0.04793796, 0.02855129, 0.04723299, 0.0211491 ,\n",
       "        0.03701093, 0.02714135, 0.04124075, 0.02044413, 0.02925626,\n",
       "        0.04159323, 0.02855129, 0.03031371, 0.03278111, 0.03560099,\n",
       "        0.04300317, 0.01727177, 0.0447656 , 0.03912584, 0.03242862,\n",
       "        0.02855129, 0.03630596, 0.02678886, 0.01268946, 0.03242862]),\n",
       " array([0.02868852, 0.03483607, 0.05806011, 0.02493169, 0.0522541 ,\n",
       "        0.02459016, 0.0454235 , 0.03107923, 0.02117486, 0.04439891,\n",
       "        0.02834699, 0.02117486, 0.03551913, 0.03415301, 0.04303279,\n",
       "        0.0273224 , 0.02459016, 0.0273224 , 0.02629781, 0.04405738,\n",
       "        0.04030055, 0.02595628, 0.05293716, 0.01844262, 0.01434426,\n",
       "        0.02015027, 0.04303279, 0.02459016, 0.04337432, 0.03961749]),\n",
       " array([0.02961966, 0.02490744, 0.02659037, 0.0491417 , 0.0232245 ,\n",
       "        0.03433187, 0.04981488, 0.03971727, 0.04611242, 0.04779536,\n",
       "        0.04274655, 0.02726355, 0.02221474, 0.05856614, 0.02187816,\n",
       "        0.03298553, 0.03062942, 0.03332211, 0.01649276, 0.04274655,\n",
       "        0.030966  , 0.02692696, 0.02692696, 0.04476607, 0.01851229,\n",
       "        0.03433187, 0.03433187, 0.01985863, 0.02457085, 0.03870751]),\n",
       " array([0.02235569, 0.01768435, 0.0343677 , 0.02202202, 0.03203203,\n",
       "        0.03470137, 0.03169837, 0.02402402, 0.05238572, 0.02869536,\n",
       "        0.02235569, 0.02002002, 0.05905906, 0.03203203, 0.02435769,\n",
       "        0.04704705, 0.02569236, 0.0373707 , 0.05405405, 0.03803804,\n",
       "        0.03970637, 0.01901902, 0.04671338, 0.04137471, 0.02268936,\n",
       "        0.0323657 , 0.02035369, 0.01835169, 0.04938272, 0.05005005])]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "71b82fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.063787</td>\n",
       "      <td>0.041860</td>\n",
       "      <td>0.036545</td>\n",
       "      <td>0.031894</td>\n",
       "      <td>0.038538</td>\n",
       "      <td>0.019601</td>\n",
       "      <td>0.037209</td>\n",
       "      <td>0.044186</td>\n",
       "      <td>0.028239</td>\n",
       "      <td>0.039203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020930</td>\n",
       "      <td>0.031229</td>\n",
       "      <td>0.036545</td>\n",
       "      <td>0.015947</td>\n",
       "      <td>0.040199</td>\n",
       "      <td>0.022591</td>\n",
       "      <td>0.052824</td>\n",
       "      <td>0.034219</td>\n",
       "      <td>0.024252</td>\n",
       "      <td>0.041528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.045517</td>\n",
       "      <td>0.035172</td>\n",
       "      <td>0.022759</td>\n",
       "      <td>0.044483</td>\n",
       "      <td>0.047586</td>\n",
       "      <td>0.025517</td>\n",
       "      <td>0.027931</td>\n",
       "      <td>0.066897</td>\n",
       "      <td>0.016897</td>\n",
       "      <td>0.032069</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037586</td>\n",
       "      <td>0.026552</td>\n",
       "      <td>0.042414</td>\n",
       "      <td>0.034138</td>\n",
       "      <td>0.031379</td>\n",
       "      <td>0.039655</td>\n",
       "      <td>0.028621</td>\n",
       "      <td>0.026552</td>\n",
       "      <td>0.041724</td>\n",
       "      <td>0.030690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.015614</td>\n",
       "      <td>0.027412</td>\n",
       "      <td>0.045108</td>\n",
       "      <td>0.021513</td>\n",
       "      <td>0.058987</td>\n",
       "      <td>0.018737</td>\n",
       "      <td>0.040250</td>\n",
       "      <td>0.039903</td>\n",
       "      <td>0.045108</td>\n",
       "      <td>0.026371</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023942</td>\n",
       "      <td>0.021860</td>\n",
       "      <td>0.035739</td>\n",
       "      <td>0.026718</td>\n",
       "      <td>0.029146</td>\n",
       "      <td>0.060722</td>\n",
       "      <td>0.025677</td>\n",
       "      <td>0.024983</td>\n",
       "      <td>0.015267</td>\n",
       "      <td>0.045455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.030602</td>\n",
       "      <td>0.021761</td>\n",
       "      <td>0.017681</td>\n",
       "      <td>0.032982</td>\n",
       "      <td>0.030942</td>\n",
       "      <td>0.028222</td>\n",
       "      <td>0.034682</td>\n",
       "      <td>0.026862</td>\n",
       "      <td>0.066984</td>\n",
       "      <td>0.044203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026522</td>\n",
       "      <td>0.038422</td>\n",
       "      <td>0.046923</td>\n",
       "      <td>0.031282</td>\n",
       "      <td>0.031962</td>\n",
       "      <td>0.034682</td>\n",
       "      <td>0.017681</td>\n",
       "      <td>0.024141</td>\n",
       "      <td>0.068004</td>\n",
       "      <td>0.025162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.045194</td>\n",
       "      <td>0.024283</td>\n",
       "      <td>0.023946</td>\n",
       "      <td>0.048229</td>\n",
       "      <td>0.030691</td>\n",
       "      <td>0.031703</td>\n",
       "      <td>0.035750</td>\n",
       "      <td>0.030354</td>\n",
       "      <td>0.026981</td>\n",
       "      <td>0.027993</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033390</td>\n",
       "      <td>0.035076</td>\n",
       "      <td>0.022260</td>\n",
       "      <td>0.029005</td>\n",
       "      <td>0.033052</td>\n",
       "      <td>0.067454</td>\n",
       "      <td>0.027319</td>\n",
       "      <td>0.048904</td>\n",
       "      <td>0.029005</td>\n",
       "      <td>0.027993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.030753</td>\n",
       "      <td>0.052522</td>\n",
       "      <td>0.022460</td>\n",
       "      <td>0.034900</td>\n",
       "      <td>0.044229</td>\n",
       "      <td>0.022460</td>\n",
       "      <td>0.024188</td>\n",
       "      <td>0.026261</td>\n",
       "      <td>0.031790</td>\n",
       "      <td>0.031099</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044229</td>\n",
       "      <td>0.039737</td>\n",
       "      <td>0.036973</td>\n",
       "      <td>0.030408</td>\n",
       "      <td>0.027643</td>\n",
       "      <td>0.043538</td>\n",
       "      <td>0.027298</td>\n",
       "      <td>0.029026</td>\n",
       "      <td>0.021424</td>\n",
       "      <td>0.046303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.029282</td>\n",
       "      <td>0.040518</td>\n",
       "      <td>0.031324</td>\n",
       "      <td>0.028941</td>\n",
       "      <td>0.040518</td>\n",
       "      <td>0.032346</td>\n",
       "      <td>0.059925</td>\n",
       "      <td>0.024855</td>\n",
       "      <td>0.025536</td>\n",
       "      <td>0.044263</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048689</td>\n",
       "      <td>0.023153</td>\n",
       "      <td>0.023834</td>\n",
       "      <td>0.033027</td>\n",
       "      <td>0.035410</td>\n",
       "      <td>0.026217</td>\n",
       "      <td>0.041539</td>\n",
       "      <td>0.024515</td>\n",
       "      <td>0.043582</td>\n",
       "      <td>0.026558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.035423</td>\n",
       "      <td>0.031173</td>\n",
       "      <td>0.021962</td>\n",
       "      <td>0.029756</td>\n",
       "      <td>0.024088</td>\n",
       "      <td>0.024442</td>\n",
       "      <td>0.040383</td>\n",
       "      <td>0.066596</td>\n",
       "      <td>0.031881</td>\n",
       "      <td>0.035069</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019837</td>\n",
       "      <td>0.046050</td>\n",
       "      <td>0.084662</td>\n",
       "      <td>0.025859</td>\n",
       "      <td>0.030818</td>\n",
       "      <td>0.025859</td>\n",
       "      <td>0.024796</td>\n",
       "      <td>0.023734</td>\n",
       "      <td>0.027630</td>\n",
       "      <td>0.025151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.023305</td>\n",
       "      <td>0.079802</td>\n",
       "      <td>0.026483</td>\n",
       "      <td>0.021540</td>\n",
       "      <td>0.027542</td>\n",
       "      <td>0.032486</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>0.024011</td>\n",
       "      <td>0.046963</td>\n",
       "      <td>0.030720</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024364</td>\n",
       "      <td>0.045198</td>\n",
       "      <td>0.044845</td>\n",
       "      <td>0.033192</td>\n",
       "      <td>0.025777</td>\n",
       "      <td>0.030720</td>\n",
       "      <td>0.029661</td>\n",
       "      <td>0.044138</td>\n",
       "      <td>0.022246</td>\n",
       "      <td>0.022246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.025321</td>\n",
       "      <td>0.079529</td>\n",
       "      <td>0.028174</td>\n",
       "      <td>0.053138</td>\n",
       "      <td>0.031027</td>\n",
       "      <td>0.024251</td>\n",
       "      <td>0.037447</td>\n",
       "      <td>0.042796</td>\n",
       "      <td>0.033167</td>\n",
       "      <td>0.024608</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023538</td>\n",
       "      <td>0.016762</td>\n",
       "      <td>0.024251</td>\n",
       "      <td>0.042083</td>\n",
       "      <td>0.039586</td>\n",
       "      <td>0.028531</td>\n",
       "      <td>0.036733</td>\n",
       "      <td>0.022825</td>\n",
       "      <td>0.041369</td>\n",
       "      <td>0.033524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.028690</td>\n",
       "      <td>0.038714</td>\n",
       "      <td>0.029036</td>\n",
       "      <td>0.050467</td>\n",
       "      <td>0.035949</td>\n",
       "      <td>0.026962</td>\n",
       "      <td>0.032492</td>\n",
       "      <td>0.027307</td>\n",
       "      <td>0.017283</td>\n",
       "      <td>0.037331</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032838</td>\n",
       "      <td>0.016592</td>\n",
       "      <td>0.032492</td>\n",
       "      <td>0.023851</td>\n",
       "      <td>0.029381</td>\n",
       "      <td>0.044936</td>\n",
       "      <td>0.040788</td>\n",
       "      <td>0.042516</td>\n",
       "      <td>0.046664</td>\n",
       "      <td>0.024196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.031962</td>\n",
       "      <td>0.041142</td>\n",
       "      <td>0.039442</td>\n",
       "      <td>0.025842</td>\n",
       "      <td>0.030602</td>\n",
       "      <td>0.024141</td>\n",
       "      <td>0.037742</td>\n",
       "      <td>0.037742</td>\n",
       "      <td>0.032302</td>\n",
       "      <td>0.029242</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031962</td>\n",
       "      <td>0.034682</td>\n",
       "      <td>0.017681</td>\n",
       "      <td>0.040462</td>\n",
       "      <td>0.024821</td>\n",
       "      <td>0.028902</td>\n",
       "      <td>0.032642</td>\n",
       "      <td>0.018361</td>\n",
       "      <td>0.061544</td>\n",
       "      <td>0.041823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.016518</td>\n",
       "      <td>0.061597</td>\n",
       "      <td>0.028906</td>\n",
       "      <td>0.039573</td>\n",
       "      <td>0.039229</td>\n",
       "      <td>0.043014</td>\n",
       "      <td>0.030970</td>\n",
       "      <td>0.041982</td>\n",
       "      <td>0.019270</td>\n",
       "      <td>0.036132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031315</td>\n",
       "      <td>0.019615</td>\n",
       "      <td>0.015141</td>\n",
       "      <td>0.016862</td>\n",
       "      <td>0.034412</td>\n",
       "      <td>0.049553</td>\n",
       "      <td>0.037853</td>\n",
       "      <td>0.037853</td>\n",
       "      <td>0.037853</td>\n",
       "      <td>0.037853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.025478</td>\n",
       "      <td>0.042575</td>\n",
       "      <td>0.035535</td>\n",
       "      <td>0.020114</td>\n",
       "      <td>0.030841</td>\n",
       "      <td>0.046597</td>\n",
       "      <td>0.026483</td>\n",
       "      <td>0.013409</td>\n",
       "      <td>0.043245</td>\n",
       "      <td>0.055649</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023802</td>\n",
       "      <td>0.044921</td>\n",
       "      <td>0.022461</td>\n",
       "      <td>0.026483</td>\n",
       "      <td>0.045592</td>\n",
       "      <td>0.038887</td>\n",
       "      <td>0.040898</td>\n",
       "      <td>0.023466</td>\n",
       "      <td>0.054308</td>\n",
       "      <td>0.017767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.025948</td>\n",
       "      <td>0.032269</td>\n",
       "      <td>0.025948</td>\n",
       "      <td>0.036261</td>\n",
       "      <td>0.031936</td>\n",
       "      <td>0.032934</td>\n",
       "      <td>0.044245</td>\n",
       "      <td>0.033267</td>\n",
       "      <td>0.024285</td>\n",
       "      <td>0.021291</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034597</td>\n",
       "      <td>0.049900</td>\n",
       "      <td>0.025283</td>\n",
       "      <td>0.021623</td>\n",
       "      <td>0.034597</td>\n",
       "      <td>0.030605</td>\n",
       "      <td>0.033932</td>\n",
       "      <td>0.050898</td>\n",
       "      <td>0.027944</td>\n",
       "      <td>0.029607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.008348</td>\n",
       "      <td>0.051826</td>\n",
       "      <td>0.037913</td>\n",
       "      <td>0.042783</td>\n",
       "      <td>0.029913</td>\n",
       "      <td>0.031652</td>\n",
       "      <td>0.018435</td>\n",
       "      <td>0.038261</td>\n",
       "      <td>0.026435</td>\n",
       "      <td>0.034783</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042087</td>\n",
       "      <td>0.041739</td>\n",
       "      <td>0.029565</td>\n",
       "      <td>0.027130</td>\n",
       "      <td>0.028174</td>\n",
       "      <td>0.025043</td>\n",
       "      <td>0.032696</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.039652</td>\n",
       "      <td>0.030957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.015753</td>\n",
       "      <td>0.031835</td>\n",
       "      <td>0.040696</td>\n",
       "      <td>0.019035</td>\n",
       "      <td>0.031178</td>\n",
       "      <td>0.020676</td>\n",
       "      <td>0.017722</td>\n",
       "      <td>0.029209</td>\n",
       "      <td>0.072530</td>\n",
       "      <td>0.038398</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042337</td>\n",
       "      <td>0.035773</td>\n",
       "      <td>0.050213</td>\n",
       "      <td>0.020348</td>\n",
       "      <td>0.036101</td>\n",
       "      <td>0.018379</td>\n",
       "      <td>0.026255</td>\n",
       "      <td>0.052839</td>\n",
       "      <td>0.043321</td>\n",
       "      <td>0.021004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.040027</td>\n",
       "      <td>0.047151</td>\n",
       "      <td>0.024763</td>\n",
       "      <td>0.035617</td>\n",
       "      <td>0.024423</td>\n",
       "      <td>0.054613</td>\n",
       "      <td>0.029851</td>\n",
       "      <td>0.024423</td>\n",
       "      <td>0.028155</td>\n",
       "      <td>0.017978</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.032564</td>\n",
       "      <td>0.048847</td>\n",
       "      <td>0.030868</td>\n",
       "      <td>0.022388</td>\n",
       "      <td>0.035617</td>\n",
       "      <td>0.046133</td>\n",
       "      <td>0.050882</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.032225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.045224</td>\n",
       "      <td>0.030712</td>\n",
       "      <td>0.029362</td>\n",
       "      <td>0.054674</td>\n",
       "      <td>0.025650</td>\n",
       "      <td>0.017212</td>\n",
       "      <td>0.019575</td>\n",
       "      <td>0.031387</td>\n",
       "      <td>0.034087</td>\n",
       "      <td>0.038137</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068512</td>\n",
       "      <td>0.047249</td>\n",
       "      <td>0.021937</td>\n",
       "      <td>0.030037</td>\n",
       "      <td>0.037125</td>\n",
       "      <td>0.031387</td>\n",
       "      <td>0.029025</td>\n",
       "      <td>0.033075</td>\n",
       "      <td>0.036450</td>\n",
       "      <td>0.021937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.020681</td>\n",
       "      <td>0.052498</td>\n",
       "      <td>0.027044</td>\n",
       "      <td>0.041362</td>\n",
       "      <td>0.061406</td>\n",
       "      <td>0.016545</td>\n",
       "      <td>0.025135</td>\n",
       "      <td>0.033408</td>\n",
       "      <td>0.023863</td>\n",
       "      <td>0.036271</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022908</td>\n",
       "      <td>0.046771</td>\n",
       "      <td>0.035953</td>\n",
       "      <td>0.019408</td>\n",
       "      <td>0.031180</td>\n",
       "      <td>0.036907</td>\n",
       "      <td>0.039771</td>\n",
       "      <td>0.043907</td>\n",
       "      <td>0.023544</td>\n",
       "      <td>0.024817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.026368</td>\n",
       "      <td>0.033289</td>\n",
       "      <td>0.052406</td>\n",
       "      <td>0.030323</td>\n",
       "      <td>0.029993</td>\n",
       "      <td>0.032630</td>\n",
       "      <td>0.044496</td>\n",
       "      <td>0.034278</td>\n",
       "      <td>0.033619</td>\n",
       "      <td>0.026038</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031971</td>\n",
       "      <td>0.026368</td>\n",
       "      <td>0.024061</td>\n",
       "      <td>0.033949</td>\n",
       "      <td>0.037245</td>\n",
       "      <td>0.034608</td>\n",
       "      <td>0.023401</td>\n",
       "      <td>0.028675</td>\n",
       "      <td>0.040870</td>\n",
       "      <td>0.031641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.022696</td>\n",
       "      <td>0.037263</td>\n",
       "      <td>0.038957</td>\n",
       "      <td>0.024051</td>\n",
       "      <td>0.032520</td>\n",
       "      <td>0.032520</td>\n",
       "      <td>0.043360</td>\n",
       "      <td>0.029810</td>\n",
       "      <td>0.028117</td>\n",
       "      <td>0.030149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023374</td>\n",
       "      <td>0.036585</td>\n",
       "      <td>0.022358</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.025745</td>\n",
       "      <td>0.048442</td>\n",
       "      <td>0.030488</td>\n",
       "      <td>0.029810</td>\n",
       "      <td>0.018970</td>\n",
       "      <td>0.040989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.022878</td>\n",
       "      <td>0.035477</td>\n",
       "      <td>0.042440</td>\n",
       "      <td>0.026857</td>\n",
       "      <td>0.025862</td>\n",
       "      <td>0.044430</td>\n",
       "      <td>0.030172</td>\n",
       "      <td>0.054045</td>\n",
       "      <td>0.027851</td>\n",
       "      <td>0.045424</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024536</td>\n",
       "      <td>0.038130</td>\n",
       "      <td>0.031499</td>\n",
       "      <td>0.016578</td>\n",
       "      <td>0.028846</td>\n",
       "      <td>0.044098</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.032162</td>\n",
       "      <td>0.025199</td>\n",
       "      <td>0.024867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.027293</td>\n",
       "      <td>0.034592</td>\n",
       "      <td>0.073310</td>\n",
       "      <td>0.036814</td>\n",
       "      <td>0.037131</td>\n",
       "      <td>0.019676</td>\n",
       "      <td>0.024437</td>\n",
       "      <td>0.028245</td>\n",
       "      <td>0.037766</td>\n",
       "      <td>0.031419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023485</td>\n",
       "      <td>0.031736</td>\n",
       "      <td>0.015868</td>\n",
       "      <td>0.045700</td>\n",
       "      <td>0.039670</td>\n",
       "      <td>0.040622</td>\n",
       "      <td>0.024119</td>\n",
       "      <td>0.039670</td>\n",
       "      <td>0.021580</td>\n",
       "      <td>0.025706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.025834</td>\n",
       "      <td>0.037316</td>\n",
       "      <td>0.014352</td>\n",
       "      <td>0.042698</td>\n",
       "      <td>0.044133</td>\n",
       "      <td>0.042339</td>\n",
       "      <td>0.033010</td>\n",
       "      <td>0.032652</td>\n",
       "      <td>0.032293</td>\n",
       "      <td>0.032293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021170</td>\n",
       "      <td>0.021529</td>\n",
       "      <td>0.043057</td>\n",
       "      <td>0.049516</td>\n",
       "      <td>0.025117</td>\n",
       "      <td>0.035881</td>\n",
       "      <td>0.040545</td>\n",
       "      <td>0.038034</td>\n",
       "      <td>0.020093</td>\n",
       "      <td>0.037316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.021565</td>\n",
       "      <td>0.034783</td>\n",
       "      <td>0.044870</td>\n",
       "      <td>0.048000</td>\n",
       "      <td>0.018087</td>\n",
       "      <td>0.032000</td>\n",
       "      <td>0.030609</td>\n",
       "      <td>0.039304</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.041739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023304</td>\n",
       "      <td>0.034435</td>\n",
       "      <td>0.034783</td>\n",
       "      <td>0.066435</td>\n",
       "      <td>0.026087</td>\n",
       "      <td>0.033391</td>\n",
       "      <td>0.042087</td>\n",
       "      <td>0.018435</td>\n",
       "      <td>0.055652</td>\n",
       "      <td>0.026435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.045135</td>\n",
       "      <td>0.029756</td>\n",
       "      <td>0.009027</td>\n",
       "      <td>0.049482</td>\n",
       "      <td>0.040455</td>\n",
       "      <td>0.028419</td>\n",
       "      <td>0.035105</td>\n",
       "      <td>0.017385</td>\n",
       "      <td>0.041123</td>\n",
       "      <td>0.024407</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025410</td>\n",
       "      <td>0.024407</td>\n",
       "      <td>0.050485</td>\n",
       "      <td>0.023069</td>\n",
       "      <td>0.027081</td>\n",
       "      <td>0.039452</td>\n",
       "      <td>0.029756</td>\n",
       "      <td>0.024741</td>\n",
       "      <td>0.046138</td>\n",
       "      <td>0.040789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.013252</td>\n",
       "      <td>0.029901</td>\n",
       "      <td>0.025824</td>\n",
       "      <td>0.045532</td>\n",
       "      <td>0.033639</td>\n",
       "      <td>0.039755</td>\n",
       "      <td>0.048590</td>\n",
       "      <td>0.037717</td>\n",
       "      <td>0.022086</td>\n",
       "      <td>0.027183</td>\n",
       "      <td>...</td>\n",
       "      <td>0.049949</td>\n",
       "      <td>0.036697</td>\n",
       "      <td>0.029901</td>\n",
       "      <td>0.062521</td>\n",
       "      <td>0.018009</td>\n",
       "      <td>0.030581</td>\n",
       "      <td>0.026164</td>\n",
       "      <td>0.027863</td>\n",
       "      <td>0.031261</td>\n",
       "      <td>0.034319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.038369</td>\n",
       "      <td>0.039054</td>\n",
       "      <td>0.023638</td>\n",
       "      <td>0.032545</td>\n",
       "      <td>0.031175</td>\n",
       "      <td>0.060980</td>\n",
       "      <td>0.030490</td>\n",
       "      <td>0.043165</td>\n",
       "      <td>0.032888</td>\n",
       "      <td>0.048989</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028434</td>\n",
       "      <td>0.027064</td>\n",
       "      <td>0.038027</td>\n",
       "      <td>0.018842</td>\n",
       "      <td>0.022953</td>\n",
       "      <td>0.041453</td>\n",
       "      <td>0.035971</td>\n",
       "      <td>0.016787</td>\n",
       "      <td>0.021925</td>\n",
       "      <td>0.025351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.021850</td>\n",
       "      <td>0.050188</td>\n",
       "      <td>0.031751</td>\n",
       "      <td>0.046091</td>\n",
       "      <td>0.030386</td>\n",
       "      <td>0.020826</td>\n",
       "      <td>0.018095</td>\n",
       "      <td>0.026972</td>\n",
       "      <td>0.037214</td>\n",
       "      <td>0.038921</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029020</td>\n",
       "      <td>0.045067</td>\n",
       "      <td>0.035166</td>\n",
       "      <td>0.035848</td>\n",
       "      <td>0.031751</td>\n",
       "      <td>0.030727</td>\n",
       "      <td>0.025606</td>\n",
       "      <td>0.041652</td>\n",
       "      <td>0.028679</td>\n",
       "      <td>0.028337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.028075</td>\n",
       "      <td>0.039773</td>\n",
       "      <td>0.052139</td>\n",
       "      <td>0.044452</td>\n",
       "      <td>0.053810</td>\n",
       "      <td>0.028409</td>\n",
       "      <td>0.036765</td>\n",
       "      <td>0.030414</td>\n",
       "      <td>0.040775</td>\n",
       "      <td>0.035428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032754</td>\n",
       "      <td>0.031083</td>\n",
       "      <td>0.032420</td>\n",
       "      <td>0.041778</td>\n",
       "      <td>0.032420</td>\n",
       "      <td>0.015040</td>\n",
       "      <td>0.026404</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.017045</td>\n",
       "      <td>0.029078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.042112</td>\n",
       "      <td>0.022393</td>\n",
       "      <td>0.027741</td>\n",
       "      <td>0.034091</td>\n",
       "      <td>0.027741</td>\n",
       "      <td>0.020722</td>\n",
       "      <td>0.033088</td>\n",
       "      <td>0.047794</td>\n",
       "      <td>0.051805</td>\n",
       "      <td>0.071190</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026404</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.031083</td>\n",
       "      <td>0.026070</td>\n",
       "      <td>0.035762</td>\n",
       "      <td>0.043449</td>\n",
       "      <td>0.050802</td>\n",
       "      <td>0.025401</td>\n",
       "      <td>0.017714</td>\n",
       "      <td>0.028075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.027901</td>\n",
       "      <td>0.040830</td>\n",
       "      <td>0.041851</td>\n",
       "      <td>0.027901</td>\n",
       "      <td>0.019394</td>\n",
       "      <td>0.036747</td>\n",
       "      <td>0.033345</td>\n",
       "      <td>0.025519</td>\n",
       "      <td>0.033004</td>\n",
       "      <td>0.026880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023477</td>\n",
       "      <td>0.039809</td>\n",
       "      <td>0.024838</td>\n",
       "      <td>0.044913</td>\n",
       "      <td>0.027901</td>\n",
       "      <td>0.025179</td>\n",
       "      <td>0.017693</td>\n",
       "      <td>0.024158</td>\n",
       "      <td>0.046274</td>\n",
       "      <td>0.045934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.029795</td>\n",
       "      <td>0.018932</td>\n",
       "      <td>0.021726</td>\n",
       "      <td>0.037554</td>\n",
       "      <td>0.031037</td>\n",
       "      <td>0.067349</td>\n",
       "      <td>0.022657</td>\n",
       "      <td>0.027933</td>\n",
       "      <td>0.036934</td>\n",
       "      <td>0.023588</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030106</td>\n",
       "      <td>0.041279</td>\n",
       "      <td>0.027933</td>\n",
       "      <td>0.024209</td>\n",
       "      <td>0.050900</td>\n",
       "      <td>0.033520</td>\n",
       "      <td>0.027933</td>\n",
       "      <td>0.034140</td>\n",
       "      <td>0.022036</td>\n",
       "      <td>0.042210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.030500</td>\n",
       "      <td>0.027255</td>\n",
       "      <td>0.034718</td>\n",
       "      <td>0.023037</td>\n",
       "      <td>0.032446</td>\n",
       "      <td>0.044452</td>\n",
       "      <td>0.021415</td>\n",
       "      <td>0.042180</td>\n",
       "      <td>0.033744</td>\n",
       "      <td>0.054186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030500</td>\n",
       "      <td>0.052239</td>\n",
       "      <td>0.027579</td>\n",
       "      <td>0.036989</td>\n",
       "      <td>0.034393</td>\n",
       "      <td>0.016872</td>\n",
       "      <td>0.025957</td>\n",
       "      <td>0.027904</td>\n",
       "      <td>0.028877</td>\n",
       "      <td>0.025957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.023103</td>\n",
       "      <td>0.023448</td>\n",
       "      <td>0.049655</td>\n",
       "      <td>0.040690</td>\n",
       "      <td>0.032759</td>\n",
       "      <td>0.025862</td>\n",
       "      <td>0.036552</td>\n",
       "      <td>0.031724</td>\n",
       "      <td>0.032759</td>\n",
       "      <td>0.026552</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034828</td>\n",
       "      <td>0.040690</td>\n",
       "      <td>0.038276</td>\n",
       "      <td>0.023103</td>\n",
       "      <td>0.029655</td>\n",
       "      <td>0.028276</td>\n",
       "      <td>0.038966</td>\n",
       "      <td>0.023103</td>\n",
       "      <td>0.054828</td>\n",
       "      <td>0.035517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.029207</td>\n",
       "      <td>0.029207</td>\n",
       "      <td>0.032526</td>\n",
       "      <td>0.034185</td>\n",
       "      <td>0.029871</td>\n",
       "      <td>0.021573</td>\n",
       "      <td>0.024560</td>\n",
       "      <td>0.027215</td>\n",
       "      <td>0.035845</td>\n",
       "      <td>0.031198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038168</td>\n",
       "      <td>0.032194</td>\n",
       "      <td>0.018918</td>\n",
       "      <td>0.032526</td>\n",
       "      <td>0.032526</td>\n",
       "      <td>0.032194</td>\n",
       "      <td>0.025224</td>\n",
       "      <td>0.044806</td>\n",
       "      <td>0.029871</td>\n",
       "      <td>0.023233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.051105</td>\n",
       "      <td>0.018992</td>\n",
       "      <td>0.026588</td>\n",
       "      <td>0.038674</td>\n",
       "      <td>0.060083</td>\n",
       "      <td>0.025207</td>\n",
       "      <td>0.026934</td>\n",
       "      <td>0.030387</td>\n",
       "      <td>0.015539</td>\n",
       "      <td>0.025552</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031077</td>\n",
       "      <td>0.023135</td>\n",
       "      <td>0.027970</td>\n",
       "      <td>0.027624</td>\n",
       "      <td>0.063191</td>\n",
       "      <td>0.040401</td>\n",
       "      <td>0.030041</td>\n",
       "      <td>0.024862</td>\n",
       "      <td>0.031077</td>\n",
       "      <td>0.027970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.027721</td>\n",
       "      <td>0.026694</td>\n",
       "      <td>0.023614</td>\n",
       "      <td>0.041410</td>\n",
       "      <td>0.045517</td>\n",
       "      <td>0.057495</td>\n",
       "      <td>0.023272</td>\n",
       "      <td>0.044148</td>\n",
       "      <td>0.028747</td>\n",
       "      <td>0.030116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046886</td>\n",
       "      <td>0.024298</td>\n",
       "      <td>0.032170</td>\n",
       "      <td>0.021218</td>\n",
       "      <td>0.035250</td>\n",
       "      <td>0.029774</td>\n",
       "      <td>0.029774</td>\n",
       "      <td>0.034565</td>\n",
       "      <td>0.027721</td>\n",
       "      <td>0.027036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.059399</td>\n",
       "      <td>0.023962</td>\n",
       "      <td>0.024975</td>\n",
       "      <td>0.015862</td>\n",
       "      <td>0.035775</td>\n",
       "      <td>0.043874</td>\n",
       "      <td>0.022950</td>\n",
       "      <td>0.053662</td>\n",
       "      <td>0.023625</td>\n",
       "      <td>0.040837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028012</td>\n",
       "      <td>0.022612</td>\n",
       "      <td>0.029025</td>\n",
       "      <td>0.033412</td>\n",
       "      <td>0.029025</td>\n",
       "      <td>0.045899</td>\n",
       "      <td>0.025312</td>\n",
       "      <td>0.030375</td>\n",
       "      <td>0.052312</td>\n",
       "      <td>0.025312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.018266</td>\n",
       "      <td>0.029558</td>\n",
       "      <td>0.053803</td>\n",
       "      <td>0.030223</td>\n",
       "      <td>0.050149</td>\n",
       "      <td>0.038858</td>\n",
       "      <td>0.037861</td>\n",
       "      <td>0.030223</td>\n",
       "      <td>0.033212</td>\n",
       "      <td>0.041182</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030887</td>\n",
       "      <td>0.034540</td>\n",
       "      <td>0.031883</td>\n",
       "      <td>0.011956</td>\n",
       "      <td>0.032879</td>\n",
       "      <td>0.040518</td>\n",
       "      <td>0.048157</td>\n",
       "      <td>0.036201</td>\n",
       "      <td>0.045500</td>\n",
       "      <td>0.033212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.049482</td>\n",
       "      <td>0.035105</td>\n",
       "      <td>0.016717</td>\n",
       "      <td>0.030759</td>\n",
       "      <td>0.037780</td>\n",
       "      <td>0.038449</td>\n",
       "      <td>0.040789</td>\n",
       "      <td>0.031428</td>\n",
       "      <td>0.033768</td>\n",
       "      <td>0.016717</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036443</td>\n",
       "      <td>0.036108</td>\n",
       "      <td>0.015714</td>\n",
       "      <td>0.026078</td>\n",
       "      <td>0.021398</td>\n",
       "      <td>0.054831</td>\n",
       "      <td>0.041458</td>\n",
       "      <td>0.024072</td>\n",
       "      <td>0.021732</td>\n",
       "      <td>0.027750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.025519</td>\n",
       "      <td>0.040150</td>\n",
       "      <td>0.038789</td>\n",
       "      <td>0.049677</td>\n",
       "      <td>0.040150</td>\n",
       "      <td>0.029602</td>\n",
       "      <td>0.020415</td>\n",
       "      <td>0.021436</td>\n",
       "      <td>0.030282</td>\n",
       "      <td>0.021096</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026880</td>\n",
       "      <td>0.020755</td>\n",
       "      <td>0.022116</td>\n",
       "      <td>0.031984</td>\n",
       "      <td>0.023477</td>\n",
       "      <td>0.020075</td>\n",
       "      <td>0.035046</td>\n",
       "      <td>0.056482</td>\n",
       "      <td>0.029262</td>\n",
       "      <td>0.035046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.043424</td>\n",
       "      <td>0.032725</td>\n",
       "      <td>0.012901</td>\n",
       "      <td>0.032410</td>\n",
       "      <td>0.020138</td>\n",
       "      <td>0.025173</td>\n",
       "      <td>0.032725</td>\n",
       "      <td>0.035557</td>\n",
       "      <td>0.047829</td>\n",
       "      <td>0.040277</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028320</td>\n",
       "      <td>0.027690</td>\n",
       "      <td>0.026117</td>\n",
       "      <td>0.030837</td>\n",
       "      <td>0.034613</td>\n",
       "      <td>0.039333</td>\n",
       "      <td>0.041221</td>\n",
       "      <td>0.042794</td>\n",
       "      <td>0.038074</td>\n",
       "      <td>0.039333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.042568</td>\n",
       "      <td>0.027806</td>\n",
       "      <td>0.029866</td>\n",
       "      <td>0.018881</td>\n",
       "      <td>0.019911</td>\n",
       "      <td>0.041881</td>\n",
       "      <td>0.048747</td>\n",
       "      <td>0.015791</td>\n",
       "      <td>0.025403</td>\n",
       "      <td>0.019567</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022657</td>\n",
       "      <td>0.043598</td>\n",
       "      <td>0.018881</td>\n",
       "      <td>0.033642</td>\n",
       "      <td>0.049090</td>\n",
       "      <td>0.041195</td>\n",
       "      <td>0.046344</td>\n",
       "      <td>0.020254</td>\n",
       "      <td>0.024717</td>\n",
       "      <td>0.034329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.017952</td>\n",
       "      <td>0.037566</td>\n",
       "      <td>0.033910</td>\n",
       "      <td>0.036237</td>\n",
       "      <td>0.020612</td>\n",
       "      <td>0.032247</td>\n",
       "      <td>0.023271</td>\n",
       "      <td>0.032247</td>\n",
       "      <td>0.024934</td>\n",
       "      <td>0.043883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030585</td>\n",
       "      <td>0.026263</td>\n",
       "      <td>0.019614</td>\n",
       "      <td>0.033245</td>\n",
       "      <td>0.048205</td>\n",
       "      <td>0.021941</td>\n",
       "      <td>0.040226</td>\n",
       "      <td>0.025598</td>\n",
       "      <td>0.046210</td>\n",
       "      <td>0.030585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.030666</td>\n",
       "      <td>0.049700</td>\n",
       "      <td>0.033134</td>\n",
       "      <td>0.036306</td>\n",
       "      <td>0.035249</td>\n",
       "      <td>0.032781</td>\n",
       "      <td>0.047938</td>\n",
       "      <td>0.028551</td>\n",
       "      <td>0.047233</td>\n",
       "      <td>0.021149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043003</td>\n",
       "      <td>0.017272</td>\n",
       "      <td>0.044766</td>\n",
       "      <td>0.039126</td>\n",
       "      <td>0.032429</td>\n",
       "      <td>0.028551</td>\n",
       "      <td>0.036306</td>\n",
       "      <td>0.026789</td>\n",
       "      <td>0.012689</td>\n",
       "      <td>0.032429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.028689</td>\n",
       "      <td>0.034836</td>\n",
       "      <td>0.058060</td>\n",
       "      <td>0.024932</td>\n",
       "      <td>0.052254</td>\n",
       "      <td>0.024590</td>\n",
       "      <td>0.045423</td>\n",
       "      <td>0.031079</td>\n",
       "      <td>0.021175</td>\n",
       "      <td>0.044399</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040301</td>\n",
       "      <td>0.025956</td>\n",
       "      <td>0.052937</td>\n",
       "      <td>0.018443</td>\n",
       "      <td>0.014344</td>\n",
       "      <td>0.020150</td>\n",
       "      <td>0.043033</td>\n",
       "      <td>0.024590</td>\n",
       "      <td>0.043374</td>\n",
       "      <td>0.039617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.029620</td>\n",
       "      <td>0.024907</td>\n",
       "      <td>0.026590</td>\n",
       "      <td>0.049142</td>\n",
       "      <td>0.023225</td>\n",
       "      <td>0.034332</td>\n",
       "      <td>0.049815</td>\n",
       "      <td>0.039717</td>\n",
       "      <td>0.046112</td>\n",
       "      <td>0.047795</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030966</td>\n",
       "      <td>0.026927</td>\n",
       "      <td>0.026927</td>\n",
       "      <td>0.044766</td>\n",
       "      <td>0.018512</td>\n",
       "      <td>0.034332</td>\n",
       "      <td>0.034332</td>\n",
       "      <td>0.019859</td>\n",
       "      <td>0.024571</td>\n",
       "      <td>0.038708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.022356</td>\n",
       "      <td>0.017684</td>\n",
       "      <td>0.034368</td>\n",
       "      <td>0.022022</td>\n",
       "      <td>0.032032</td>\n",
       "      <td>0.034701</td>\n",
       "      <td>0.031698</td>\n",
       "      <td>0.024024</td>\n",
       "      <td>0.052386</td>\n",
       "      <td>0.028695</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039706</td>\n",
       "      <td>0.019019</td>\n",
       "      <td>0.046713</td>\n",
       "      <td>0.041375</td>\n",
       "      <td>0.022689</td>\n",
       "      <td>0.032366</td>\n",
       "      <td>0.020354</td>\n",
       "      <td>0.018352</td>\n",
       "      <td>0.049383</td>\n",
       "      <td>0.050050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   0.063787  0.041860  0.036545  0.031894  0.038538  0.019601  0.037209   \n",
       "1   0.045517  0.035172  0.022759  0.044483  0.047586  0.025517  0.027931   \n",
       "2   0.015614  0.027412  0.045108  0.021513  0.058987  0.018737  0.040250   \n",
       "3   0.030602  0.021761  0.017681  0.032982  0.030942  0.028222  0.034682   \n",
       "4   0.045194  0.024283  0.023946  0.048229  0.030691  0.031703  0.035750   \n",
       "5   0.030753  0.052522  0.022460  0.034900  0.044229  0.022460  0.024188   \n",
       "6   0.029282  0.040518  0.031324  0.028941  0.040518  0.032346  0.059925   \n",
       "7   0.035423  0.031173  0.021962  0.029756  0.024088  0.024442  0.040383   \n",
       "8   0.023305  0.079802  0.026483  0.021540  0.027542  0.032486  0.033898   \n",
       "9   0.025321  0.079529  0.028174  0.053138  0.031027  0.024251  0.037447   \n",
       "10  0.028690  0.038714  0.029036  0.050467  0.035949  0.026962  0.032492   \n",
       "11  0.031962  0.041142  0.039442  0.025842  0.030602  0.024141  0.037742   \n",
       "12  0.016518  0.061597  0.028906  0.039573  0.039229  0.043014  0.030970   \n",
       "13  0.025478  0.042575  0.035535  0.020114  0.030841  0.046597  0.026483   \n",
       "14  0.025948  0.032269  0.025948  0.036261  0.031936  0.032934  0.044245   \n",
       "15  0.008348  0.051826  0.037913  0.042783  0.029913  0.031652  0.018435   \n",
       "16  0.015753  0.031835  0.040696  0.019035  0.031178  0.020676  0.017722   \n",
       "17  0.040027  0.047151  0.024763  0.035617  0.024423  0.054613  0.029851   \n",
       "18  0.045224  0.030712  0.029362  0.054674  0.025650  0.017212  0.019575   \n",
       "19  0.020681  0.052498  0.027044  0.041362  0.061406  0.016545  0.025135   \n",
       "20  0.026368  0.033289  0.052406  0.030323  0.029993  0.032630  0.044496   \n",
       "21  0.022696  0.037263  0.038957  0.024051  0.032520  0.032520  0.043360   \n",
       "22  0.022878  0.035477  0.042440  0.026857  0.025862  0.044430  0.030172   \n",
       "23  0.027293  0.034592  0.073310  0.036814  0.037131  0.019676  0.024437   \n",
       "24  0.025834  0.037316  0.014352  0.042698  0.044133  0.042339  0.033010   \n",
       "25  0.021565  0.034783  0.044870  0.048000  0.018087  0.032000  0.030609   \n",
       "26  0.045135  0.029756  0.009027  0.049482  0.040455  0.028419  0.035105   \n",
       "27  0.013252  0.029901  0.025824  0.045532  0.033639  0.039755  0.048590   \n",
       "28  0.038369  0.039054  0.023638  0.032545  0.031175  0.060980  0.030490   \n",
       "29  0.021850  0.050188  0.031751  0.046091  0.030386  0.020826  0.018095   \n",
       "30  0.028075  0.039773  0.052139  0.044452  0.053810  0.028409  0.036765   \n",
       "31  0.042112  0.022393  0.027741  0.034091  0.027741  0.020722  0.033088   \n",
       "32  0.027901  0.040830  0.041851  0.027901  0.019394  0.036747  0.033345   \n",
       "33  0.029795  0.018932  0.021726  0.037554  0.031037  0.067349  0.022657   \n",
       "34  0.030500  0.027255  0.034718  0.023037  0.032446  0.044452  0.021415   \n",
       "35  0.023103  0.023448  0.049655  0.040690  0.032759  0.025862  0.036552   \n",
       "36  0.029207  0.029207  0.032526  0.034185  0.029871  0.021573  0.024560   \n",
       "37  0.051105  0.018992  0.026588  0.038674  0.060083  0.025207  0.026934   \n",
       "38  0.027721  0.026694  0.023614  0.041410  0.045517  0.057495  0.023272   \n",
       "39  0.059399  0.023962  0.024975  0.015862  0.035775  0.043874  0.022950   \n",
       "40  0.018266  0.029558  0.053803  0.030223  0.050149  0.038858  0.037861   \n",
       "41  0.049482  0.035105  0.016717  0.030759  0.037780  0.038449  0.040789   \n",
       "42  0.025519  0.040150  0.038789  0.049677  0.040150  0.029602  0.020415   \n",
       "43  0.043424  0.032725  0.012901  0.032410  0.020138  0.025173  0.032725   \n",
       "44  0.042568  0.027806  0.029866  0.018881  0.019911  0.041881  0.048747   \n",
       "45  0.017952  0.037566  0.033910  0.036237  0.020612  0.032247  0.023271   \n",
       "46  0.030666  0.049700  0.033134  0.036306  0.035249  0.032781  0.047938   \n",
       "47  0.028689  0.034836  0.058060  0.024932  0.052254  0.024590  0.045423   \n",
       "48  0.029620  0.024907  0.026590  0.049142  0.023225  0.034332  0.049815   \n",
       "49  0.022356  0.017684  0.034368  0.022022  0.032032  0.034701  0.031698   \n",
       "\n",
       "          7         8         9   ...        20        21        22        23  \\\n",
       "0   0.044186  0.028239  0.039203  ...  0.020930  0.031229  0.036545  0.015947   \n",
       "1   0.066897  0.016897  0.032069  ...  0.037586  0.026552  0.042414  0.034138   \n",
       "2   0.039903  0.045108  0.026371  ...  0.023942  0.021860  0.035739  0.026718   \n",
       "3   0.026862  0.066984  0.044203  ...  0.026522  0.038422  0.046923  0.031282   \n",
       "4   0.030354  0.026981  0.027993  ...  0.033390  0.035076  0.022260  0.029005   \n",
       "5   0.026261  0.031790  0.031099  ...  0.044229  0.039737  0.036973  0.030408   \n",
       "6   0.024855  0.025536  0.044263  ...  0.048689  0.023153  0.023834  0.033027   \n",
       "7   0.066596  0.031881  0.035069  ...  0.019837  0.046050  0.084662  0.025859   \n",
       "8   0.024011  0.046963  0.030720  ...  0.024364  0.045198  0.044845  0.033192   \n",
       "9   0.042796  0.033167  0.024608  ...  0.023538  0.016762  0.024251  0.042083   \n",
       "10  0.027307  0.017283  0.037331  ...  0.032838  0.016592  0.032492  0.023851   \n",
       "11  0.037742  0.032302  0.029242  ...  0.031962  0.034682  0.017681  0.040462   \n",
       "12  0.041982  0.019270  0.036132  ...  0.031315  0.019615  0.015141  0.016862   \n",
       "13  0.013409  0.043245  0.055649  ...  0.023802  0.044921  0.022461  0.026483   \n",
       "14  0.033267  0.024285  0.021291  ...  0.034597  0.049900  0.025283  0.021623   \n",
       "15  0.038261  0.026435  0.034783  ...  0.042087  0.041739  0.029565  0.027130   \n",
       "16  0.029209  0.072530  0.038398  ...  0.042337  0.035773  0.050213  0.020348   \n",
       "17  0.024423  0.028155  0.017978  ...  0.022727  0.032564  0.048847  0.030868   \n",
       "18  0.031387  0.034087  0.038137  ...  0.068512  0.047249  0.021937  0.030037   \n",
       "19  0.033408  0.023863  0.036271  ...  0.022908  0.046771  0.035953  0.019408   \n",
       "20  0.034278  0.033619  0.026038  ...  0.031971  0.026368  0.024061  0.033949   \n",
       "21  0.029810  0.028117  0.030149  ...  0.023374  0.036585  0.022358  0.055556   \n",
       "22  0.054045  0.027851  0.045424  ...  0.024536  0.038130  0.031499  0.016578   \n",
       "23  0.028245  0.037766  0.031419  ...  0.023485  0.031736  0.015868  0.045700   \n",
       "24  0.032652  0.032293  0.032293  ...  0.021170  0.021529  0.043057  0.049516   \n",
       "25  0.039304  0.025391  0.041739  ...  0.023304  0.034435  0.034783  0.066435   \n",
       "26  0.017385  0.041123  0.024407  ...  0.025410  0.024407  0.050485  0.023069   \n",
       "27  0.037717  0.022086  0.027183  ...  0.049949  0.036697  0.029901  0.062521   \n",
       "28  0.043165  0.032888  0.048989  ...  0.028434  0.027064  0.038027  0.018842   \n",
       "29  0.026972  0.037214  0.038921  ...  0.029020  0.045067  0.035166  0.035848   \n",
       "30  0.030414  0.040775  0.035428  ...  0.032754  0.031083  0.032420  0.041778   \n",
       "31  0.047794  0.051805  0.071190  ...  0.026404  0.029412  0.031083  0.026070   \n",
       "32  0.025519  0.033004  0.026880  ...  0.023477  0.039809  0.024838  0.044913   \n",
       "33  0.027933  0.036934  0.023588  ...  0.030106  0.041279  0.027933  0.024209   \n",
       "34  0.042180  0.033744  0.054186  ...  0.030500  0.052239  0.027579  0.036989   \n",
       "35  0.031724  0.032759  0.026552  ...  0.034828  0.040690  0.038276  0.023103   \n",
       "36  0.027215  0.035845  0.031198  ...  0.038168  0.032194  0.018918  0.032526   \n",
       "37  0.030387  0.015539  0.025552  ...  0.031077  0.023135  0.027970  0.027624   \n",
       "38  0.044148  0.028747  0.030116  ...  0.046886  0.024298  0.032170  0.021218   \n",
       "39  0.053662  0.023625  0.040837  ...  0.028012  0.022612  0.029025  0.033412   \n",
       "40  0.030223  0.033212  0.041182  ...  0.030887  0.034540  0.031883  0.011956   \n",
       "41  0.031428  0.033768  0.016717  ...  0.036443  0.036108  0.015714  0.026078   \n",
       "42  0.021436  0.030282  0.021096  ...  0.026880  0.020755  0.022116  0.031984   \n",
       "43  0.035557  0.047829  0.040277  ...  0.028320  0.027690  0.026117  0.030837   \n",
       "44  0.015791  0.025403  0.019567  ...  0.022657  0.043598  0.018881  0.033642   \n",
       "45  0.032247  0.024934  0.043883  ...  0.030585  0.026263  0.019614  0.033245   \n",
       "46  0.028551  0.047233  0.021149  ...  0.043003  0.017272  0.044766  0.039126   \n",
       "47  0.031079  0.021175  0.044399  ...  0.040301  0.025956  0.052937  0.018443   \n",
       "48  0.039717  0.046112  0.047795  ...  0.030966  0.026927  0.026927  0.044766   \n",
       "49  0.024024  0.052386  0.028695  ...  0.039706  0.019019  0.046713  0.041375   \n",
       "\n",
       "          24        25        26        27        28        29  \n",
       "0   0.040199  0.022591  0.052824  0.034219  0.024252  0.041528  \n",
       "1   0.031379  0.039655  0.028621  0.026552  0.041724  0.030690  \n",
       "2   0.029146  0.060722  0.025677  0.024983  0.015267  0.045455  \n",
       "3   0.031962  0.034682  0.017681  0.024141  0.068004  0.025162  \n",
       "4   0.033052  0.067454  0.027319  0.048904  0.029005  0.027993  \n",
       "5   0.027643  0.043538  0.027298  0.029026  0.021424  0.046303  \n",
       "6   0.035410  0.026217  0.041539  0.024515  0.043582  0.026558  \n",
       "7   0.030818  0.025859  0.024796  0.023734  0.027630  0.025151  \n",
       "8   0.025777  0.030720  0.029661  0.044138  0.022246  0.022246  \n",
       "9   0.039586  0.028531  0.036733  0.022825  0.041369  0.033524  \n",
       "10  0.029381  0.044936  0.040788  0.042516  0.046664  0.024196  \n",
       "11  0.024821  0.028902  0.032642  0.018361  0.061544  0.041823  \n",
       "12  0.034412  0.049553  0.037853  0.037853  0.037853  0.037853  \n",
       "13  0.045592  0.038887  0.040898  0.023466  0.054308  0.017767  \n",
       "14  0.034597  0.030605  0.033932  0.050898  0.027944  0.029607  \n",
       "15  0.028174  0.025043  0.032696  0.025391  0.039652  0.030957  \n",
       "16  0.036101  0.018379  0.026255  0.052839  0.043321  0.021004  \n",
       "17  0.022388  0.035617  0.046133  0.050882  0.045455  0.032225  \n",
       "18  0.037125  0.031387  0.029025  0.033075  0.036450  0.021937  \n",
       "19  0.031180  0.036907  0.039771  0.043907  0.023544  0.024817  \n",
       "20  0.037245  0.034608  0.023401  0.028675  0.040870  0.031641  \n",
       "21  0.025745  0.048442  0.030488  0.029810  0.018970  0.040989  \n",
       "22  0.028846  0.044098  0.038462  0.032162  0.025199  0.024867  \n",
       "23  0.039670  0.040622  0.024119  0.039670  0.021580  0.025706  \n",
       "24  0.025117  0.035881  0.040545  0.038034  0.020093  0.037316  \n",
       "25  0.026087  0.033391  0.042087  0.018435  0.055652  0.026435  \n",
       "26  0.027081  0.039452  0.029756  0.024741  0.046138  0.040789  \n",
       "27  0.018009  0.030581  0.026164  0.027863  0.031261  0.034319  \n",
       "28  0.022953  0.041453  0.035971  0.016787  0.021925  0.025351  \n",
       "29  0.031751  0.030727  0.025606  0.041652  0.028679  0.028337  \n",
       "30  0.032420  0.015040  0.026404  0.022727  0.017045  0.029078  \n",
       "31  0.035762  0.043449  0.050802  0.025401  0.017714  0.028075  \n",
       "32  0.027901  0.025179  0.017693  0.024158  0.046274  0.045934  \n",
       "33  0.050900  0.033520  0.027933  0.034140  0.022036  0.042210  \n",
       "34  0.034393  0.016872  0.025957  0.027904  0.028877  0.025957  \n",
       "35  0.029655  0.028276  0.038966  0.023103  0.054828  0.035517  \n",
       "36  0.032526  0.032194  0.025224  0.044806  0.029871  0.023233  \n",
       "37  0.063191  0.040401  0.030041  0.024862  0.031077  0.027970  \n",
       "38  0.035250  0.029774  0.029774  0.034565  0.027721  0.027036  \n",
       "39  0.029025  0.045899  0.025312  0.030375  0.052312  0.025312  \n",
       "40  0.032879  0.040518  0.048157  0.036201  0.045500  0.033212  \n",
       "41  0.021398  0.054831  0.041458  0.024072  0.021732  0.027750  \n",
       "42  0.023477  0.020075  0.035046  0.056482  0.029262  0.035046  \n",
       "43  0.034613  0.039333  0.041221  0.042794  0.038074  0.039333  \n",
       "44  0.049090  0.041195  0.046344  0.020254  0.024717  0.034329  \n",
       "45  0.048205  0.021941  0.040226  0.025598  0.046210  0.030585  \n",
       "46  0.032429  0.028551  0.036306  0.026789  0.012689  0.032429  \n",
       "47  0.014344  0.020150  0.043033  0.024590  0.043374  0.039617  \n",
       "48  0.018512  0.034332  0.034332  0.019859  0.024571  0.038708  \n",
       "49  0.022689  0.032366  0.020354  0.018352  0.049383  0.050050  \n",
       "\n",
       "[50 rows x 30 columns]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_feats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826716ab",
   "metadata": {},
   "source": [
    "Another thing I'll be doing to reduce the abuse on my CPU, is saving this sample in a separate csv-file in the data-folder. This way, I won't have to re-execute this operation over and over again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "8afe368a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_feats_df.to_csv('data/brecht-sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "165ccd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = pd.read_csv(\"data/brecht-sample.csv\")\n",
    "Y = sample[\"choice\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490ef6eb",
   "metadata": {},
   "source": [
    "#### The Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "fed43b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dec_tree = DecisionTreeClassifier()\n",
    "dec_tree = dec_tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "4b7deedc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_tree.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64d5479",
   "metadata": {},
   "source": [
    "#### The Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "f3287dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Brech\\anaconda3\\envs\\chidi\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "log_reg = LogisticRegressionCV()\n",
    "log_reg = log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "71166759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16b39cd",
   "metadata": {},
   "source": [
    "#### The \"Perceptotron\" (non-deep neural network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6633522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "382ccbbf",
   "metadata": {},
   "source": [
    "### Deep-learning neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a793f6",
   "metadata": {},
   "source": [
    "Any machine learning model is considered deep learning if it has at least three layers total. This includes input layers, output layers and the hidden layers in between."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec485bd",
   "metadata": {},
   "source": [
    "When it comes to computer vision, neural networks have become the preferred option. I'll be making several neural networks with varying numbers of hidden layers. This is to test the impact an increasing number of layers has on the speed and performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff40de3",
   "metadata": {},
   "source": [
    "It wouldn't be an analysis if I didn't have an initial hypothesis: I'm going to start off from the viewpoint that the performance of our model will be directly proportional to the total number of hidden layers, while our speed will be inversely proportional to it. In other words: the more layers, the better our model performs, but the longer it will take to make a prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "9421fe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Well, what are we waiting for? Let's load in our data and get analyzing!\n",
    "sample = os.listdir(\"data/images/3 De nayer (Loop piste)/partition 1\")\n",
    "sample = data.loc[data[\"image\"].isin(sample)].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "54d1dee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our X will contain the pixels of our loaded images, while our Y contains all of the classifiers\n",
    "X = np.array([cv.imread(\"/\".join(['data','images','3 De nayer (Loop piste)','partition 1',img])) for img in sample[\"image\"]])\n",
    "Y = np.array(sample[\"choice\"])\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95872dac",
   "metadata": {},
   "source": [
    "Our training set consists of 30 images, each of which consist of 480 x 640 BRG-pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "955235c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 480, 640, 3)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "3e6ff3c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "0f1dcf4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 2., 2., 0., 2., 2., 1., 2., 2., 0., 2., 1., 1., 0., 2., 2.,\n",
       "       2., 0., 2., 2., 2., 2., 2., 1., 2., 0., 2., 1., 2.])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "9b184b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "9daa6b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eca09ba",
   "metadata": {},
   "source": [
    "#### The models I made"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7465c1e0",
   "metadata": {},
   "source": [
    "* First Model: Includes only a Flatten-layer and a Dense-layer, the latter consisting of 3 neurons (as many as there are classes) and activated by the softmax-function.\n",
    "* Second Model: Consists of 6 layers, including input and output, for a total of 4 hidden layers.\n",
    "* Third Model: Consists of 8 layers, including input and output, for a total of 6 hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "f0f041bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(480,640,3)),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "second_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(480,640,3)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "third_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(480,640,3)),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(256, activation='selu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='selu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16e60f2",
   "metadata": {},
   "source": [
    "In essence, my first model is nothing more than essentially our Logistic Regression-model from earlier converted into a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "74f126fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_6 (Flatten)         (None, 921600)            0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 3)                 2764803   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,764,803\n",
      "Trainable params: 2,764,803\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "first_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "first_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "729c28ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "3/3 [==============================] - 0s 144ms/step - loss: 2.3445 - accuracy: 0.7083 - val_loss: 14.2155 - val_accuracy: 0.5000\n",
      "Epoch 2/25\n",
      "3/3 [==============================] - 0s 130ms/step - loss: 9.3181 - accuracy: 0.7083 - val_loss: 8.3855 - val_accuracy: 0.5000\n",
      "Epoch 3/25\n",
      "3/3 [==============================] - 0s 99ms/step - loss: 5.0289 - accuracy: 0.8333 - val_loss: 19.7735 - val_accuracy: 0.1667\n",
      "Epoch 4/25\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 5.0639 - accuracy: 0.5833 - val_loss: 12.5198 - val_accuracy: 0.5000\n",
      "Epoch 5/25\n",
      "3/3 [==============================] - 0s 108ms/step - loss: 3.8144 - accuracy: 0.6250 - val_loss: 12.8609 - val_accuracy: 0.5000\n",
      "Epoch 6/25\n",
      "3/3 [==============================] - 0s 111ms/step - loss: 8.7390 - accuracy: 0.6250 - val_loss: 8.6029 - val_accuracy: 0.3333\n",
      "Epoch 7/25\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 9.2138 - accuracy: 0.5000 - val_loss: 21.2543 - val_accuracy: 0.5000\n",
      "Epoch 8/25\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 12.9370 - accuracy: 0.7083 - val_loss: 40.6014 - val_accuracy: 0.5000\n",
      "Epoch 9/25\n",
      "3/3 [==============================] - 0s 98ms/step - loss: 14.2468 - accuracy: 0.7083 - val_loss: 21.9114 - val_accuracy: 0.1667\n",
      "Epoch 10/25\n",
      "3/3 [==============================] - 0s 69ms/step - loss: 5.8883 - accuracy: 0.5417 - val_loss: 13.0777 - val_accuracy: 0.5000\n",
      "Epoch 11/25\n",
      "3/3 [==============================] - 0s 71ms/step - loss: 5.3986 - accuracy: 0.7500 - val_loss: 35.1111 - val_accuracy: 0.1667\n",
      "Epoch 12/25\n",
      "3/3 [==============================] - 0s 89ms/step - loss: 8.1975 - accuracy: 0.5417 - val_loss: 6.0788 - val_accuracy: 0.3333\n",
      "Epoch 13/25\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 5.7163 - accuracy: 0.7917 - val_loss: 2.9271 - val_accuracy: 0.3333\n",
      "Epoch 14/25\n",
      "3/3 [==============================] - 0s 105ms/step - loss: 1.9238 - accuracy: 0.8333 - val_loss: 6.9487 - val_accuracy: 0.3333\n",
      "Epoch 15/25\n",
      "3/3 [==============================] - 0s 111ms/step - loss: 0.1770 - accuracy: 0.9583 - val_loss: 6.4913 - val_accuracy: 0.5000\n",
      "Epoch 16/25\n",
      "3/3 [==============================] - 0s 98ms/step - loss: 0.6656 - accuracy: 0.9583 - val_loss: 5.6407 - val_accuracy: 0.3333\n",
      "Epoch 17/25\n",
      "3/3 [==============================] - 0s 97ms/step - loss: 1.2999e-04 - accuracy: 1.0000 - val_loss: 13.5355 - val_accuracy: 0.1667\n",
      "Epoch 18/25\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 0.6366 - accuracy: 0.8750 - val_loss: 3.7305 - val_accuracy: 0.5000\n",
      "Epoch 19/25\n",
      "3/3 [==============================] - 0s 98ms/step - loss: 1.6857 - accuracy: 0.8750 - val_loss: 2.9634 - val_accuracy: 0.5000\n",
      "Epoch 20/25\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 0.5935 - accuracy: 0.9167 - val_loss: 25.0509 - val_accuracy: 0.1667\n",
      "Epoch 21/25\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 5.1902 - accuracy: 0.8333 - val_loss: 4.6728 - val_accuracy: 0.5000\n",
      "Epoch 22/25\n",
      "3/3 [==============================] - 0s 98ms/step - loss: 0.9503 - accuracy: 0.8333 - val_loss: 7.7235 - val_accuracy: 0.5000\n",
      "Epoch 23/25\n",
      "3/3 [==============================] - 0s 106ms/step - loss: 0.3816 - accuracy: 0.9583 - val_loss: 9.5800 - val_accuracy: 0.5000\n",
      "Epoch 24/25\n",
      "3/3 [==============================] - 0s 104ms/step - loss: 1.2189 - accuracy: 0.8750 - val_loss: 20.7188 - val_accuracy: 0.5000\n",
      "Epoch 25/25\n",
      "3/3 [==============================] - 0s 87ms/step - loss: 2.2023 - accuracy: 0.7083 - val_loss: 11.2790 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x243635ff5e0>"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_model.fit(X_train, Y_train, epochs=25, batch_size=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "799f4c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 70ms/step - loss: 5.4937 - accuracy: 0.8500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5.493687629699707, 0.8500000238418579]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b3475b",
   "metadata": {},
   "source": [
    "Our first model takes 74 milliseconds to make a prediction, with an accuracy of 80%. Performance-wise, it is better than even our Logistic Regression-model earlier. Considering this is supposed to be the \"worst\" deep learning model, I have high hopes for the rest of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "d6e5e81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_7 (Flatten)         (None, 921600)            0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 128)               117964928 \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 32)                4128      \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 117,969,155\n",
      "Trainable params: 117,969,155\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "second_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "second_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "5b5c68a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "3/3 [==============================] - 4s 1s/step - loss: 0.7012 - accuracy: 0.7500 - val_loss: 1.3715 - val_accuracy: 0.5000\n",
      "Epoch 2/25\n",
      "3/3 [==============================] - 3s 1s/step - loss: 2.2412 - accuracy: 0.2083 - val_loss: 1.7491 - val_accuracy: 0.5000\n",
      "Epoch 3/25\n",
      "3/3 [==============================] - 4s 1s/step - loss: 2.2687 - accuracy: 0.5000 - val_loss: 5.1659 - val_accuracy: 0.5000\n",
      "Epoch 4/25\n",
      "3/3 [==============================] - 4s 1s/step - loss: 3.0736 - accuracy: 0.7083 - val_loss: 9.7794 - val_accuracy: 0.5000\n",
      "Epoch 5/25\n",
      "3/3 [==============================] - 4s 1s/step - loss: 3.9725 - accuracy: 0.7500 - val_loss: 7.4839 - val_accuracy: 0.5000\n",
      "Epoch 6/25\n",
      "3/3 [==============================] - 4s 1s/step - loss: 2.5652 - accuracy: 0.7083 - val_loss: 3.8156 - val_accuracy: 0.5000\n",
      "Epoch 7/25\n",
      "3/3 [==============================] - 4s 1s/step - loss: 2.3077 - accuracy: 0.5417 - val_loss: 6.3970 - val_accuracy: 0.5000\n",
      "Epoch 8/25\n",
      "3/3 [==============================] - 4s 1s/step - loss: 3.3370 - accuracy: 0.4167 - val_loss: 2.0667 - val_accuracy: 0.5000\n",
      "Epoch 9/25\n",
      "3/3 [==============================] - 4s 1s/step - loss: 2.3383 - accuracy: 0.5833 - val_loss: 3.4654 - val_accuracy: 0.5000\n",
      "Epoch 10/25\n",
      "3/3 [==============================] - 4s 1s/step - loss: 2.3787 - accuracy: 0.7083 - val_loss: 5.8413 - val_accuracy: 0.6667\n",
      "Epoch 11/25\n",
      "3/3 [==============================] - 4s 1s/step - loss: 2.1102 - accuracy: 0.8333 - val_loss: 3.2197 - val_accuracy: 0.5000\n",
      "Epoch 12/25\n",
      "3/3 [==============================] - 4s 1s/step - loss: 0.4483 - accuracy: 0.8750 - val_loss: 4.1243 - val_accuracy: 0.3333\n",
      "Epoch 13/25\n",
      "3/3 [==============================] - 4s 1s/step - loss: 2.8037 - accuracy: 0.5000 - val_loss: 1.9775 - val_accuracy: 0.5000\n",
      "Epoch 14/25\n",
      "3/3 [==============================] - 3s 1s/step - loss: 1.6346 - accuracy: 0.6250 - val_loss: 3.2459 - val_accuracy: 0.6667\n",
      "Epoch 15/25\n",
      "3/3 [==============================] - 4s 1s/step - loss: 1.7549 - accuracy: 0.6667 - val_loss: 6.5504 - val_accuracy: 0.6667\n",
      "Epoch 16/25\n",
      "3/3 [==============================] - 4s 1s/step - loss: 2.2999 - accuracy: 0.7500 - val_loss: 3.5058 - val_accuracy: 0.5000\n",
      "Epoch 17/25\n",
      "3/3 [==============================] - 4s 1s/step - loss: 2.8939 - accuracy: 0.5417 - val_loss: 2.8070 - val_accuracy: 0.5000\n",
      "Epoch 18/25\n",
      "3/3 [==============================] - 4s 1s/step - loss: 1.8856 - accuracy: 0.7083 - val_loss: 3.5768 - val_accuracy: 0.5000\n",
      "Epoch 19/25\n",
      "3/3 [==============================] - 4s 1s/step - loss: 1.4315 - accuracy: 0.7083 - val_loss: 1.4995 - val_accuracy: 0.5000\n",
      "Epoch 20/25\n",
      "3/3 [==============================] - 4s 1s/step - loss: 0.8122 - accuracy: 0.7917 - val_loss: 3.9818 - val_accuracy: 0.5000\n",
      "Epoch 21/25\n",
      "3/3 [==============================] - 4s 1s/step - loss: 1.3286 - accuracy: 0.7500 - val_loss: 1.0363 - val_accuracy: 0.3333\n",
      "Epoch 22/25\n",
      "3/3 [==============================] - 4s 1s/step - loss: 1.4718 - accuracy: 0.5833 - val_loss: 1.2568 - val_accuracy: 0.5000\n",
      "Epoch 23/25\n",
      "3/3 [==============================] - 4s 1s/step - loss: 2.3393 - accuracy: 0.5000 - val_loss: 3.8376 - val_accuracy: 0.6667\n",
      "Epoch 24/25\n",
      "3/3 [==============================] - 4s 1s/step - loss: 1.7124 - accuracy: 0.8333 - val_loss: 7.6189 - val_accuracy: 0.5000\n",
      "Epoch 25/25\n",
      "3/3 [==============================] - 4s 1s/step - loss: 2.2862 - accuracy: 0.6667 - val_loss: 3.1447 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2436344dd90>"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_model.fit(X_train, Y_train, epochs=25, batch_size=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd37b55",
   "metadata": {},
   "source": [
    "Training this model took considerably longer than our first model, with every epoch taking a handful of seconds to complete. The longest epoch in our last model lasted about one second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "5ef658a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 121ms/step - loss: 2.0548 - accuracy: 0.7500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.054837465286255, 0.75]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdd0fb0",
   "metadata": {},
   "source": [
    "Speed seems to have gone down to 121 milliseconds per prediction made, but the model still has an accuracy 5% higher than our first neural network, and 10% higher than our Logistic Regression-model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1828475d",
   "metadata": {},
   "source": [
    "It does, however, depend on the data used. In a previous attempt, before the pixel-values of our images were converted to values between 0 and 1, our second model actually performed *worse* than not just our first model, but also every single one of our non-deep learning models. The performance of a neural network, therefore, is not just dependent on the structure of its layers, but also on the data fed into its inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "d432d603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_8 (Flatten)         (None, 921600)            0         \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 256)               235929856 \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 236,055,491\n",
      "Trainable params: 236,055,491\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "third_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "third_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "41a39580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "3/3 [==============================] - 8s 2s/step - loss: 8.7798 - accuracy: 0.6250 - val_loss: 8.7169 - val_accuracy: 0.5000\n",
      "Epoch 2/25\n",
      "3/3 [==============================] - 8s 3s/step - loss: 3.9329 - accuracy: 0.7083 - val_loss: 3.9863 - val_accuracy: 0.5000\n",
      "Epoch 3/25\n",
      "3/3 [==============================] - 8s 3s/step - loss: 2.3695 - accuracy: 0.6667 - val_loss: 14.9114 - val_accuracy: 0.1667\n",
      "Epoch 4/25\n",
      "3/3 [==============================] - 8s 3s/step - loss: 5.5436 - accuracy: 0.5833 - val_loss: 19.3375 - val_accuracy: 0.1667\n",
      "Epoch 5/25\n",
      "3/3 [==============================] - 8s 3s/step - loss: 8.3505 - accuracy: 0.4583 - val_loss: 23.5491 - val_accuracy: 0.5000\n",
      "Epoch 6/25\n",
      "3/3 [==============================] - 8s 3s/step - loss: 15.0465 - accuracy: 0.7083 - val_loss: 39.3249 - val_accuracy: 0.5000\n",
      "Epoch 7/25\n",
      "3/3 [==============================] - 8s 3s/step - loss: 20.6045 - accuracy: 0.3750 - val_loss: 30.2747 - val_accuracy: 0.5000\n",
      "Epoch 8/25\n",
      "3/3 [==============================] - 8s 3s/step - loss: 16.5594 - accuracy: 0.7083 - val_loss: 27.5551 - val_accuracy: 0.3333\n",
      "Epoch 9/25\n",
      "3/3 [==============================] - 8s 3s/step - loss: 22.3032 - accuracy: 0.5000 - val_loss: 44.6924 - val_accuracy: 0.5000\n",
      "Epoch 10/25\n",
      "3/3 [==============================] - 8s 3s/step - loss: 15.1166 - accuracy: 0.7083 - val_loss: 17.6601 - val_accuracy: 0.1667\n",
      "Epoch 11/25\n",
      "3/3 [==============================] - 8s 3s/step - loss: 18.0932 - accuracy: 0.2083 - val_loss: 28.9025 - val_accuracy: 0.5000\n",
      "Epoch 12/25\n",
      "3/3 [==============================] - 8s 3s/step - loss: 12.9901 - accuracy: 0.7083 - val_loss: 35.3644 - val_accuracy: 0.5000\n",
      "Epoch 13/25\n",
      "3/3 [==============================] - 8s 3s/step - loss: 11.2608 - accuracy: 0.6667 - val_loss: 4.2914 - val_accuracy: 0.3333\n",
      "Epoch 14/25\n",
      "3/3 [==============================] - 8s 3s/step - loss: 4.7114 - accuracy: 0.4583 - val_loss: 5.7385 - val_accuracy: 0.5000\n",
      "Epoch 15/25\n",
      "3/3 [==============================] - 8s 3s/step - loss: 8.8425 - accuracy: 0.4167 - val_loss: 18.5604 - val_accuracy: 0.5000\n",
      "Epoch 16/25\n",
      "3/3 [==============================] - 8s 3s/step - loss: 9.5567 - accuracy: 0.4167 - val_loss: 28.6089 - val_accuracy: 0.5000\n",
      "Epoch 17/25\n",
      "3/3 [==============================] - 8s 3s/step - loss: 9.4546 - accuracy: 0.4167 - val_loss: 4.5315 - val_accuracy: 0.1667\n",
      "Epoch 18/25\n",
      "3/3 [==============================] - 8s 3s/step - loss: 2.4153 - accuracy: 0.5833 - val_loss: 11.3844 - val_accuracy: 0.5000\n",
      "Epoch 19/25\n",
      "3/3 [==============================] - 8s 3s/step - loss: 6.0551 - accuracy: 0.7500 - val_loss: 17.0391 - val_accuracy: 0.5000\n",
      "Epoch 20/25\n",
      "3/3 [==============================] - 8s 3s/step - loss: 8.2048 - accuracy: 0.7083 - val_loss: 21.1408 - val_accuracy: 0.3333\n",
      "Epoch 21/25\n",
      "3/3 [==============================] - 8s 3s/step - loss: 12.3203 - accuracy: 0.3750 - val_loss: 7.0934 - val_accuracy: 0.5000\n",
      "Epoch 22/25\n",
      "3/3 [==============================] - 8s 3s/step - loss: 7.1772 - accuracy: 0.5000 - val_loss: 1.8124 - val_accuracy: 0.5000\n",
      "Epoch 23/25\n",
      "3/3 [==============================] - 8s 3s/step - loss: 5.5936 - accuracy: 0.5417 - val_loss: 18.2682 - val_accuracy: 0.5000\n",
      "Epoch 24/25\n",
      "3/3 [==============================] - 8s 3s/step - loss: 10.8857 - accuracy: 0.7083 - val_loss: 22.7880 - val_accuracy: 0.5000\n",
      "Epoch 25/25\n",
      "3/3 [==============================] - 8s 3s/step - loss: 8.7625 - accuracy: 0.7083 - val_loss: 8.4707 - val_accuracy: 0.3333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24363506790>"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "third_model.fit(X_train, Y_train, epochs=25, batch_size=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "dc7999cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 201ms/step - loss: 8.0819 - accuracy: 0.1500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[8.081899642944336, 0.15000000596046448]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "third_model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22625e02",
   "metadata": {},
   "source": [
    "Every prediction took about 169 milliseconds, and our accuracy has shot up to above 75%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fae4dcd",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db3a010",
   "metadata": {},
   "source": [
    "* The way the data fed into a neural network is structured plays a big role in determining the overall accuracy of a neural network when classifying images."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
